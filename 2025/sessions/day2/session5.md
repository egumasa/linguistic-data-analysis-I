---
title: "Session 5"
subtitle: ""
---

## One-liner


## üéØ Learning Objectives

By the end of this session, you will be able to:

> - Compute frequency of a single-word lexical item in reference corpora
> - Derive vocabulary frequency list using concordancing software (e.g., AntConc)
> - Apply tokenization on the Japanese language corpus for frequency analysis
> - Conduct Lexical Profiling using a web-application or desktop application (e.g., AntWordProfiler)


## üîë Key Concepts

- Lexical profiling
- Frequency Lists
- Zipf's law
- Lexical coverage


##  üõ†Ô∏è Tools Used

- **[AntConc](https://www.laurenceanthony.net/software/antconc/)**
- **[AntWordProfiler](https://www.laurenceanthony.net/software/antwordprofiler/)**
- **[New Word Levels Checker](https://nwlc.pythonanywhere.com/)**
- **[LexTutor](https://www.lextutor.ca/)**

## Materials

### Slides for the session (Under construction)

<!-- [View slides in fullscreen](../../slides/session-5.html){target="_blank"} 

<iframe src="../../slides/session-5.html" width="100%" height="600px" frameborder="0" allowfullscreen></iframe> -->



## Reflection


<!-- 
<iframe src="session1-intro/slides/slides.html" width="100%" height="600px" frameborder="0"></iframe>

[View slides in fullscreen](session1-intro/slides/slides.html){target="_blank"} -->


---

# Hands-on activity


## Frequency list


## Lexical Profiling


## Keyness Analysis
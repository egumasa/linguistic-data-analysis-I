---
title: "Session 5"
subtitle: ""
---

## One-liner

You will learn how to create frequency-lists for English and Japanese.

## ğŸ¯ Learning Objectives

By the end of this session, you will be able to:

> - Compute frequency of a single-word lexical item in reference corpora
> - Derive vocabulary frequency list using concordancing software (e.g., AntConc)
> - Apply tokenization on the Japanese language corpus for frequency analysis
> - Conduct Lexical Profiling using a web-application or desktop application (e.g., AntWordProfiler)


## ğŸ”‘ Key Concepts

- Lexical profiling
- Frequency Lists
- Zipf's law
- Lexical coverage


## ğŸŒŠ Dive Deeper - Recommended Readings

##  ğŸ› ï¸ Tools Used

- **[AntConc](https://www.laurenceanthony.net/software/antconc/)**
- **[AntWordProfiler](https://www.laurenceanthony.net/software/antwordprofiler/)**
- **[New Word Levels Checker](https://nwlc.pythonanywhere.com/)**
- **[LexTutor](https://www.lextutor.ca/)**

## Materials

### Slides for the session

<!-- <div class="d-flex gap-2 mb-3">
  
[ğŸ“Š View Interactive Slides (Under construction)](../../slides/session-5.html){.btn .btn-primary .btn-lg target="_blank"} 

</div>  -->



## Reflection


<!-- 
<iframe src="session1-intro/slides/slides.html" width="100%" height="600px" frameborder="0"></iframe>

[View slides in fullscreen](session1-intro/slides/slides.html){target="_blank"} -->


---

# Hands-on activity


## Frequency list


## Lexical Profiling


## Keyness Analysis

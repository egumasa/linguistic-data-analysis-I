{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2115b015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD tokens: ['彼', 'は', '昨', '日本', 'を', '買っ', 'て', '読み', '始め', 'まし', 'た', '。']\n",
      "ALT tokens (mode A): ['彼', 'は', '昨', '日本', 'を', '買っ', 'て', '読み', '始め', 'まし', 'た', '。']\n",
      "STD POS: ['PRON', 'ADP', 'NOUN', 'PROPN', 'ADP', 'VERB', 'SCONJ', 'VERB', 'VERB', 'AUX', 'AUX', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sudachipy import dictionary, tokenizer\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# 1. Standard pipeline (for reliable POS/DEP):\n",
    "std_nlp = spacy.load(\"ja_core_news_sm\")\n",
    "\n",
    "# 2. Alternate segmentation pipeline (no tagging):\n",
    "sudachi = dictionary.Dictionary().create()\n",
    "MODE = tokenizer.Tokenizer.SplitMode.A  # A=short, C=long\n",
    "\n",
    "alt_nlp = spacy.blank(\"ja\")\n",
    "\n",
    "def sudachi_tokenizer_func(text):\n",
    "    ms = sudachi.tokenize(text, MODE)\n",
    "    words = [m.surface() for m in ms]\n",
    "    spaces = [False]*len(words)\n",
    "    return Doc(alt_nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "alt_nlp.tokenizer = sudachi_tokenizer_func\n",
    "\n",
    "text = \"彼は昨日本を買って読み始めました。\"\n",
    "doc_std = std_nlp(text)\n",
    "doc_alt = alt_nlp(text)\n",
    "\n",
    "print(\"STD tokens:\", [t.text for t in doc_std])\n",
    "print(\"ALT tokens (mode A):\", [t.text for t in doc_alt])\n",
    "print(\"STD POS:\", [t.pos_ for t in doc_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93cf5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年', 'の', '干支', 'は', '庚子', 'です', '。', '東京', 'オリンピック', 'たのし', 'み', 'だ', 'なあ', '。']\n",
      "[('今年', 'NOUN', '名詞-普通名詞-副詞可能'), ('の', 'ADP', '助詞-格助詞'), ('干支', 'NOUN', '名詞-普通名詞-一般'), ('は', 'ADP', '助詞-係助詞'), ('庚子', 'NOUN', '名詞-普通名詞-一般'), ('です', 'AUX', '助動詞'), ('。', 'PUNCT', '補助記号-句点'), ('東京', 'PROPN', '名詞-固有名詞-地名-一般'), ('オリンピック', 'NOUN', '名詞-普通名詞-一般'), ('楽しい', 'ADJ', '形容詞-一般'), ('味', 'PART', '接尾辞-名詞的-一般'), ('だ', 'AUX', '助動詞'), ('な', 'PART', '助詞-終助詞'), ('。', 'PUNCT', '補助記号-句点')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sudachipy import tokenizer, dictionary\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "sudachi = dictionary.Dictionary().create()\n",
    "MODE = tokenizer.Tokenizer.SplitMode.C  # change to A for short proxy\n",
    "\n",
    "nlp = spacy.blank(\"ja\")\n",
    "\n",
    "def sudachi_tokenizer_func(text):\n",
    "    sudachi_tokens = sudachi.tokenize(text, MODE)\n",
    "    words = [m.surface() for m in sudachi_tokens]\n",
    "    spaces = [False]*len(words)\n",
    "    return Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# nlp.tokenizer = sudachi_tokenizer_func\n",
    "\n",
    "doc = nlp(\"今年の干支は庚子です。東京オリンピックたのしみだなあ。\")\n",
    "print([t.text for t in doc])\n",
    "print([(t.norm_, t.pos_, t.tag_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4487d1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:75: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  relative_imports = re.findall(\"^\\s*import\\s+\\.(\\S+)\\s*$\", content, flags=re.MULTILINE)\n",
      "/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:77: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  relative_imports += re.findall(\"^\\s*from\\s+\\.(\\S+)\\s+import\", content, flags=re.MULTILINE)\n",
      "/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:119: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  imports = re.findall(\"^\\s*import\\s+(\\S+)\\s*$\", content, flags=re.MULTILINE)\n",
      "/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:121: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  imports += re.findall(\"^\\s*from\\s+(\\S+)\\s+import\", content, flags=re.MULTILINE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ひごろ ひごろ\n",
      "日ごろ 日ごろ\n",
      "日頃 日頃\n",
      "呑み 呑む\n",
      "呑ん 呑む\n",
      "で で\n",
      "飲ん 飲む\n",
      "で で\n",
      "書きあらわす 書きあらわす\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "doc = nlp(\"ひごろ 日ごろ 日頃 呑み 呑んで 飲んで 書きあらわす\")\n",
    "for tok in doc:\n",
    "    print(tok.text, tok.lemma_)  # lemma_ ~ Sudachi dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2a4131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日ごろ 日ごろ 日ごろ 名詞,普通名詞,副詞可能,*\n",
      "ひごろ ひごろ ひごろ 名詞,普通名詞,副詞可能,*\n",
      "日頃 日頃 日頃 名詞,普通名詞,副詞可能,*\n",
      "居る 居る 居る 動詞,非自立可能,*,*\n",
      "いる いる いる 動詞,非自立可能,*,*\n",
      "書きあらわす 書きあらわす 書きあらわす 動詞,一般,*,*\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from fugashi import Tagger\n",
    "\n",
    "# Initialize UniDic Tagger\n",
    "tagger = Tagger()   # fugashi auto-loads UniDic if installed\n",
    "\n",
    "nlp = spacy.blank(\"ja\")        # blank Japanese pipeline\n",
    "\n",
    "# Register custom token extensions\n",
    "from spacy.tokens import Token\n",
    "Token.set_extension(\"unidic_lemma\", default=None, force=True)\n",
    "Token.set_extension(\"unidic_reading\", default=None, force=True)\n",
    "Token.set_extension(\"unidic_pos\", default=None, force=True)\n",
    "Token.set_extension(\"unidic_feats\", default=None, force=True)\n",
    "\n",
    "def mecab_tokenizer(text):\n",
    "    words = []\n",
    "    spaces = []\n",
    "    lemmas = []\n",
    "    analyses = tagger(text)\n",
    "    for m in analyses:\n",
    "        surface = m.surface\n",
    "        words.append(surface)\n",
    "        spaces.append(False)  # Japanese generally no spaces\n",
    "    doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "    # Attach UniDic info\n",
    "    for tok, m in zip(doc, analyses):\n",
    "        # m.feature: tuple with UniDic columns. Structure depends on UniDic version.\n",
    "        # Typical indices (verify!) e.g. lemma at feature[10], reading at feature[9].\n",
    "        \n",
    "        feats = m.feature\n",
    "        # Safer: use fugashi attribute helpers\n",
    "        tok._.unidic_lemma = m.feature[10]  # or m.feature[10]\n",
    "        tok._.unidic_reading = m.feature[10]  # unified katakana reading\n",
    "        tok._.unidic_pos = \",\".join(m.feature[:4])  # hierarchical POS tuple\n",
    "        tok._.unidic_feats = feats\n",
    "    return doc\n",
    "\n",
    "nlp.tokenizer = mecab_tokenizer\n",
    "\n",
    "# (Optionally add your own components after this, e.g. a statistical tagger trained on this segmentation)\n",
    "doc = nlp(\"日ごろ ひごろ 日頃 居る いる 書きあらわす\")\n",
    "for t in doc:\n",
    "    print(t.text, t._.unidic_lemma, t._.unidic_reading, t._.unidic_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b9aa05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mdir\u001b[39m(\u001b[43mm\u001b[49m))\n",
      "\u001b[31mNameError\u001b[39m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "print(dir(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc13e1fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <function unicdic_enricher at 0x13ca313a0> (name: 'unidic_enricher').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m                 doc[i]._.unidic_pos = \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(m.pos)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43municdic_enricher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munidic_enricher\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m doc = nlp(\u001b[33m\"\u001b[39m\u001b[33m日ごろ ひごろ 日頃 居る いる 書きあらわす\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/spacy/language.py:811\u001b[39m, in \u001b[36mLanguage.add_pipe\u001b[39m\u001b[34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[39m\n\u001b[32m    809\u001b[39m     bad_val = \u001b[38;5;28mrepr\u001b[39m(factory_name)\n\u001b[32m    810\u001b[39m     err = Errors.E966.format(component=bad_val, name=name)\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[32m    812\u001b[39m name = name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m factory_name\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.component_names:\n",
      "\u001b[31mValueError\u001b[39m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <function unicdic_enricher at 0x13ca313a0> (name: 'unidic_enricher').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from fugashi import Tagger\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "tagger = Tagger()\n",
    "\n",
    "# Register extension fields (only if not already)\n",
    "from spacy.tokens import Token\n",
    "for field in [\"unidic_lemma\",\"unidic_reading\",\"unidic_pos\"]:\n",
    "    if not Token.has_extension(field):\n",
    "        Token.set_extension(field, default=None)\n",
    "\n",
    "def unicdic_enricher(doc):\n",
    "    text = doc.text\n",
    "    # Build a char->token index map (start offsets)\n",
    "    char2token = {}\n",
    "    for i, tok in enumerate(doc):\n",
    "        for pos in range(tok.idx, tok.idx + len(tok.text)):\n",
    "            char2token.setdefault(pos, i)\n",
    "    # Collect MeCab tokens with offsets\n",
    "    cursor = 0\n",
    "    for m in tagger(text):\n",
    "        surf = m.surface\n",
    "        start = text.find(surf, cursor)\n",
    "        cursor = start + len(surf)\n",
    "        # Find spaCy token that *starts* here (approx.)\n",
    "        if start in char2token:\n",
    "            i = char2token[start]\n",
    "            # Only annotate if exact surface match (avoid mid-token)\n",
    "            if doc[i].text.startswith(surf):\n",
    "                doc[i]._.unidic_lemma = m.dictionary_form\n",
    "                doc[i]._.unidic_reading = m.reading\n",
    "                doc[i]._.unidic_pos = \",\".join(m.pos)\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(unicdic_enricher, name=\"unidic_enricher\", last=True)\n",
    "\n",
    "doc = nlp(\"日ごろ ひごろ 日頃 居る いる 書きあらわす\")\n",
    "for t in doc:\n",
    "    print(t.text, t.lemma_, t._.unidic_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f56e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linguistic-data-analysis-I",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

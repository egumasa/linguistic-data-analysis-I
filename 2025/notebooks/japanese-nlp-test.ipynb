{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japanese NLP Analysis: Comparative Study of UniDic-based Approaches\n",
    "\n",
    "This notebook implements and compares two approaches for Japanese morphological analysis with BCCWJ frequency matching:\n",
    "\n",
    "- **Plan A**: MeCab (fugashi) + UniDic direct pipeline\n",
    "- **Plan B**: GiNZA (Sudachi) + UniDic alignment pipeline\n",
    "\n",
    "Each approach is designed for reproducible setup, implementation, validation, and operational use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Verification\n",
    "\n",
    "First, let's verify and set up our environment with all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.2 (main, Feb 25 2024, 03:55:42) [Clang 17.0.6 ]\n",
      "Working directory: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/2025/notebooks\n",
      "\n",
      "Checking package availability:\n",
      "✓ fugashi\n",
      "✓ unidic\n",
      "✗ unidic-lite - NOT FOUND\n",
      "✓ spacy\n",
      "✓ ginza\n",
      "✗ ja-ginza - NOT FOUND\n",
      "✓ sudachipy\n",
      "✓ pandas\n",
      "✓ numpy\n",
      "✓ matplotlib\n",
      "✓ collections (built-in)\n"
     ]
    }
   ],
   "source": [
    "# Environment verification and setup\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Required packages\n",
    "required_packages = [\n",
    "    'fugashi', 'unidic', 'unidic-lite', 'spacy', 'ginza', \n",
    "    'ja-ginza', 'sudachipy', 'pandas', 'numpy', 'matplotlib', 'collections'\n",
    "]\n",
    "\n",
    "print(\"\\nChecking package availability:\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        if package == 'collections':\n",
    "            import collections\n",
    "            print(f\"✓ {package} (built-in)\")\n",
    "        else:\n",
    "            __import__(package)\n",
    "            print(f\"✓ {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"✗ {package} - NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy not available - will use numpy for correlation\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# Japanese NLP libraries\n",
    "import fugashi\n",
    "import unidic\n",
    "import spacy\n",
    "from spacy.tokens import Token, Doc\n",
    "\n",
    "# Statistical analysis\n",
    "try:\n",
    "    from scipy.stats import spearmanr\n",
    "    scipy_available = True\n",
    "except ImportError:\n",
    "    print(\"scipy not available - will use numpy for correlation\")\n",
    "    scipy_available = False\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniDic directory: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/unidic/dicdir\n",
      "UniDic is properly installed\n",
      "Fugashi + UniDic test successful: テスト\n"
     ]
    }
   ],
   "source": [
    "# Check UniDic installation and download if needed\n",
    "try:\n",
    "    print(f\"UniDic directory: {unidic.DICDIR}\")\n",
    "    print(\"UniDic is properly installed\")\n",
    "except Exception as e:\n",
    "    print(f\"UniDic issue: {e}\")\n",
    "    print(\"You may need to run: python -m unidic download\")\n",
    "\n",
    "# Test basic fugashi functionality\n",
    "try:\n",
    "    tagger = fugashi.Tagger(f'-d \"{unidic.DICDIR}\"')\n",
    "    test_result = list(tagger(\"テスト\"))\n",
    "    print(f\"Fugashi + UniDic test successful: {test_result[0].surface}\")\n",
    "except Exception as e:\n",
    "    print(f\"Fugashi test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Data Preparation\n",
    "\n",
    "Let's create realistic Japanese text samples for testing our pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample texts prepared:\n",
      " 1. 彼は日ごろから本を読むのが好きです。\n",
      " 2. ひごろの勉強が大切だと思います。\n",
      " 3. 日頃の努力が実を結ぶでしょう。\n",
      " 4. 彼女は書きあらわすことが得意です。\n",
      " 5. その問題を書き表すのは難しい。\n",
      " 6. 今日は東京オリンピックについて話しましょう。\n",
      " 7. コーヒーを飲んで、呑み込んで、また飲んでしまった。\n",
      " 8. 国際的な協力が必要不可欠です。\n",
      " 9. 機械学習の技術が進歩している。\n",
      "10. 自然言語処理は興味深い分野だ。\n",
      "\n",
      "Extended corpus: 30 texts\n"
     ]
    }
   ],
   "source": [
    "# Sample Japanese texts for testing\n",
    "sample_texts = [\n",
    "    \"彼は日ごろから本を読むのが好きです。\",\n",
    "    \"ひごろの勉強が大切だと思います。\",\n",
    "    \"日頃の努力が実を結ぶでしょう。\",\n",
    "    \"彼女は書きあらわすことが得意です。\",\n",
    "    \"その問題を書き表すのは難しい。\",\n",
    "    \"今日は東京オリンピックについて話しましょう。\",\n",
    "    \"コーヒーを飲んで、呑み込んで、また飲んでしまった。\",\n",
    "    \"国際的な協力が必要不可欠です。\",\n",
    "    \"機械学習の技術が進歩している。\",\n",
    "    \"自然言語処理は興味深い分野だ。\"\n",
    "]\n",
    "\n",
    "print(\"Sample texts prepared:\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"{i:2d}. {text}\")\n",
    "\n",
    "# Create a larger corpus by repeating and slightly modifying texts\n",
    "extended_corpus = sample_texts * 3  # Simulate frequency variations\n",
    "print(f\"\\nExtended corpus: {len(extended_corpus)} texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock BCCWJ frequency data:\n",
      "  lemma reading   pos  freq_bccwj               key\n",
      "0    日頃     ヒゴロ    名詞        1250     (日頃, ヒゴロ, 名詞)\n",
      "1     本      ホン    名詞        8500       (本, ホン, 名詞)\n",
      "2    読む      ヨム    動詞        3200      (読む, ヨム, 動詞)\n",
      "3    好き      スキ  形容動詞        2100    (好き, スキ, 形容動詞)\n",
      "4    勉強   ベンキョウ    名詞        4200   (勉強, ベンキョウ, 名詞)\n",
      "5    大切    タイセツ  形容動詞        1800  (大切, タイセツ, 形容動詞)\n",
      "6    思う     オモウ    動詞        9500     (思う, オモウ, 動詞)\n",
      "7    努力    ドリョク    名詞        2200    (努力, ドリョク, 名詞)\n",
      "8     実       ミ    名詞        1100        (実, ミ, 名詞)\n",
      "9    結ぶ     ムスブ    動詞         800     (結ぶ, ムスブ, 動詞)\n",
      "\n",
      "Total entries: 25\n"
     ]
    }
   ],
   "source": [
    "# Create mock BCCWJ frequency data for testing\n",
    "# In real usage, this would be loaded from an actual BCCWJ frequency file\n",
    "\n",
    "mock_bccwj_data = [\n",
    "    ('日頃', 'ヒゴロ', '名詞', 1250),\n",
    "    ('本', 'ホン', '名詞', 8500),\n",
    "    ('読む', 'ヨム', '動詞', 3200),\n",
    "    ('好き', 'スキ', '形容動詞', 2100),\n",
    "    ('勉強', 'ベンキョウ', '名詞', 4200),\n",
    "    ('大切', 'タイセツ', '形容動詞', 1800),\n",
    "    ('思う', 'オモウ', '動詞', 9500),\n",
    "    ('努力', 'ドリョク', '名詞', 2200),\n",
    "    ('実', 'ミ', '名詞', 1100),\n",
    "    ('結ぶ', 'ムスブ', '動詞', 800),\n",
    "    ('書く', 'カク', '動詞', 4100),\n",
    "    ('表す', 'アラワス', '動詞', 1500),\n",
    "    ('得意', 'トクイ', '形容動詞', 1300),\n",
    "    ('問題', 'モンダイ', '名詞', 6200),\n",
    "    ('難しい', 'ムズカシイ', '形容詞', 3800),\n",
    "    ('今日', 'キョウ', '名詞', 5500),\n",
    "    ('東京', 'トウキョウ', '名詞', 4800),\n",
    "    ('話す', 'ハナス', '動詞', 3600),\n",
    "    ('飲む', 'ノム', '動詞', 2400),\n",
    "    ('呑む', 'ノム', '動詞', 150),\n",
    "    ('国際', 'コクサイ', '名詞', 2800),\n",
    "    ('協力', 'キョウリョク', '名詞', 1900),\n",
    "    ('必要', 'ヒツヨウ', '形容動詞', 4500),\n",
    "    ('技術', 'ギジュツ', '名詞', 3900),\n",
    "    ('進歩', 'シンポ', '名詞', 1100)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_bccwj = pd.DataFrame(mock_bccwj_data, columns=['lemma', 'reading', 'pos', 'freq_bccwj'])\n",
    "df_bccwj['key'] = list(zip(df_bccwj.lemma, df_bccwj.reading, df_bccwj.pos))\n",
    "\n",
    "print(\"Mock BCCWJ frequency data:\")\n",
    "print(df_bccwj.head(10))\n",
    "print(f\"\\nTotal entries: {len(df_bccwj)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plan A: MeCab (fugashi) + UniDic Direct Pipeline\n",
    "\n",
    "### A-1 to A-3: Setup and Configuration\n",
    "\n",
    "UniDic provides the morphological analysis system used in BCCWJ, making it ideal for frequency matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Plan A: fugashi + UniDic pipeline\n",
      "Tagger initialized with UniDic dictionary: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/unidic/dicdir\n",
      "\n",
      "Test analysis of '日ごろから勉強している。':\n",
      "  日ごろ -> 日頃 [名,詞,,,普,通,名,詞,,,副,詞,可,能,,,*]\n",
      "  から -> から [助,詞,,,格,助,詞,,,*,,,*]\n",
      "  勉強 -> 勉強 [名,詞,,,普,通,名,詞,,,サ,変,可,能,,,*]\n",
      "  し -> 為る [動,詞,,,非,自,立,可,能,,,*,,,*]\n",
      "  て -> て [助,詞,,,接,続,助,詞,,,*,,,*]\n",
      "  いる -> 居る [動,詞,,,非,自,立,可,能,,,*,,,*]\n",
      "  。 -> 。 [補,助,記,号,,,句,点,,,*,,,*]\n"
     ]
    }
   ],
   "source": [
    "# A-3: Initialize fugashi with UniDic\n",
    "print(\"Initializing Plan A: fugashi + UniDic pipeline\")\n",
    "\n",
    "# Initialize tagger with explicit UniDic path\n",
    "tagger_a = fugashi.Tagger(f'-d \"{unidic.DICDIR}\"')\n",
    "print(f\"Tagger initialized with UniDic dictionary: {unidic.DICDIR}\")\n",
    "\n",
    "# Test the tagger\n",
    "test_text = \"日ごろから勉強している。\"\n",
    "tokens = list(tagger_a(test_text))\n",
    "print(f\"\\nTest analysis of '{test_text}':\")\n",
    "for token in tokens:\n",
    "    print(f\"  {token.surface} -> {token.feature.lemma} [{','.join(token.pos)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keys from '日ごろから勉強している。':\n",
      "  (日ごろ, ヒゴロ, 名)\n",
      "  (から, カラ, 助)\n",
      "  (勉強, ベンキョー, 名)\n",
      "  (する, スル, 動)\n",
      "  (て, テ, 助)\n",
      "  (いる, イル, 動)\n",
      "  (。, *, 補)\n"
     ]
    }
   ],
   "source": [
    "# A-4: Morphological field extraction function\n",
    "def iter_lemma_keys_plan_a(text: str, tagger) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Extract (lemma, reading, pos_major) tuples from text using UniDic.\n",
    "    \n",
    "    Args:\n",
    "        text: Input Japanese text\n",
    "        tagger: fugashi Tagger instance\n",
    "    \n",
    "    Returns:\n",
    "        List of (dictionary_form, reading, pos_major) tuples\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    for m in tagger(text):\n",
    "        if m.surface.strip():  # Skip empty tokens\n",
    "            # UniDic POS is hierarchical; use major category (pos[0])\n",
    "            pos_major = m.pos[0] if m.pos else 'UNKNOWN'\n",
    "            lemma = m.feature[10] if m.feature[10] else m.surface\n",
    "            reading = m.feature[11] if m.feature[11] else ''\n",
    "            keys.append((lemma, reading, pos_major))\n",
    "    return keys\n",
    "\n",
    "# Test the extraction function\n",
    "test_keys = iter_lemma_keys_plan_a(test_text, tagger_a)\n",
    "print(f\"Extracted keys from '{test_text}':\")\n",
    "for lemma, reading, pos in test_keys:\n",
    "    print(f\"  ({lemma}, {reading}, {pos})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keys from '日ごろから勉強している。' (fixed version):\n",
      "  (日ごろ, ヒゴロ, 名)\n",
      "  (から, カラ, 助)\n",
      "  (勉強, ベンキョー, 名)\n",
      "  (する, シ, 動)\n",
      "  (て, テ, 助)\n",
      "  (いる, イル, 動)\n",
      "  (。, *, 補)\n"
     ]
    }
   ],
   "source": [
    "# Fixed version with proper fugashi/UniDic attribute handling\n",
    "def iter_lemma_keys_fixed(text: str, tagger) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Extract (lemma, reading, pos_major) tuples from text using UniDic.\n",
    "    Fixed version that handles fugashi attribute variations.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    for m in tagger(text):\n",
    "        if m.surface.strip():  # Skip empty tokens\n",
    "            # UniDic POS is hierarchical; use major category (pos[0])\n",
    "            pos_major = m.pos[0] if m.pos else 'UNKNOWN'\n",
    "            \n",
    "            # Handle different attribute names for lemma\n",
    "            try:\n",
    "                lemma = m.lemma if hasattr(m, 'lemma') else m.feature[10]\n",
    "            except:\n",
    "                lemma = m.surface  # fallback\n",
    "            \n",
    "            # Handle different attribute names for reading\n",
    "            try:\n",
    "                reading = m.feature[9] if len(m.feature) > 9 else ''\n",
    "            except:\n",
    "                reading = ''  # fallback\n",
    "            \n",
    "            keys.append((lemma, reading, pos_major))\n",
    "    return keys\n",
    "\n",
    "# Use the fixed function\n",
    "iter_lemma_keys_plan_a = iter_lemma_keys_fixed\n",
    "\n",
    "# Test the fixed function\n",
    "test_keys = iter_lemma_keys_plan_a(test_text, tagger_a)\n",
    "print(f\"Extracted keys from '{test_text}' (fixed version):\")\n",
    "for lemma, reading, pos in test_keys:\n",
    "    print(f\"  ({lemma}, {reading}, {pos})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 30 texts with Plan A...\n",
      "\n",
      "Plan A Results (top 15):\n",
      "   lemma reading pos  freq_local  freq_bccwj\n",
      "11     。       *   補          30         NaN\n",
      "8      が       ガ   助          18         NaN\n",
      "1      は       ワ   助          15         NaN\n",
      "7      の       ノ   助          15         NaN\n",
      "5      を       オ   助          12         NaN\n",
      "10    です      デス   助           9         NaN\n",
      "42     で       デ   助           9         NaN\n",
      "15     だ       ダ   助           6         NaN\n",
      "37     て       テ   助           6         NaN\n",
      "41    飲む      ノン   動           6         NaN\n",
      "43     、       *   補           6         NaN\n",
      "48    国際    コクサイ   名           3         NaN\n",
      "47     た       タ   助           3         NaN\n",
      "46   しまう     シマッ   動           3         NaN\n",
      "0      彼      カレ   代           3         NaN\n"
     ]
    }
   ],
   "source": [
    "# A-5: Frequency analysis with BCCWJ matching\n",
    "def analyze_corpus_plan_a(corpus: List[str], tagger, bccwj_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Analyze corpus using Plan A and match with BCCWJ frequencies.\"\"\"\n",
    "    freq = Counter()\n",
    "    \n",
    "    print(f\"Analyzing {len(corpus)} texts with Plan A...\")\n",
    "    for text in corpus:\n",
    "        for key in iter_lemma_keys_plan_a(text, tagger):\n",
    "            freq[key] += 1\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    for (lemma, reading, pos), count in freq.items():\n",
    "        rows.append((lemma, reading, pos, count))\n",
    "    \n",
    "    df_local = pd.DataFrame(rows, columns=['lemma', 'reading', 'pos', 'freq_local'])\n",
    "    df_local['key'] = list(zip(df_local.lemma, df_local.reading, df_local.pos))\n",
    "    \n",
    "    # Merge with BCCWJ data\n",
    "    merged = df_local.merge(bccwj_df[['key', 'freq_bccwj']], on='key', how='left')\n",
    "    \n",
    "    return merged.sort_values('freq_local', ascending=False)\n",
    "\n",
    "# Run Plan A analysis\n",
    "results_a = analyze_corpus_plan_a(extended_corpus, tagger_a, df_bccwj)\n",
    "print(f\"\\nPlan A Results (top 15):\")\n",
    "print(results_a.head(15)[['lemma', 'reading', 'pos', 'freq_local', 'freq_bccwj']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan A Evaluation Metrics:\n",
      "  type_coverage: 0.000\n",
      "  token_coverage: 0.000\n",
      "  correlation: None\n",
      "  p_value: None\n",
      "  total_types: 66\n",
      "  matched_types: 0\n",
      "  total_tokens: 297\n",
      "  matched_tokens: 0\n"
     ]
    }
   ],
   "source": [
    "# A-6: Evaluation metrics for Plan A\n",
    "def calculate_metrics(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Calculate coverage and correlation metrics.\"\"\"\n",
    "    # Coverage: percentage of local tokens found in BCCWJ\n",
    "    matched = df.dropna(subset=['freq_bccwj'])\n",
    "    coverage = len(matched) / len(df) * 100\n",
    "    \n",
    "    # Token coverage (by frequency)\n",
    "    total_tokens = df['freq_local'].sum()\n",
    "    matched_tokens = matched['freq_local'].sum()\n",
    "    token_coverage = matched_tokens / total_tokens * 100\n",
    "    \n",
    "    # Spearman correlation for matched items\n",
    "    if len(matched) > 1:\n",
    "        if scipy_available:\n",
    "            correlation, p_value = spearmanr(matched['freq_local'], matched['freq_bccwj'])\n",
    "        else:\n",
    "            correlation = np.corrcoef(matched['freq_local'].rank(), matched['freq_bccwj'].rank())[0,1]\n",
    "            p_value = None\n",
    "    else:\n",
    "        correlation, p_value = None, None\n",
    "    \n",
    "    return {\n",
    "        'type_coverage': coverage,\n",
    "        'token_coverage': token_coverage,\n",
    "        'correlation': correlation,\n",
    "        'p_value': p_value,\n",
    "        'total_types': len(df),\n",
    "        'matched_types': len(matched),\n",
    "        'total_tokens': total_tokens,\n",
    "        'matched_tokens': matched_tokens\n",
    "    }\n",
    "\n",
    "metrics_a = calculate_metrics(results_a)\n",
    "print(\"Plan A Evaluation Metrics:\")\n",
    "for key, value in metrics_a.items():\n",
    "    if isinstance(value, float) and value is not None:\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Fugashi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "彼 [('彼', '代名詞', '代')]\n",
      "は [('は', '助詞', '助')]\n",
      "日ごろ [('日頃', '名詞', '名')]\n",
      "本 [('本', '名詞', '名')]\n",
      "を [('を', '助詞', '助')]\n",
      "読む [('読む', '動詞', '動')]\n",
      "。 [('。', '補助記号', '補')]\n"
     ]
    }
   ],
   "source": [
    "import fugashi, unidic\n",
    "from spacy.tokens import Token\n",
    "tagger = fugashi.Tagger()\n",
    "tagger = fugashi.Tagger(f'-d \"{unidic.DICDIR}\"')\n",
    "\n",
    "if not Token.has_extension(\"unidic_lemmas\"):\n",
    "    Token.set_extension(\"unidic_lemmas\", default=None)\n",
    "\n",
    "def enrich_with_unidic(doc):\n",
    "    text = doc.text\n",
    "    # GiNZA token start index -> token\n",
    "    start_map = {tok.idx: tok for tok in doc}\n",
    "    cursor = 0\n",
    "    for m in tagger(text):\n",
    "        surf = m.surface\n",
    "        start = text.find(surf, cursor)\n",
    "        if start < 0:\n",
    "            continue\n",
    "        cursor = start + len(surf)\n",
    "        tok = start_map.get(start)\n",
    "        if tok:\n",
    "            if tok._.unidic_lemmas is None:\n",
    "                tok._.unidic_lemmas = []\n",
    "            tok._.unidic_lemmas.append(\n",
    "                (m.feature.lemma, m.feature.pos1, m.pos[0])\n",
    "            )\n",
    "    return doc\n",
    "\n",
    "doc = enrich_with_unidic(doc)\n",
    "for t in doc:\n",
    "    print(t.text, t._.unidic_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"日頃からの日ごろをてっていする。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from fugashi import Tagger\n",
    "import unidic   # or unidic_lite\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "tagger = Tagger(f'-d \"{unidic.DICDIR}\"')  # フル UniDic\n",
    "doc = nlp(text)\n",
    "mecab_tokens = list(tagger(text))\n",
    "# → 文字オフセットでアライメントして doc の token に UniDic 情報を付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[日頃, から, の, 日ごろ, を, てってい, する, 。]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fugashi.fugashi.Tagger object at 0x1183bad80>\n"
     ]
    }
   ],
   "source": [
    "print(tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using unidic at: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/unidic/dicdir\n"
     ]
    }
   ],
   "source": [
    "import unidic\n",
    "print(\"Using unidic at:\", unidic.DICDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_len: 29\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(tagger(\"テスト\")))\n",
    "print(\"feature_len:\", len(sample.feature))\n",
    "# 17 = unidic-lite (2.1.2), 29前後 = フル UniDic 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dictionary_info']\n"
     ]
    }
   ],
   "source": [
    "print([a for a in dir(tagger) if 'dic' in a.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available attrs: ['char_type', 'feature', 'feature_raw', 'is_unk', 'length', 'pos', 'posid', 'rlength', 'stat', 'surface', 'white_space']\n"
     ]
    }
   ],
   "source": [
    "import fugashi\n",
    "from fugashi import Tagger\n",
    "\n",
    "tagger = Tagger()  # まずオプションなし\n",
    "m = next(iter(tagger(\"日ごろ\")))\n",
    "print(\"Available attrs:\", [a for a in dir(m) if not a.startswith('_')][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagger repr: <fugashi.fugashi.Tagger object at 0x13f33b5c0>\n",
      "surface: 日ごろ\n",
      "feature_len: 29\n",
      "raw feature: UnidicFeatures29(pos1='名詞', pos2='普通名詞', pos3='副詞可能', pos4='*', cType='*', cForm='*', lForm='ヒゴロ', lemma='日頃', orth='日ごろ', pron='ヒゴロ', orthBase='日ごろ', pronBase='ヒゴロ', goshu='和', iType='*', iForm='*', fType='*', fForm='*', iConType='*', fConType='*', type='体', kana='ヒゴロ', kanaBase='ヒゴロ', form='ヒゴロ', formBase='ヒゴロ', aType='0', aConType='C2', aModType='*', lid='8605061500510720', lemma_id='31305')\n"
     ]
    }
   ],
   "source": [
    "import fugashi\n",
    "t = fugashi.Tagger()\n",
    "print(\"Tagger repr:\", t)   # ここに 'ipa' や 'unidic' などヒントが出ることが多い\n",
    "\n",
    "w = next(iter(t(\"日ごろ\")))\n",
    "print(\"surface:\", w.surface)\n",
    "print(\"feature_len:\", len(w.feature))\n",
    "print(\"raw feature:\", w.feature)          # まず 1語分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linguistic-data-analysis-I",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

---
title: "Session 5: Hands-on activity #2"
subtitle: "Frequency list"
format: revealjs
metadata-files:
  - _slides.yml
  - ../../_metadata.yml
---


# Housekeeping


---


## üéØ Learning Objectives

By the end of this session, you will be able to:

> - Compute frequency of a single-word lexical item in reference corpora
> - Derive vocabulary frequency list using concordancing software (e.g., AntConc)
> - Apply tokenization on the Japanese language corpus for frequency analysis
> - Conduct Lexical Profiling using a web-application or desktop application (e.g., AntWordProfiler)

---

## Tasks

1. Loading a corpus to AntConc
2. Creating a frequency list
3. Visualize frequency distributions
4. Tokenize non-English language
5. (If time left) Vocabulary profiling



## Introduction

- AntConc is free concordancing tool.
- Developed by Laurence ANTHONY.
 

---

## Hands-on Activity 

## Task 1: Loading a corpus to AntConc

## Open AntConc

![AntConc](../../assets/session-5/AntConc-Landing.png)

## AntConc window

![AntConc2](../../assets/session-5/AntConc-menu.png)


## Load a corpus

Now, let's load a corpus.

- We will use BROWN Corpus.

![Load-corpus](../../assets/session-5/Loading-corpus.gif)



---

## Task 1: Creating a frequency list

## Word
Let's now create a frequency list 

1. Select `Word` analysis option

2. Set `Min. Freq` and `Min. Range`
  - Min. Freq = the number of times the word should occur in the corpus
  - Min. Range = the number of files in which the word should occur

3. Hit `Start`

## Let's try

- Set min. frequency = 3; min. range = 3
![Load-corpus](../../assets/session-5/Word-list.gif)

## Saving the frequency list

- From `File` hit `save the current results`

![save-list](../../assets/session-5/save-freq-list.gif)

## Frequency list

- We will use the BROWN frequency list in the next session.

![save-list](../../assets/session-5/Freq-list.png)


## Task 2: Plot frequencies

- Let's now understand the distributions of words in language.
- Visit our [simple-text-analyzer tool](https://huggingface.co/spaces/egumasa/simple-text-analyzer).
- Hit Frequency analysis and upload the frequency list.
- What did you notice?

## Frequency Plot

- Very few words occupy most of the corpus.

![BROWN frequency](../../assets/session-5/Frequency-plot.png)


## Discussion 

- What did you notice in the plot? What are the characteristics of words?
- What are the characteristics of frequent words? What about infrequent words?

---

## Task 3: Tokenizing non-English languages for frequency analysis

- Up to this point, we only dealt with English.
- English is very convenient in corpus analysis because of the white spaces.
- Asian languages have completely different writing system from Indo-European language, and it makes it difficult to `tokenize` texts in to words.
  
  ```
  I am planning to eat Oysters after this intensive course.
  ```
  ```
  „Åì„ÅÆÁü≠ÊúüÈõÜ‰∏≠Ë¨õÂ∫ß„ÅåÁµÇ„Çè„Å£„Åü„Çâ„ÄÅ„Ç´„Ç≠„ÇíÈ£ü„Åπ„Åü„ÅÑ„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ
  ```


## TagAnt

- `Tokenization` (segmenting running text into words) needs more advanced statistical algorithms.
- TagAnt is a free tool (again developped by Laurence ANTHONY).
- It uses modern natural language processing tool (called spaCy) to tokenize input texts.


## Tokenizing Japanese

- Download and open `TagAnt`.
![TagAnt sections](../../assets/session-5/TagAnt-sections.png)

## Tokenizing Japanese

- Input text 
- Select language.
- Select Output format.


## Result of TagAnt segmentation

:::: {.columns}

::: {.column width="50%"}

![Horizontal display](../../assets/session-5/TagAnt-Horizontal.png)

:::

::: {.column width="50%"}

![Vertical display](../../assets/session-5/TagAnt-vertical.png)

:::

::::

- TagAnt can do more than this. 
- We will come back to POS tagging on Day 4.


## Compiling Japanese Word Frequency list

### Task
Compile a Japanese frequency list based on a corpus.

### Resource
- Download a Japanese text `Aozora 500` from Google Drive.
- Use AntConc and TagAnt.

### Submission

- Submit a frequency list
- Description of word frequency pattern in Japanese.

---

## Task 4: Vocabulary Profiling

## Frequency information

- We are familiar with the notion of frequency in language.
- Frequency is one important driving force of human cognition and language processing.

- Profiling text using corpus frequency information is one important approach to understand textual characteristics.


## Vocabulary Profiling

- Vocabulary profiling is a technique to use corpus frequency to understand characteristics of vocabulary use in the input text
- For lexical sophistication measure
  - How much of the learner produced vocabulary is e.g., beyond 2000 word level (LFP; Laufer and Nation, 1995) ?

- For lexical coverage
  - How many words do readers/listeners need to know in order to comprehend the text (90, 95, or 98% coverage)

## Vocabulary Profiling tools

- RANGE program
- [VocabProfiler in LexTutor](https://www.lextutor.ca/vp/)
- [New Word Levels Checker](https://nwlc.pythonanywhere.com/)
- AntWordProfiler 

---
title: "Session 5: Hands-on activity #2"
subtitle: "Frequency list"
format: revealjs
metadata-files:
  - _slides.yml
  - ../../_metadata.yml
---


# Housekeeping


---


## ğŸ¯ Learning Objectives

By the end of this session, you will be able to:

> - Compute frequency of a single-word lexical item in reference corpora
> - Derive vocabulary frequency list using concordancing software (e.g., AntConc)
> - Apply tokenization on the Japanese language corpus for frequency analysis
> - Conduct Lexical Profiling using a web-application or desktop application (e.g., AntWordProfiler)

---

## Tasks

1. Loading a corpus to AntConc
2. Creating a frequency list
3. Visualize frequency distributions
4. Tokenize non-English language
5. (If time left) Vocabulary profiling 

---

# Task 1: Loading a corpus to AntConc

## Open AntConc

![AntConc](../../assets/session-5/AntConc-Landing.png)

## AntConc window

![AntConc2](../../assets/session-5/AntConc-menu.png)


## Load a corpus

Now, let's load a corpus.

- We will use BROWN Corpus.

![Load-corpus](../../assets/session-5/Loading-corpus.gif)


---

# Task 2: Creating a frequency list {.center} 


## Word Frequency

Let's now create a word frequency list from a corpus

1. Select `Word` analysis option

2. Set `Min. Freq` and `Min. Range`
  - Min. Freq = the number of times the word should occur in the corpus
  - Min. Range = the number of files in which the word should occur

3. Hit `Start`

## Let's try

- Set min. frequency = 3; min. range = 3
![Load-corpus](../../assets/session-5/Word-list.gif)

## Saving the frequency list

- From `File` hit `save the current results`

![save-list](../../assets/session-5/save-freq-list.gif)

## Frequency list

- We will use the BROWN frequency list in the next session.


# Task 3: Plot frequencies

## Visualizing frequency distributions

- Let's now understand the distributions of words in BROWN corpus.
- Visit our [simple-text-analyzer tool](https://huggingface.co/spaces/egumasa/simple-text-analyzer).
- Hit Frequency analysis and upload the frequency list.


## Frequency Plot

::: {style = "font-size:70%"}
- X-Axis = Group of 500/1,000/2,000 words
- Y-Axis = Average Frequency across all words in the band
:::

![BROWN frequency](../../assets/session-5/Frequency-plot.png)


## Discussion 

- What do you learn from the previous bar graph?
- For each frequency band, what are the characteristics of the sample words you see? 


# Task 4: Creating frequency lists of Japanese {.center}

## So far...

- Up to this point, we only dealt with the English language.
- Let's try doing the same for Japanese.
- But we need to do something extra... 
- Can you guess what that is?

::: {.incremental}
- I am planning to eat Oysters after this intensive course.
- ã“ã®çŸ­æœŸé›†ä¸­è¬›åº§ãŒçµ‚ã‚ã£ãŸã‚‰ã€ã‚«ã‚­ã‚’é£Ÿã¹ãŸã„ã¨æ€ã£ã¦ã„ã¾ã™ã€‚
:::


## Tokenization

- English is very convenient in corpus analysis because of the white spaces.
- Asian languages have completely different writing system from Indo-European language, and it makes it difficult to `tokenize` texts into words.
  
**English text**

  ```
  I am planning to eat Oysters after this intensive course.
  ```

**Japanese text**

  ```
  ã“ã®çŸ­æœŸé›†ä¸­è¬›åº§ãŒçµ‚ã‚ã£ãŸã‚‰ã€ã‚«ã‚­ã‚’é£Ÿã¹ãŸã„ã¨æ€ã£ã¦ã„ã¾ã™ã€‚
  ```

## Tokenization

- `Tokenization` =  segmenting running text into words 
- It needs more advanced statistical algorithms for Asian languages.
  
  - How would you chunk these

  ```
  ã“ã®çŸ­æœŸé›†ä¸­è¬›åº§ãŒçµ‚ã‚ã£ãŸã‚‰ã€ã‚«ã‚­ã‚’é£Ÿã¹ãŸã„ã¨æ€ã£ã¦ã„ã¾ã™ã€‚
  ```

## Tokenization with TagAnt

- TagAnt is a free tool (again developped by Laurence ANTHONY) to conduct tokenization (and POS tagging).
- It uses modern natural language processing tool (called spaCy) to tokenize input texts.


## Tokenizing Japanese

- Download and open `TagAnt`.
![TagAnt sections](../../assets/session-5/TagAnt-sections.png)

## Tokenizing Japanese

- Input text 
- Select language.
- Select Output format.


## Result of TagAnt segmentation

You can choose from two output formats. 


:::: {.columns}

::: {.column width="50%"}

![Horizontal display](../../assets/session-5/TagAnt-Horizontal.png)

:::

::: {.column width="50%"}

![Vertical display](../../assets/session-5/TagAnt-vertical.png)

:::

::::

## Part-Of-Speech Tagging in TagAnt

- TagAnt can do more than tokenization. 

- It allows you to automatically annotate the token for Part-Of-Speech (POS).

- POS = Grammatical category of lexical items (NOUN, VERB, etc.)

## Choosing the right format for POS representation

- You can ask TagAnt for different output formats.

- For now, let's choose `word+POS`.

::: {.incremental}

- ```ã“ã®çŸ­æœŸé›†ä¸­è¬›åº§ãŒçµ‚ã‚ã£ãŸã‚‰ã€ã‚«ã‚­ã‚’é£Ÿã¹ãŸã„ã¨æ€ã£ã¦ã„ã¾ã™ã€‚```

- ```ã“ã®_DET çŸ­æœŸ_NOUN é›†ä¸­_NOUN è¬›åº§_NOUN ãŒ_ADP çµ‚ã‚ã£_VERB ãŸã‚‰_AUX ã€_PUNCT ã‚«ã‚­_NOUN ã‚’_ADP é£Ÿã¹_VERB ãŸã„_AUX ã¨_ADP æ€ã£_VERB ã¦_SCONJ ã„_VERB ã¾ã™_AUX ã€‚_PUNCT```

:::

## Now try different options...

- Let's analyze your own example with the following:
  - `word+pos`
  - `word+pos_tag`
  - `word+lemma`
  - `word+pos+lemma`
- What do these choice give you? Share it with your neighbor.

## Other choices and expected results

| Display Information | Example |
|-----|------------|
| word | `ã‚«ã‚­ ã‚’ é£Ÿã¹ ãŸã„` |
| word+pos | `ã‚«ã‚­_NOUN ã‚’_ADP é£Ÿã¹_VERB ãŸã„_AUX` |
| word+pos_tag | `ã‚«ã‚­_åè©-æ™®é€šåè©-ä¸€èˆ¬ ã‚’_åŠ©è©-æ ¼åŠ©è© é£Ÿã¹_å‹•è©-ä¸€èˆ¬ ãŸã„_åŠ©å‹•è©` |
| word+lemma | `ã‚«ã‚­_ã‚«ã‚­ ã‚’_ã‚’ é£Ÿã¹_é£Ÿã¹ã‚‹ ãŸã„_ãŸã„` |

## Questions? {.center}


# Corpus Lab 2: Task 1 {.center}

::: {style = "color:gray"}
Instruction
:::

## Task 1: Compile a Japanese Word Frequency list

::: {style = "font-size:80%"}

### Task
Compile a Japanese frequency list based on a corpus.

### Resource
- Download a Japanese text `Aozora 500` from Google Drive.
- Use AntConc, TagAnt, and Simple Text Analyzer.

### Submission

- Submit a frequency list `.tsv` or `.txt`.
- Description of word frequency pattern in Japanese.

:::

::: {.callout-important}
# Success Criteria

Your submission ...

- [ ] includes a frequency list of Japanese words based on Aozora 500
- [ ] provides a description of word-frequency patterns in Aozora 500 using example words from each frequency bins

:::


---

# Task 4: Vocabulary Profiling

**Due to possible time limitation, let's come back to this if we have time at the end of session 6.**

<!-- ## Frequency information

- Frequency is one important driving force of human cognition and language processing.

- Profiling is one important approach to understand textual characteristics.
  - LFP (Laufer & Nation, 1995)
  - Lexical coverage  -->


## Vocabulary Profiling

- Vocabulary profiling is a technique to use corpus frequency to understand characteristics of vocabulary use in the input text
- For lexical sophistication measure
  - LFP, Beyond 2000

- For lexical coverage
  - How many words do readers/listeners need to know in order to comprehend the text (90, 95, or 98% coverage)

## Vocabulary Profiling tools

- RANGE program
- [VocabProfiler in LexTutor](https://www.lextutor.ca/vp/)
- AntWordProfiler (desktop; multi-language)
- [New Word Levels Checker](https://nwlc.pythonanywhere.com/)


<!-- ## Using AntWordProfiler for simple lexical profiling -->


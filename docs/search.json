[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linguistic Data Analysis I",
    "section": "",
    "text": "Under Construction\n\n\n\nThis course website is currently under construction and will be ready for the class starting August 2nd, 2025. Content is being actively developed and updated."
  },
  {
    "objectID": "index.html#welcome-to-linguistic-data-analysis-i",
    "href": "index.html#welcome-to-linguistic-data-analysis-i",
    "title": "Linguistic Data Analysis I",
    "section": "Welcome to Linguistic Data Analysis I",
    "text": "Welcome to Linguistic Data Analysis I\nThis intensive 5-day graduate course introduces students to corpus linguistics and learner language analysis. Through hands-on activities and practical applications, youâ€™ll learn to use computational tools to analyze linguistic data, with a special focus on learner corpora.\n\n\n\n\n\n\nQuick Links\n\n\n\n\nğŸ“‹ Course Syllabus\nğŸ“… Schedule\nğŸ’» Sessions\nğŸ“ Assignments\nğŸ”§ Resources"
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Linguistic Data Analysis I",
    "section": "Course Overview",
    "text": "Course Overview\n\n\n\nWhat Youâ€™ll Learn\n\nCorpus analysis techniques\nLearner language analysis methods\nPractical applications with real corpora\nResearch methodology in corpus linguistics\n\n\n\n\nKey Tools\n\nAntConc - Corpus analysis software\nBYU Corpora - Online corpus interfaces\nPython - Text processing (via Google Colab)\nJASP - Statistical analysis"
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Linguistic Data Analysis I",
    "section": "Course Structure",
    "text": "Course Structure\nThe course is organized into 5 days:\n\n\n\nDay\nTheme\nSessions\n\n\n\n\nDay 1\nIntroduction & Corpus Basics\n\n\n\nDay 2\nAnalysis of Vocabulary & Multiword Units (1)\n\n\n\nDay 3\nAnalysis of Vocabulary & Multiword Units (2)\n\n\n\nDay 4\nAnalysis of Grammar\n\n\n\nDay 5\nAdvanced Topics & Projects"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Linguistic Data Analysis I",
    "section": "Getting Started",
    "text": "Getting Started\n\nReview the syllabus for course policies and expectations\nCheck the detailed schedule for session timings\nInstall required software using our setup guides\nBrowse the resources section for helpful materials"
  },
  {
    "objectID": "index.html#instructor-information",
    "href": "index.html#instructor-information",
    "title": "Linguistic Data Analysis I",
    "section": "Instructor Information",
    "text": "Instructor Information\nInstructor: Masaki Eguchi, Ph.D.\nEmail: You can contact me through Google Classroom"
  },
  {
    "objectID": "index.html#course-communication",
    "href": "index.html#course-communication",
    "title": "Linguistic Data Analysis I",
    "section": "Course Communication",
    "text": "Course Communication\n\n\n\n\n\n\nStay Connected\n\n\n\n\nCourse Website: This site\nCommunication: Google Classroom\nAssignment Submission: Google Classroom"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Linguistic Data Analysis I",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis course builds on materials and approaches from:\n\nDr.Â Kris Kyle (University of Oregon) for his previous corpus linguistics/NLP classes from University of Hawaiâ€™i and Oregon.\nDr.Â Andrew Heiss (Georgia State University) for his Quarto-based materials and website settings, which significantly enhanced the accessibility of the course content."
  },
  {
    "objectID": "resources/corpora/learner-corpora.html",
    "href": "resources/corpora/learner-corpora.html",
    "title": "Learner Corpora",
    "section": "",
    "text": "Content to be added.\n\nJapanese Learner corpus Natane\nInternational Center for Japanese Studies\næ—¥æœ¬èªå­¦ç¿’è€…ä¼šè©±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
    "crumbs": [
      "Resources",
      "Corpora",
      "Learner Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/learner-corpora.html#placeholder",
    "href": "resources/corpora/learner-corpora.html#placeholder",
    "title": "Learner Corpora",
    "section": "",
    "text": "Content to be added.\n\nJapanese Learner corpus Natane\nInternational Center for Japanese Studies\næ—¥æœ¬èªå­¦ç¿’è€…ä¼šè©±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
    "crumbs": [
      "Resources",
      "Corpora",
      "Learner Corpora"
    ]
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Course resources organized by category.",
    "crumbs": [
      "Resources",
      "Resources"
    ]
  },
  {
    "objectID": "resources/index.html#overview",
    "href": "resources/index.html#overview",
    "title": "Resources",
    "section": "",
    "text": "Course resources organized by category.",
    "crumbs": [
      "Resources",
      "Resources"
    ]
  },
  {
    "objectID": "resources/index.html#categories",
    "href": "resources/index.html#categories",
    "title": "Resources",
    "section": "Categories",
    "text": "Categories\n\nTools and Software\nCorpora\nCode Examples",
    "crumbs": [
      "Resources",
      "Resources"
    ]
  },
  {
    "objectID": "resources/tools/index.html",
    "href": "resources/tools/index.html",
    "title": "Tools and Software",
    "section": "",
    "text": "BYU Corpora Guide\nAntConc Guide\nPython (google colab) Setup\nJASP Guide",
    "crumbs": [
      "Resources",
      "Tools",
      "Tools and Software"
    ]
  },
  {
    "objectID": "resources/tools/index.html#available-guides",
    "href": "resources/tools/index.html#available-guides",
    "title": "Tools and Software",
    "section": "",
    "text": "BYU Corpora Guide\nAntConc Guide\nPython (google colab) Setup\nJASP Guide",
    "crumbs": [
      "Resources",
      "Tools",
      "Tools and Software"
    ]
  },
  {
    "objectID": "resources/tools/index.html#other-useful-corpus-resources",
    "href": "resources/tools/index.html#other-useful-corpus-resources",
    "title": "Tools and Software",
    "section": "Other useful corpus resources",
    "text": "Other useful corpus resources\n\nCIABATTTA\nCorpus and Repository of Writing",
    "crumbs": [
      "Resources",
      "Tools",
      "Tools and Software"
    ]
  },
  {
    "objectID": "resources/tools/antconc-guide.html",
    "href": "resources/tools/antconc-guide.html",
    "title": "AntConc Guide",
    "section": "",
    "text": "We will use AntConc, a one of the most widely used corpus tool developed by Laurence ANTHONY (Waseda University).\nThanksfully, Laurence has shared tutorials on basic features of AntConc on Youtube.\nYou will need AntConc on Day 2 and 3.\nPlease complete the following steps before Day 2.\n\nDownload AntConc\n\n\nVisit AntConc Website; download the software to your computer.\n\n\nWatch the following tutorial videos\n\n\nLaurence Anthonyâ€™s intro to AntConc\n\nGetting started (10 mins)\nCorpus manager Basics (18 mins)\nKWIC tool basics (17 mins)\n\n\nIf you are unsure about these steps, do not hesitate to reach out to me through google classroom or through email.",
    "crumbs": [
      "Resources",
      "Tools",
      "AntConc Guide"
    ]
  },
  {
    "objectID": "resources/tools/antconc-guide.html#overview",
    "href": "resources/tools/antconc-guide.html#overview",
    "title": "AntConc Guide",
    "section": "",
    "text": "We will use AntConc, a one of the most widely used corpus tool developed by Laurence ANTHONY (Waseda University).\nThanksfully, Laurence has shared tutorials on basic features of AntConc on Youtube.\nYou will need AntConc on Day 2 and 3.\nPlease complete the following steps before Day 2.\n\nDownload AntConc\n\n\nVisit AntConc Website; download the software to your computer.\n\n\nWatch the following tutorial videos\n\n\nLaurence Anthonyâ€™s intro to AntConc\n\nGetting started (10 mins)\nCorpus manager Basics (18 mins)\nKWIC tool basics (17 mins)\n\n\nIf you are unsure about these steps, do not hesitate to reach out to me through google classroom or through email.",
    "crumbs": [
      "Resources",
      "Tools",
      "AntConc Guide"
    ]
  },
  {
    "objectID": "resources/code-examples/python/dl_aozorabunko.html#lets-download-the-data-from-huggingface",
    "href": "resources/code-examples/python/dl_aozorabunko.html#lets-download-the-data-from-huggingface",
    "title": "Downloading Aozorabunko data and make it a usable corpus",
    "section": "Letâ€™s download the data from Huggingface",
    "text": "Letâ€™s download the data from Huggingface\nWe will use the data from the following repository\n\n\nShow code\nimport pandas as pd\n\ndf = pd.read_json(\"hf://datasets/globis-university/aozorabunko-clean/aozorabunko-dedupe-clean.jsonl.gz\", lines=True)\n\n\n\n\nShow code\ndf\n\n\n\n\n\n\n\n\n\ntext\nfootnote\nmeta\n\n\n\n\n0\næ·±ã„ãŠã©ã‚ãã«ã†ãŸã‚Œã¦ã€\\nåé«˜ã„ã‚¦ã‚§ã‚¹ãƒˆãƒŸãƒ³ã‚¹ã‚¿ãƒ¼ã«\\nçœŸé®ã‚„çŸ³ã®è¨˜å¿µç¢‘ã¨ãªã£ã¦\\nã™ã¹ã¦...\nåº•æœ¬ï¼šã€Œã‚¹ã‚±ãƒƒãƒãƒ»ãƒ–ãƒƒã‚¯ã€æ–°æ½®æ–‡åº«ã€æ–°æ½®ç¤¾\\nã€€ã€€ã€€1957ï¼ˆæ˜­å’Œ32ï¼‰å¹´5æœˆ20æ—¥ç™ºè¡Œ\\n...\n{'ä½œå“ID': '059898', 'ä½œå“å': 'ã‚¦ã‚§ã‚¹ãƒˆãƒŸãƒ³ã‚¹ã‚¿ãƒ¼å¯ºé™¢', 'ä½œå“åèª­...\n\n\n1\nã„ã–ã€ã“ã‚Œã‚ˆã‚Šæ¨‚ã—ã¾ã‚€ã€\\nä»•ç½®ã‚’å—ãã‚‹æ†‚ãªãã€\\néŠã³ãŸã®ã—ã‚€æ™‚ãä¾†ã¬ã€\\næ™‚ãä¾†ã¬ã‚Œã°ã€...\nåº•æœ¬ï¼šã€Œã‚¹ã‚±ãƒƒãƒãƒ»ãƒ–ãƒƒã‚¯ã€å²©æ³¢æ–‡åº«ã€å²©æ³¢æ›¸åº—\\nã€€ã€€ã€€1935ï¼ˆæ˜­å’Œ10ï¼‰å¹´9æœˆ15æ—¥ç¬¬1åˆ·...\n{'ä½œå“ID': '056078', 'ä½œå“å': 'é§…ä¼é¦¬è»Š', 'ä½œå“åèª­ã¿': 'ãˆã...\n\n\n2\nã™ã¹ã¦ã‚ˆã—ã€‚\\nä½•ã—ã¦éŠã¼ã¨\\nå±ã‚‰ã‚Œãªã„ã€‚\\næ™‚ã¯ããŸã€‚\\nã•ã£ã•ã¨\\næœ¬ãªã©æŠ•ã’ã ãã†ã€‚...\nåº•æœ¬ï¼šã€Œã‚¹ã‚±ãƒƒãƒãƒ»ãƒ–ãƒƒã‚¯ã€æ–°æ½®æ–‡åº«ã€æ–°æ½®ç¤¾\\nã€€ã€€ã€€1957ï¼ˆæ˜­å’Œ32ï¼‰å¹´5æœˆ20æ—¥ç™ºè¡Œ\\n...\n{'ä½œå“ID': '060224', 'ä½œå“å': 'é§…é¦¬è»Š', 'ä½œå“åèª­ã¿': 'ãˆãã°...\n\n\n3\nå¹´è€ã„ãŸäººã‚’ã„ãŸã‚ã‚Šãªã•ã„ã€‚ãã®éŠ€é«ªã¯ã€\\nåèª‰ã¨å°Šæ•¬ã‚’ã¤ã­ã«é›†ã‚ã¦ããŸã®ã§ã™ã€‚\\nâ€•â€•ãƒãƒ¼...\nåº•æœ¬ï¼šã€Œã‚¹ã‚±ãƒƒãƒãƒ»ãƒ–ãƒƒã‚¯ã€æ–°æ½®æ–‡åº«ã€æ–°æ½®ç¤¾\\nã€€ã€€ã€€1957ï¼ˆæ˜­å’Œ32ï¼‰å¹´5æœˆ20æ—¥ç™ºè¡Œ\\n...\n{'ä½œå“ID': '060225', 'ä½œå“å': 'å¯¡å©¦ã¨ãã®å­', 'ä½œå“åèª­ã¿': '...\n\n\n4\nã ãŒã€ã‚ã®ãªã¤ã‹ã—ã„ã€æ€ã„å‡ºãµã‹ã„ã‚¯ãƒªã‚¹ãƒã‚¹ã®ãŠçˆºã•ã‚“ã¯ã‚‚ã†é€ã£ã¦ã—ã¾ã£ãŸã®ã ã‚ã†ã‹ã€‚ã‚ã¨...\nåº•æœ¬ï¼šã€Œã‚¹ã‚±ãƒƒãƒãƒ»ãƒ–ãƒƒã‚¯ã€æ–°æ½®æ–‡åº«ã€æ–°æ½®ç¤¾\\nã€€ã€€ã€€1957ï¼ˆæ˜­å’Œ32ï¼‰å¹´5æœˆ20æ—¥ç™ºè¡Œ\\n...\n{'ä½œå“ID': '060231', 'ä½œå“å': 'ã‚¯ãƒªã‚¹ãƒã‚¹', 'ä½œå“åèª­ã¿': 'ã‚¯...\n\n\n...\n...\n...\n...\n\n\n16946\nå¤¢ã®è©±ã‚’ã™ã‚‹ã®ã¯ã‚ã¾ã‚Šæ°—ã®ãã„ãŸã“ã¨ã§ã¯ãªã„ã€‚ç¢ºã‹ç—´äººå¤¢ã‚’èª¬ãã¨ã„ã†è¨€è‘‰ãŒã‚ã£ãŸã¯ãšã ã€‚ã...\nåº•æœ¬ï¼šã€Œå’Œè¾»å“²éƒå…¨é›†ã€€è£œéºã€å²©æ³¢æ›¸åº—\\nã€€ã€€ã€€1978ï¼ˆæ˜­å’Œ53ï¼‰å¹´6æœˆ16æ—¥ç¬¬1åˆ·ç™ºè¡Œ\\n...\n{'ä½œå“ID': '055622', 'ä½œå“å': 'å¤¢', 'ä½œå“åèª­ã¿': 'ã‚†ã‚', ...\n\n\n16947\nè‡ªåˆ†ã¯ç¾ä»£ã®ç”»å®¶ä¸­ã«å²¸ç”°å›ã»ã©æ˜ã‚‰ã‹ãªã€Œæˆé•·ã€ã‚’ç¤ºã—ã¦ã„ã‚‹äººã‚’çŸ¥ã‚‰ãªã„ã€‚èª‡å¼µã§ãªãå²¸ç”°å›ã¯...\nåº•æœ¬ï¼šã€Œå’Œè¾»å“²éƒéšç­†é›†ã€å²©æ³¢æ–‡åº«ã€å²©æ³¢æ›¸åº—\\nã€€ã€€ã€€1995ï¼ˆå¹³æˆ7ï¼‰å¹´9æœˆ18æ—¥ç¬¬1åˆ·ç™ºè¡Œ...\n{'ä½œå“ID': '049876', 'ä½œå“å': 'ã€åŠ‰ç”Ÿç”»é›†åŠèŠ¸è¡“è¦³ã€ã«ã¤ã„ã¦', 'ä½œ...\n\n\n16948\nä¸€\\n\\nã€€è’æ¼ ãŸã‚‹ç§‹ã®é‡ã«ç«‹ã¤ã€‚æ˜Ÿã¯æœˆã®å¾¡åº§ã‚’å›²ã¿æœˆã¯æ¸…ã‚‰ã‹ã«åœ°ã®èŠ±ã‚’è¼ã‚‰ã™ã€‚èŠ±ã¯ç´…ã¨å’²ã...\nåº•æœ¬ï¼šã€Œå¶åƒå†èˆˆãƒ»é¢ã¨ãƒšãƒ«ã‚½ãƒŠã€€å’Œè¾»å“²éƒæ„Ÿæƒ³é›†ã€è¬›è«‡ç¤¾æ–‡èŠ¸æ–‡åº«ã€è¬›è«‡ç¤¾\\nã€€ã€€ã€€2007ï¼ˆå¹³...\n{'ä½œå“ID': '049913', 'ä½œå“å': 'éœŠçš„æœ¬èƒ½ä¸»ç¾©', 'ä½œå“åèª­ã¿': '...\n\n\n16949\né–¢æ±å¤§éœ‡ç½ã®å‰æ•°å¹´ã®é–“ã€å…ˆè¼©ãŸã¡ã«ã¾ã˜ã£ã¦éœ²ä¼´å…ˆç”Ÿã‹ã‚‰ä¿³è«§ã®æŒ‡å°ã‚’ã†ã‘ãŸã“ã¨ãŒã‚ã‚‹ã€‚ãã®æ™‚...\nåº•æœ¬ï¼šã€Œå’Œè¾»å“²éƒéšç­†é›†ã€å²©æ³¢æ–‡åº«ã€å²©æ³¢æ›¸åº—\\nã€€ã€€ã€€1995ï¼ˆå¹³æˆ7ï¼‰å¹´9æœˆ18æ—¥ç¬¬1åˆ·ç™ºè¡Œ...\n{'ä½œå“ID': '049914', 'ä½œå“å': 'éœ²ä¼´å…ˆç”Ÿã®æ€ã„å‡º', 'ä½œå“åèª­ã¿':...\n\n\n16950\nè¨³è€…åº\\n\\nã€€ä¸€ä¹ã€‡ä¹å¹´ã€ãƒ¬ã‚ªãƒ³ãƒ»ãƒ¯ãƒ«ãƒ©ã‚¹ã®ä¸ƒåäº”æ­³ã®é½¢ã‚’è¨˜å¿µã—ã¦ã€ãƒ­ãƒ¼ã‚¶ãƒ³ãƒŒå¤§...\nåº•æœ¬ï¼šã€Œç´”ç²¹ç¶“æ¿Ÿå­¸è¦è«–ã€€ä¸Šå·ã€å²©æ³¢æ–‡åº«ã€å²©æ³¢æ›¸åº—\\nã€€ã€€ã€€1953ï¼ˆæ˜­å’Œ28ï¼‰å¹´11æœˆ25æ—¥...\n{'ä½œå“ID': '045210', 'ä½œå“å': 'ç´”ç²‹çµŒæ¸ˆå­¦è¦è«–', 'ä½œå“åèª­ã¿': ...\n\n\n\n\n16951 rows Ã— 3 columns\n\n\n\n\n\nShow code\ndf['meta'][1]\n\n\n{'ä½œå“ID': '056078',\n 'ä½œå“å': 'é§…ä¼é¦¬è»Š',\n 'ä½œå“åèª­ã¿': 'ãˆãã§ã‚“ã°ã—ã‚ƒ',\n 'ã‚½ãƒ¼ãƒˆç”¨èª­ã¿': 'ãˆãã¦ã‚“ã¯ã—ã‚„',\n 'å‰¯é¡Œ': '',\n 'å‰¯é¡Œèª­ã¿': '',\n 'åŸé¡Œ': '',\n 'åˆå‡º': '',\n 'åˆ†é¡ç•ªå·': 'NDC 933',\n 'æ–‡å­—é£ã„ç¨®åˆ¥': 'æ—§å­—æ—§ä»®å',\n 'ä½œå“è‘—ä½œæ¨©ãƒ•ãƒ©ã‚°': 'ãªã—',\n 'å…¬é–‹æ—¥': '2013-09-20',\n 'æœ€çµ‚æ›´æ–°æ—¥': '2014-09-16',\n 'å›³æ›¸ã‚«ãƒ¼ãƒ‰URL': 'https://www.aozora.gr.jp/cards/001257/card56078.html',\n 'äººç‰©ID': '001257',\n 'å§“': 'ã‚¢ãƒ¼ãƒ´ã‚£ãƒ³ã‚°',\n 'å': 'ãƒ¯ã‚·ãƒ³ãƒˆãƒ³',\n 'å§“èª­ã¿': 'ã‚¢ãƒ¼ãƒ´ã‚£ãƒ³ã‚°',\n 'åèª­ã¿': 'ãƒ¯ã‚·ãƒ³ãƒˆãƒ³',\n 'å§“èª­ã¿ã‚½ãƒ¼ãƒˆç”¨': 'ã‚ã‚ã†ã„ã‚“ã',\n 'åèª­ã¿ã‚½ãƒ¼ãƒˆç”¨': 'ã‚ã—ã‚“ã¨ã‚“',\n 'å§“ãƒ­ãƒ¼ãƒå­—': 'Irving',\n 'åãƒ­ãƒ¼ãƒå­—': 'Washington',\n 'å½¹å‰²ãƒ•ãƒ©ã‚°': 'è‘—è€…',\n 'ç”Ÿå¹´æœˆæ—¥': '1783-04-03',\n 'æ²¡å¹´æœˆæ—¥': '1859-11-28',\n 'äººç‰©è‘—ä½œæ¨©ãƒ•ãƒ©ã‚°': 'ãªã—',\n 'åº•æœ¬å1': 'ã‚¹ã‚±ãƒƒãƒãƒ»ãƒ–ãƒƒã‚¯',\n 'åº•æœ¬å‡ºç‰ˆç¤¾å1': 'å²©æ³¢æ–‡åº«ã€å²©æ³¢æ›¸åº—',\n 'åº•æœ¬åˆç‰ˆç™ºè¡Œå¹´1': '1935ï¼ˆæ˜­å’Œ10ï¼‰å¹´9æœˆ15æ—¥',\n 'å…¥åŠ›ã«ä½¿ç”¨ã—ãŸç‰ˆ1': '2010ï¼ˆå¹³æˆ22ï¼‰å¹´2æœˆ23æ—¥ç¬¬31åˆ·',\n 'æ ¡æ­£ã«ä½¿ç”¨ã—ãŸç‰ˆ1': '1992ï¼ˆå¹³æˆ4ï¼‰å¹´2æœˆ26æ—¥ç¬¬30åˆ·',\n 'åº•æœ¬ã®è¦ªæœ¬å1': '',\n 'åº•æœ¬ã®è¦ªæœ¬å‡ºç‰ˆç¤¾å1': '',\n 'åº•æœ¬ã®è¦ªæœ¬åˆç‰ˆç™ºè¡Œå¹´1': '',\n 'åº•æœ¬å2': '',\n 'åº•æœ¬å‡ºç‰ˆç¤¾å2': '',\n 'åº•æœ¬åˆç‰ˆç™ºè¡Œå¹´2': '',\n 'å…¥åŠ›ã«ä½¿ç”¨ã—ãŸç‰ˆ2': '',\n 'æ ¡æ­£ã«ä½¿ç”¨ã—ãŸç‰ˆ2': '',\n 'åº•æœ¬ã®è¦ªæœ¬å2': '',\n 'åº•æœ¬ã®è¦ªæœ¬å‡ºç‰ˆç¤¾å2': '',\n 'åº•æœ¬ã®è¦ªæœ¬åˆç‰ˆç™ºè¡Œå¹´2': '',\n 'å…¥åŠ›è€…': 'é›€',\n 'æ ¡æ­£è€…': 'å°æ—ç¹é›„',\n 'ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«URL': 'https://www.aozora.gr.jp/cards/001257/files/56078_ruby_51155.zip',\n 'ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æœ€çµ‚æ›´æ–°æ—¥': '2013-09-03',\n 'ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç¬¦å·åŒ–æ–¹å¼': 'ShiftJIS',\n 'ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ–‡å­—é›†åˆ': 'JIS X 0208',\n 'ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£å›æ•°': '0',\n 'XHTML/HTMLãƒ•ã‚¡ã‚¤ãƒ«URL': 'https://www.aozora.gr.jp/cards/001257/files/56078_51422.html',\n 'XHTML/HTMLãƒ•ã‚¡ã‚¤ãƒ«æœ€çµ‚æ›´æ–°æ—¥': '2013-09-03',\n 'XHTML/HTMLãƒ•ã‚¡ã‚¤ãƒ«ç¬¦å·åŒ–æ–¹å¼': 'ShiftJIS',\n 'XHTML/HTMLãƒ•ã‚¡ã‚¤ãƒ«æ–‡å­—é›†åˆ': 'JIS X 0208',\n 'XHTML/HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£å›æ•°': '0'}\n\n\n\n\nShow code\nimport os\n\n# Set random seed for reproducibility\nrandom_seed = 42\n\n# Sample 1000 files from the dataframe\ndf_sample = df.sample(n=1000, random_state=random_seed)\n\n# Create directory if it doesn't exist\noutput_dir = \"../../../corpus_data/aozora_1000\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save each text to a file named by ä½œå“ID\nfor idx, (_, row) in enumerate(df_sample.iterrows()):\n    # Get the ä½œå“ID from meta field\n    sakuhin_id = row['meta']['ä½œå“ID']\n    \n    # Get the text content\n    text_content = row['text']\n    \n    # Create filename based on ä½œå“ID\n    filename = f\"{sakuhin_id}.txt\"\n    filepath = os.path.join(output_dir, filename)\n    \n    # Write text to file\n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write(text_content)\n    \n    # Print progress every 100 files\n    if (idx + 1) % 100 == 0:\n        print(f\"Processed {idx + 1} files...\")\n\nprint(f\"Completed! Saved {len(df_sample)} text files to {output_dir}\")\n\n\nProcessed 100 files...\nProcessed 200 files...\nProcessed 300 files...\nProcessed 400 files...\nProcessed 500 files...\nProcessed 600 files...\nProcessed 700 files...\nProcessed 800 files...\nProcessed 900 files...\nProcessed 1000 files...\nCompleted! Saved 1000 text files to ../../../corpus_data/aozora_1000\n\n\n\n\nShow code\n# Sample 1000 files from the dataframe\ndf_sample = df.sample(n=50, random_state=random_seed)\n\n# Create directory if it doesn't exist\noutput_dir = \"../../../corpus_data/aozora_50\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save each text to a file named by ä½œå“ID\nfor idx, (_, row) in enumerate(df_sample.iterrows()):\n    # Get the ä½œå“ID from meta field\n    sakuhin_id = row['meta']['ä½œå“ID']\n    \n    # Get the text content\n    text_content = row['text']\n    \n    # Create filename based on ä½œå“ID\n    filename = f\"{sakuhin_id}.txt\"\n    filepath = os.path.join(output_dir, filename)\n    \n    # Write text to file\n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write(text_content)\n    \n    # Print progress every 100 files\n    if (idx + 1) % 100 == 0:\n        print(f\"Processed {idx + 1} files...\")\n\nprint(f\"Completed! Saved {len(df_sample)} text files to {output_dir}\")\n\n\nCompleted! Saved 50 text files to ../../../corpus_data/aozora_50"
  },
  {
    "objectID": "2025/notebooks/session-5.html",
    "href": "2025/notebooks/session-5.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nShow code\nimport spacy\nfrom sudachipy import dictionary, tokenizer\nfrom spacy.tokens import Doc\n\n# 1. Standard pipeline (for reliable POS/DEP):\nstd_nlp = spacy.load(\"ja_core_news_sm\")\n\n# 2. Alternate segmentation pipeline (no tagging):\nsudachi = dictionary.Dictionary().create()\nMODE = tokenizer.Tokenizer.SplitMode.A  # A=short, C=long\n\nalt_nlp = spacy.blank(\"ja\")\n\ndef sudachi_tokenizer_func(text):\n    ms = sudachi.tokenize(text, MODE)\n    words = [m.surface() for m in ms]\n    spaces = [False]*len(words)\n    return Doc(alt_nlp.vocab, words=words, spaces=spaces)\n\nalt_nlp.tokenizer = sudachi_tokenizer_func\n\ntext = \"å½¼ã¯æ˜¨æ—¥æœ¬ã‚’è²·ã£ã¦èª­ã¿å§‹ã‚ã¾ã—ãŸã€‚\"\ndoc_std = std_nlp(text)\ndoc_alt = alt_nlp(text)\n\nprint(\"STD tokens:\", [t.text for t in doc_std])\nprint(\"ALT tokens (mode A):\", [t.text for t in doc_alt])\nprint(\"STD POS:\", [t.pos_ for t in doc_std])\n\n\nSTD tokens: ['å½¼', 'ã¯', 'æ˜¨', 'æ—¥æœ¬', 'ã‚’', 'è²·ã£', 'ã¦', 'èª­ã¿', 'å§‹ã‚', 'ã¾ã—', 'ãŸ', 'ã€‚']\nALT tokens (mode A): ['å½¼', 'ã¯', 'æ˜¨', 'æ—¥æœ¬', 'ã‚’', 'è²·ã£', 'ã¦', 'èª­ã¿', 'å§‹ã‚', 'ã¾ã—', 'ãŸ', 'ã€‚']\nSTD POS: ['PRON', 'ADP', 'NOUN', 'PROPN', 'ADP', 'VERB', 'SCONJ', 'VERB', 'VERB', 'AUX', 'AUX', 'PUNCT']\n\n\n\n\nShow code\nimport spacy\nfrom sudachipy import tokenizer, dictionary\nfrom spacy.tokens import Doc\n\nsudachi = dictionary.Dictionary().create()\nMODE = tokenizer.Tokenizer.SplitMode.C  # change to A for short proxy\n\nnlp = spacy.blank(\"ja\")\n\ndef sudachi_tokenizer_func(text):\n    sudachi_tokens = sudachi.tokenize(text, MODE)\n    words = [m.surface() for m in sudachi_tokens]\n    spaces = [False]*len(words)\n    return Doc(nlp.vocab, words=words, spaces=spaces)\n\n# nlp.tokenizer = sudachi_tokenizer_func\n\ndoc = nlp(\"ä»Šå¹´ã®å¹²æ”¯ã¯åºšå­ã§ã™ã€‚æ±äº¬ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯ãŸã®ã—ã¿ã ãªã‚ã€‚\")\nprint([t.text for t in doc])\nprint([(t.norm_, t.pos_, t.tag_) for t in doc])\n\n\n['ä»Šå¹´', 'ã®', 'å¹²æ”¯', 'ã¯', 'åºšå­', 'ã§ã™', 'ã€‚', 'æ±äº¬', 'ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯', 'ãŸã®ã—', 'ã¿', 'ã ', 'ãªã‚', 'ã€‚']\n[('ä»Šå¹´', 'NOUN', 'åè©-æ™®é€šåè©-å‰¯è©å¯èƒ½'), ('ã®', 'ADP', 'åŠ©è©-æ ¼åŠ©è©'), ('å¹²æ”¯', 'NOUN', 'åè©-æ™®é€šåè©-ä¸€èˆ¬'), ('ã¯', 'ADP', 'åŠ©è©-ä¿‚åŠ©è©'), ('åºšå­', 'NOUN', 'åè©-æ™®é€šåè©-ä¸€èˆ¬'), ('ã§ã™', 'AUX', 'åŠ©å‹•è©'), ('ã€‚', 'PUNCT', 'è£œåŠ©è¨˜å·-å¥ç‚¹'), ('æ±äº¬', 'PROPN', 'åè©-å›ºæœ‰åè©-åœ°å-ä¸€èˆ¬'), ('ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯', 'NOUN', 'åè©-æ™®é€šåè©-ä¸€èˆ¬'), ('æ¥½ã—ã„', 'ADJ', 'å½¢å®¹è©-ä¸€èˆ¬'), ('å‘³', 'PART', 'æ¥å°¾è¾-åè©çš„-ä¸€èˆ¬'), ('ã ', 'AUX', 'åŠ©å‹•è©'), ('ãª', 'PART', 'åŠ©è©-çµ‚åŠ©è©'), ('ã€‚', 'PUNCT', 'è£œåŠ©è¨˜å·-å¥ç‚¹')]\n\n\n\n\nShow code\nimport spacy\nnlp = spacy.load(\"ja_ginza\")\ndoc = nlp(\"ã²ã”ã‚ æ—¥ã”ã‚ æ—¥é ƒ å‘‘ã¿ å‘‘ã‚“ã§ é£²ã‚“ã§ æ›¸ãã‚ã‚‰ã‚ã™\")\nfor tok in doc:\n    print(tok.text, tok.lemma_)  # lemma_ ~ Sudachi dictionary form\n\n\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:75: SyntaxWarning: invalid escape sequence '\\s'\n  relative_imports = re.findall(\"^\\s*import\\s+\\.(\\S+)\\s*$\", content, flags=re.MULTILINE)\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:77: SyntaxWarning: invalid escape sequence '\\s'\n  relative_imports += re.findall(\"^\\s*from\\s+\\.(\\S+)\\s+import\", content, flags=re.MULTILINE)\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:119: SyntaxWarning: invalid escape sequence '\\s'\n  imports = re.findall(\"^\\s*import\\s+(\\S+)\\s*$\", content, flags=re.MULTILINE)\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:121: SyntaxWarning: invalid escape sequence '\\s'\n  imports += re.findall(\"^\\s*from\\s+(\\S+)\\s+import\", content, flags=re.MULTILINE)\n\n\nã²ã”ã‚ ã²ã”ã‚\næ—¥ã”ã‚ æ—¥ã”ã‚\næ—¥é ƒ æ—¥é ƒ\nå‘‘ã¿ å‘‘ã‚€\nå‘‘ã‚“ å‘‘ã‚€\nã§ ã§\né£²ã‚“ é£²ã‚€\nã§ ã§\næ›¸ãã‚ã‚‰ã‚ã™ æ›¸ãã‚ã‚‰ã‚ã™\n\n\n\n\nShow code\nimport spacy\nfrom spacy.tokens import Doc\nfrom fugashi import Tagger\n\n# Initialize UniDic Tagger\ntagger = Tagger()   # fugashi auto-loads UniDic if installed\n\nnlp = spacy.blank(\"ja\")        # blank Japanese pipeline\n\n# Register custom token extensions\nfrom spacy.tokens import Token\nToken.set_extension(\"unidic_lemma\", default=None, force=True)\nToken.set_extension(\"unidic_reading\", default=None, force=True)\nToken.set_extension(\"unidic_pos\", default=None, force=True)\nToken.set_extension(\"unidic_feats\", default=None, force=True)\n\ndef mecab_tokenizer(text):\n    words = []\n    spaces = []\n    lemmas = []\n    analyses = tagger(text)\n    for m in analyses:\n        surface = m.surface\n        words.append(surface)\n        spaces.append(False)  # Japanese generally no spaces\n    doc = Doc(nlp.vocab, words=words, spaces=spaces)\n    # Attach UniDic info\n    for tok, m in zip(doc, analyses):\n        # m.feature: tuple with UniDic columns. Structure depends on UniDic version.\n        # Typical indices (verify!) e.g. lemma at feature[10], reading at feature[9].\n        \n        feats = m.feature\n        # Safer: use fugashi attribute helpers\n        tok._.unidic_lemma = m.feature[10]  # or m.feature[10]\n        tok._.unidic_reading = m.feature[10]  # unified katakana reading\n        tok._.unidic_pos = \",\".join(m.feature[:4])  # hierarchical POS tuple\n        tok._.unidic_feats = feats\n    return doc\n\nnlp.tokenizer = mecab_tokenizer\n\n# (Optionally add your own components after this, e.g. a statistical tagger trained on this segmentation)\ndoc = nlp(\"æ—¥ã”ã‚ ã²ã”ã‚ æ—¥é ƒ å±…ã‚‹ ã„ã‚‹ æ›¸ãã‚ã‚‰ã‚ã™\")\nfor t in doc:\n    print(t.text, t._.unidic_lemma, t._.unidic_reading, t._.unidic_pos)\n\n\næ—¥ã”ã‚ æ—¥ã”ã‚ æ—¥ã”ã‚ åè©,æ™®é€šåè©,å‰¯è©å¯èƒ½,*\nã²ã”ã‚ ã²ã”ã‚ ã²ã”ã‚ åè©,æ™®é€šåè©,å‰¯è©å¯èƒ½,*\næ—¥é ƒ æ—¥é ƒ æ—¥é ƒ åè©,æ™®é€šåè©,å‰¯è©å¯èƒ½,*\nå±…ã‚‹ å±…ã‚‹ å±…ã‚‹ å‹•è©,éè‡ªç«‹å¯èƒ½,*,*\nã„ã‚‹ ã„ã‚‹ ã„ã‚‹ å‹•è©,éè‡ªç«‹å¯èƒ½,*,*\næ›¸ãã‚ã‚‰ã‚ã™ æ›¸ãã‚ã‚‰ã‚ã™ æ›¸ãã‚ã‚‰ã‚ã™ å‹•è©,ä¸€èˆ¬,*,*\n\n\n\n\nShow code\nprint(dir(m))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 print(dir(m))\n\nNameError: name 'm' is not defined\n\n\n\n\n\nShow code\nimport spacy\nfrom fugashi import Tagger\n\nnlp = spacy.load(\"ja_ginza\")\ntagger = Tagger()\n\n# Register extension fields (only if not already)\nfrom spacy.tokens import Token\nfor field in [\"unidic_lemma\",\"unidic_reading\",\"unidic_pos\"]:\n    if not Token.has_extension(field):\n        Token.set_extension(field, default=None)\n\ndef unicdic_enricher(doc):\n    text = doc.text\n    # Build a char-&gt;token index map (start offsets)\n    char2token = {}\n    for i, tok in enumerate(doc):\n        for pos in range(tok.idx, tok.idx + len(tok.text)):\n            char2token.setdefault(pos, i)\n    # Collect MeCab tokens with offsets\n    cursor = 0\n    for m in tagger(text):\n        surf = m.surface\n        start = text.find(surf, cursor)\n        cursor = start + len(surf)\n        # Find spaCy token that *starts* here (approx.)\n        if start in char2token:\n            i = char2token[start]\n            # Only annotate if exact surface match (avoid mid-token)\n            if doc[i].text.startswith(surf):\n                doc[i]._.unidic_lemma = m.dictionary_form\n                doc[i]._.unidic_reading = m.reading\n                doc[i]._.unidic_pos = \",\".join(m.pos)\n    return doc\n\nnlp.add_pipe(unicdic_enricher, name=\"unidic_enricher\", last=True)\n\ndoc = nlp(\"æ—¥ã”ã‚ ã²ã”ã‚ æ—¥é ƒ å±…ã‚‹ ã„ã‚‹ æ›¸ãã‚ã‚‰ã‚ã™\")\nfor t in doc:\n    print(t.text, t.lemma_, t._.unidic_lemma)\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[16], line 36\n     33                 doc[i]._.unidic_pos = \",\".join(m.pos)\n     34     return doc\n---&gt; 36 nlp.add_pipe(unicdic_enricher, name=\"unidic_enricher\", last=True)\n     38 doc = nlp(\"æ—¥ã”ã‚ ã²ã”ã‚ æ—¥é ƒ å±…ã‚‹ ã„ã‚‹ æ›¸ãã‚ã‚‰ã‚ã™\")\n     39 for t in doc:\n\nFile ~/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/spacy/language.py:811, in Language.add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\n    809     bad_val = repr(factory_name)\n    810     err = Errors.E966.format(component=bad_val, name=name)\n--&gt; 811     raise ValueError(err)\n    812 name = name if name is not None else factory_name\n    813 if name in self.component_names:\n\nValueError: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got &lt;function unicdic_enricher at 0x13ca313a0&gt; (name: 'unidic_enricher').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
  },
  {
    "objectID": "2025/notebooks/session-11.html",
    "href": "2025/notebooks/session-11.html",
    "title": "First text analysis with Python",
    "section": "",
    "text": "In this notebook, I will show you how to run a simple text analysis with spaCy package in Python.\nShow code\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\nShow code\nexample_text = \"Hi. This is my first awesome sentence to analyze.\"\nShow code\ndoc = nlp(example_text)"
  },
  {
    "objectID": "2025/notebooks/session-11.html#print-token",
    "href": "2025/notebooks/session-11.html#print-token",
    "title": "First text analysis with Python",
    "section": "Print token",
    "text": "Print token\nIn the following, you can iterate through the parsed doc and print each token in the doc object.\n\n\nShow code\nfor token in doc:\n    print(token)\n\n\nHi\n.\nThis\nis\nmy\nfirst\nawesome\nsentence\nto\nanalyze\n."
  },
  {
    "objectID": "2025/notebooks/session-11.html#print-lemmatized-form",
    "href": "2025/notebooks/session-11.html#print-lemmatized-form",
    "title": "First text analysis with Python",
    "section": "Print lemmatized form",
    "text": "Print lemmatized form\nYou can print lemmatized form by token.lemma_ (do not forget _ at the end.)\n\n\nShow code\n# Lemmatize\nfor token in doc:\n    print(token.text, token.lemma_, sep=\"\\t\")\n\n\nHi  hi\n.   .\nThis    this\nis  be\nmy  my\nfirst   first\nawesome awesome\nsentence    sentence\nto  to\nanalyze analyze\n.   ."
  },
  {
    "objectID": "2025/notebooks/session-11.html#print-part-of-speech-information",
    "href": "2025/notebooks/session-11.html#print-part-of-speech-information",
    "title": "First text analysis with Python",
    "section": "Print Part of Speech information",
    "text": "Print Part of Speech information\nYou can add more information, such as pos_\n\n\nShow code\nfor token in doc:\n    print(token.text, token.pos_, token.tag_, sep=\"\\t\")\n\n\nHi  INTJ    UH\n.   PUNCT   .\nThis    PRON    DT\nis  AUX VBZ\nmy  PRON    PRP$\nfirst   ADJ JJ\nawesome ADJ JJ\nsentence    NOUN    NN\nto  PART    TO\nanalyze VERB    VB\n.   PUNCT   ."
  },
  {
    "objectID": "2025/notebooks/session-11.html#spacy-token-information",
    "href": "2025/notebooks/session-11.html#spacy-token-information",
    "title": "First text analysis with Python",
    "section": "spaCy token information",
    "text": "spaCy token information\nSome useful token information are following:\n\n\n\ncode\nwhat it does\nexample\n\n\n\n\ntoken.lemma_\nlemmatized form\nbe, child\n\n\ntoken.pos_\nsimple POS (Universal Dependency)\nNOUN, VERB\n\n\ntoken.tag_\nfine-grained POS (PennTag set)\nNN, JJ, VB, BBZ\n\n\ntoken.dep_\ndependency type\namod, advmd\n\n\ntoken.head\ntoken information of the head of the dependency"
  },
  {
    "objectID": "2025/sessions/day1/session3.html",
    "href": "2025/sessions/day1/session3.html",
    "title": "Session 3",
    "section": "",
    "text": "You will learn how to conduct basic corpus searches.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#one-liner",
    "href": "2025/sessions/day1/session3.html#one-liner",
    "title": "Session 3",
    "section": "",
    "text": "You will learn how to conduct basic corpus searches.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#learning-objectives",
    "href": "2025/sessions/day1/session3.html#learning-objectives",
    "title": "Session 3",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nConduct KWIC searches on English-Corpora.org\nSort KWIC search results to obtain qualitative observation about language use\nUse advanced search strings such as regular expression to fine-tune the search results",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#key-concepts",
    "href": "2025/sessions/day1/session3.html#key-concepts",
    "title": "Session 3",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nKey Words In Context (KWIC)\nLexical Counting unit:\n\nToken\nLemma\nType\n\nRegular Expressions",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#tools-used",
    "href": "2025/sessions/day1/session3.html#tools-used",
    "title": "Session 3",
    "section": "ğŸ› ï¸ Tools Used",
    "text": "ğŸ› ï¸ Tools Used\n\nEnglish-Corpora.org\nAntConc",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day1/session3.html#dive-deeper---recommended-readings",
    "title": "Session 3",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#materials",
    "href": "2025/sessions/day1/session3.html#materials",
    "title": "Session 3",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#reflection",
    "href": "2025/sessions/day1/session3.html#reflection",
    "title": "Session 3",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#learning-objectives",
    "href": "2025/sessions/day1/session1.html#learning-objectives",
    "title": "Session 1",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nOverview the content of the current course\nExplain key success criteria in this course\nConduct the very first corpus search\nExplain different types of corpus linguistic analysis for different focus:\n\nfrequency analysis,\nconcordance analysis,\ncollocation analysis,\nPart-Of-Speech Tagging, etc.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#key-concepts",
    "href": "2025/sessions/day1/session1.html#key-concepts",
    "title": "Session 1",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nConceptual Overview of the corpus linguistic methods\n\nFrequency\nConcordance\nCollocation analysis\nPart-Of-Speech Tagging\nDependency Parsing",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#required-readings",
    "href": "2025/sessions/day1/session1.html#required-readings",
    "title": "Session 1",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\n(Skim) Davies (2015) Available through the shared drive.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#notes",
    "href": "2025/sessions/day1/session1.html#notes",
    "title": "Session 1",
    "section": "ğŸ“ Notes",
    "text": "ğŸ“ Notes\nNeeds analysis (After giving overview) is conducted at the end of this session.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#materials",
    "href": "2025/sessions/day1/session1.html#materials",
    "title": "Session 1",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#reflection",
    "href": "2025/sessions/day1/session1.html#reflection",
    "title": "Session 1",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html",
    "href": "2025/sessions/day5/index.html",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "",
    "text": "Day 5 explores cutting-edge applications of Large Language Models in corpus linguistics and provides dedicated time for final project development.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#overview",
    "href": "2025/sessions/day5/index.html#overview",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "",
    "text": "Day 5 explores cutting-edge applications of Large Language Models in corpus linguistics and provides dedicated time for final project development.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#key-concepts",
    "href": "2025/sessions/day5/index.html#key-concepts",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nLarge Language Models (LLMs) and Language Generation\nPrompt engineering\nFine-tuning\nLLM-assisted linguistic annotation\nResearch design and methodology\nProject presentation skills",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#preparation",
    "href": "2025/sessions/day5/index.html#preparation",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 5:\n\nRead:\n\nMizumoto, A., Shintani, N., Sasaki, M., & Teng, M. F. (2024). Testing the viability of ChatGPT as a companion in L2 writing accuracy assessment. Research Methods in Applied Linguistics, 3(2), 100116.\n\nSkim:\n\nKim, M., & Lu, X. (2024). Exploring the potential of using ChatGPT for rhetorical move-step analysis. Journal of English for Academic Purposes, 71, 101422.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#schedule",
    "href": "2025/sessions/day5/index.html#schedule",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 13: LLMs in Linguistic Analysis\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 14: Group Project Time\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 15: Project Presentations and Wrap-up\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#assignments",
    "href": "2025/sessions/day5/index.html#assignments",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Assignments",
    "text": "Assignments\n\nFinal Project: Final Project Guidelines\nGroup presentations today\nFinal submission deadline: [Check syllabus]",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#reflection",
    "href": "2025/sessions/day5/index.html#reflection",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session14.html",
    "href": "2025/sessions/day5/session14.html",
    "title": "Session 14",
    "section": "",
    "text": "Group project time. Please use the time wisely.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 14"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session14.html#one-liner",
    "href": "2025/sessions/day5/session14.html#one-liner",
    "title": "Session 14",
    "section": "",
    "text": "Group project time. Please use the time wisely.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 14"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session14.html#reflection",
    "href": "2025/sessions/day5/session14.html#reflection",
    "title": "Session 14",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 14"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html",
    "href": "2025/sessions/day2/index.html",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "",
    "text": "Day 2 focuses on analyzing vocabulary in corpus linguistics, introducing concepts of lexical richness, particularly diversity and sophistication.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#overview",
    "href": "2025/sessions/day2/index.html#overview",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "",
    "text": "Day 2 focuses on analyzing vocabulary in corpus linguistics, introducing concepts of lexical richness, particularly diversity and sophistication.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#key-concepts",
    "href": "2025/sessions/day2/index.html#key-concepts",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nLexical Richness (text internal vs external measures)\nLexical Diversity (Type-Token Ratio, MTLD)\nLexical Sophistication (frequency, concreteness, phonological neighbors)\nLexical profiling\nFrequency Lists and Zipf law",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#preparation",
    "href": "2025/sessions/day2/index.html#preparation",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 2:\n\nRead:\n\nDurrant Ch. 3\n\nSkim:\n\nDurrant Ch. 4 (Ignore R codes if you are not familiar)\nEguchi, M., & Kyle, K. (2020). Continuing to Explore the Multidimensional Nature of Lexical Sophistication. The Modern Language Journal, 104(2), 381â€“400.\n\nWatch:\n\nLaurence Anthonyâ€™s intro to AntConc\n\nGetting started (10 mins)\nCorpus manager Basics (18 mins) \nWord list tool basics (7 mins)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#schedule",
    "href": "2025/sessions/day2/index.html#schedule",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 4: Analyzing vocabulary (1) â€” Conceptual overview\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 5: Frequency Analysis and Lexical Profiling\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 6: Computing Lexical Measures\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#assignments",
    "href": "2025/sessions/day2/index.html#assignments",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Assignments",
    "text": "Assignments\n\nDue Tomorrow: Hands-on Assignment 2\nComplete lexical analysis exercises using AntConc and web applications",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#reflection",
    "href": "2025/sessions/day2/index.html#reflection",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#learning-objectives",
    "href": "2025/sessions/day2/session5.html#learning-objectives",
    "title": "Session 5",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, you will be able to:\n\n\nCompute frequency of a single-word lexical item in reference corpora\nDerive vocabulary frequency list using concordancing software (e.g., AntConc)\nApply tokenization on the Japanese language corpus for frequency analysis\nConduct Lexical Profiling using a web-application or desktop application (e.g., AntWordProfiler)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#key-concepts",
    "href": "2025/sessions/day2/session5.html#key-concepts",
    "title": "Session 5",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nLexical profiling\nFrequency Lists\nZipfâ€™s law\nLexical coverage",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day2/session5.html#dive-deeper---recommended-readings",
    "title": "Session 5",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#tools-used",
    "href": "2025/sessions/day2/session5.html#tools-used",
    "title": "Session 5",
    "section": "ğŸ› ï¸ Tools Used",
    "text": "ğŸ› ï¸ Tools Used\n\nAntConc\nAntWordProfiler\nNew Word Levels Checker\nLexTutor",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#materials",
    "href": "2025/sessions/day2/session5.html#materials",
    "title": "Session 5",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#reflection",
    "href": "2025/sessions/day2/session5.html#reflection",
    "title": "Session 5",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#frequency-list",
    "href": "2025/sessions/day2/session5.html#frequency-list",
    "title": "Session 5",
    "section": "Frequency list",
    "text": "Frequency list",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#lexical-profiling",
    "href": "2025/sessions/day2/session5.html#lexical-profiling",
    "title": "Session 5",
    "section": "Lexical Profiling",
    "text": "Lexical Profiling",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#keyness-analysis",
    "href": "2025/sessions/day2/session5.html#keyness-analysis",
    "title": "Session 5",
    "section": "Keyness Analysis",
    "text": "Keyness Analysis",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#learning-objectives",
    "href": "2025/sessions/day3/session8.html#learning-objectives",
    "title": "Session 8",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nSearch for window-based collocations and n-grams in AntConc\nCalculate commonly used strengths of association measures by hand using spreadsheet software\nDiscuss benefits and drawbacks of different strength of association measures",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#key-concepts",
    "href": "2025/sessions/day3/session8.html#key-concepts",
    "title": "Session 8",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nn-gram search\nWindow-based collocation search\nStrengths of Association measures â€” T-score, Mutual Information, LogDice",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#required-readings",
    "href": "2025/sessions/day3/session8.html#required-readings",
    "title": "Session 8",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\n(Skim) Durrant (2023) Ch. 8 (Ignore R codes if you are not familiar)\nStephanie Evertâ€™s website on collocation measures\n\nThis webpage provides formulas to calculate various Strengths Of Association measures.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day3/session8.html#dive-deeper---recommended-readings",
    "title": "Session 8",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#materials",
    "href": "2025/sessions/day3/session8.html#materials",
    "title": "Session 8",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#reflection",
    "href": "2025/sessions/day3/session8.html#reflection",
    "title": "Session 8",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now do the followings:\n\nGenerate lists of n-grams using AntConc.\nSearch for collocates with AntConc.\nCalculate major Strengths of Association (SOA) measures by hand.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html",
    "href": "2025/sessions/day3/index.html",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "",
    "text": "Day 3 explores multiword units, collocations, and statistical measures for analyzing word combinations in corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#overview",
    "href": "2025/sessions/day3/index.html#overview",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "",
    "text": "Day 3 explores multiword units, collocations, and statistical measures for analyzing word combinations in corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#key-concepts",
    "href": "2025/sessions/day3/index.html#key-concepts",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nTypes of multiword units (collocation, n-grams, lexical bundles)\nAssociation strengths (t-score, Mutual Information, LogDice)\nContext window vs dependency bigram approaches\nn-gram search and window-based collocation search\nLinear regression analysis for corpus data",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#preparation",
    "href": "2025/sessions/day3/index.html#preparation",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 3:\n\nRead:\n\nDurrant (2023) Ch. 7\nGablasova, D., Brezina, V., & McEnery, T. (2017). Collocations in Corpusâ€Based Language Learning Research. Language Learning, 67(S1), 155â€“179.\n\nSkim:\n\nDurrant (2023) Ch. 8 (Ignore R codes if you are not familiar)\nEguchi & Kyle (2020) - review if needed",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#schedule",
    "href": "2025/sessions/day3/index.html#schedule",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 7: Multiword Units â€” Conceptual Overview\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 8: Hands-on Collocation Analysis\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 9: Learner Corpus Mini-Research\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#assignments",
    "href": "2025/sessions/day3/index.html#assignments",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Assignments",
    "text": "Assignments\n\nDue Tomorrow: Hands-on Assignment 3\nPrepare mini-project research topic and questions for presentation",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#reflection",
    "href": "2025/sessions/day3/index.html#reflection",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#learning-objectives",
    "href": "2025/sessions/day4/session12.html#learning-objectives",
    "title": "Session 12",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nConduct linguistic complexity analysis using a template Python code provided by the instructor.\n(Optional) Apply the concept of linguistic complexity to the Japanese language.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#required-readings",
    "href": "2025/sessions/day4/session12.html#required-readings",
    "title": "Session 12",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nReread Kyle & Crossley (2018) again.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day4/session12.html#dive-deeper---recommended-readings",
    "title": "Session 12",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings\n\nKyle, K., & Crossley, S. (2017). Assessing syntactic sophistication in L2 writing: A usage-based approach. Language Testing, 34(4), 513â€“535. https://doi.org/10.1177/0265532217712554\nKyle, K., Choe, A. T., Eguchi, M., LaFlair, G., & Ziegler, N. (2021). A Comparison of Spoken and Written Language Use in Traditional and Technologyâ€Mediated Learning Environments. ETS Research Report Series, 2021(1), 1â€“29. https://doi.org/10.1002/ets2.12329",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#tools-used",
    "href": "2025/sessions/day4/session12.html#tools-used",
    "title": "Session 12",
    "section": "ğŸ› ï¸ Tools Used",
    "text": "ğŸ› ï¸ Tools Used\n\nTagAnt\nSimple Text Analyzer: A web app created for you.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#materials",
    "href": "2025/sessions/day4/session12.html#materials",
    "title": "Session 12",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#reflection",
    "href": "2025/sessions/day4/session12.html#reflection",
    "title": "Session 12",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#learning-objectives",
    "href": "2025/sessions/day4/session10.html#learning-objectives",
    "title": "Session 10",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nProvide historical overview of the syntactic complexity research\nDescribe different approaches to grammatical features:\n\nGrammatical complexity strand\nFine-grained grammatical complexity strand\nDescriptive (register-based analysis) strand\nVerb Argument Construction (VAC) strand\n\nUnderstand current trends of syntactic complexity research",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#key-concepts",
    "href": "2025/sessions/day4/session10.html#key-concepts",
    "title": "Session 10",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nGrammatical complexity\nPredictive measures versus Descriptive measures",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#required-readings",
    "href": "2025/sessions/day4/session10.html#required-readings",
    "title": "Session 10",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nDurrant Ch. 5.\nKyle, K., & Crossley, S. A. (2018). Measuring Syntactic Complexity in L2 Writing Using Fineâ€Grained Clausal and Phrasal Indices. The Modern Language Journal, 102(2), 333â€“349. https://doi.org/10.1111/modl.12468",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day4/session10.html#dive-deeper---recommended-readings",
    "title": "Session 10",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings\n\nBiber, D., Gray, B., Staples, S., & Egbert, J. (2020). Investigating grammatical complexity in L2 English writing research: Linguistic description versus predictive measurement. Journal of English for Academic Purposes, 46, 100869. https://doi.org/10.1016/j.jeap.2020.100869\nBiber, D., Gray, B., & Poonpon, K. (2011). Should We Use Characteristics of Conversation to Measure Grammatical Complexity in L2 Writing Development? TESOL Quarterly, 45(1), 5â€“35. https://doi.org/10.5054/tq.2011.244483\nNorris, J. M., & Ortega, L. (2009). Towards an Organic Approach to Investigating CAF in Instructed SLA: The Case of Complexity. Applied Linguistics, 30(4), 555â€“578. https://doi.org/10.1093/applin/amp044\nLu, X. (2011). A Corpusâ€Based Evaluation of Syntactic Complexity Measures as Indices of Collegeâ€Level ESL Writersâ€™ Language Development. TESOL Quarterly, 45(1), 36â€“62. https://doi.org/10.5054/tq.2011.240859",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#materials",
    "href": "2025/sessions/day4/session10.html#materials",
    "title": "Session 10",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#reflection",
    "href": "2025/sessions/day4/session10.html#reflection",
    "title": "Session 10",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now:\n\nDescribe classic linguistic complexity measures\nDescribe how",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/syllabus/schedule.html",
    "href": "2025/syllabus/schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "This course covers foundational concepts in corpus linguistics, corpus analysis methods, and their research applications across the following four areas: vocabulary, multiword units, and grammar.\n\n\n\nDay\nSession No.\nSession title\n\n\n\n\nDay 1 (Aug.Â 2nd, Sat)\n\nIntroduction to Linguistic Data Analysis\n\n\n\nSession 1\nGetting Started with Corpus Linguistics\n\n\n\nSession 2\nFoundations of Corpus Linguistics\n\n\n\nSession 3\nBasic Corpus Search\n\n\nDay 2 (Aug.Â 4th, Mon)\n\nAnalyzing Vocabulary\n\n\n\nSession 4\nConceptual overview\n\n\n\nSession 5\nFrequency lists & Lexical profiling\n\n\n\nSession 6\nLexical diversity & Sophistication\n\n\nDay 3 (Aug.Â 5th, Tue)\n\nAnalyzing Multiword Units\n\n\n\nSession 7\nConceptual overview\n\n\n\nSession 8\nCollocations & N-grams\n\n\n\nSession 9\nMini-research & Final project overview\n\n\nDay 4 (Aug.Â 6th, Wed)\n\nAnalyzing Grammar\n\n\n\nSession 10\nConceptual overview\n\n\n\nSession 11\nPOS-tagging and Dependency Parsing\n\n\n\nSession 12\nSyntactic Complexity\n\n\nDay 5 (Aug.Â 7th, Thu)\n\nAdvanced Topics & Wrap-up\n\n\n\nSession 13\nUsing large language models for language annotation\n\n\n\nSession 14\nFinal project preparation time\n\n\n\nSession 15\nFinal project presentation",
    "crumbs": [
      "Syllabus",
      "Course Schedule"
    ]
  },
  {
    "objectID": "2025/syllabus/schedule.html#overview",
    "href": "2025/syllabus/schedule.html#overview",
    "title": "Course Schedule",
    "section": "",
    "text": "This course covers foundational concepts in corpus linguistics, corpus analysis methods, and their research applications across the following four areas: vocabulary, multiword units, and grammar.\n\n\n\nDay\nSession No.\nSession title\n\n\n\n\nDay 1 (Aug.Â 2nd, Sat)\n\nIntroduction to Linguistic Data Analysis\n\n\n\nSession 1\nGetting Started with Corpus Linguistics\n\n\n\nSession 2\nFoundations of Corpus Linguistics\n\n\n\nSession 3\nBasic Corpus Search\n\n\nDay 2 (Aug.Â 4th, Mon)\n\nAnalyzing Vocabulary\n\n\n\nSession 4\nConceptual overview\n\n\n\nSession 5\nFrequency lists & Lexical profiling\n\n\n\nSession 6\nLexical diversity & Sophistication\n\n\nDay 3 (Aug.Â 5th, Tue)\n\nAnalyzing Multiword Units\n\n\n\nSession 7\nConceptual overview\n\n\n\nSession 8\nCollocations & N-grams\n\n\n\nSession 9\nMini-research & Final project overview\n\n\nDay 4 (Aug.Â 6th, Wed)\n\nAnalyzing Grammar\n\n\n\nSession 10\nConceptual overview\n\n\n\nSession 11\nPOS-tagging and Dependency Parsing\n\n\n\nSession 12\nSyntactic Complexity\n\n\nDay 5 (Aug.Â 7th, Thu)\n\nAdvanced Topics & Wrap-up\n\n\n\nSession 13\nUsing large language models for language annotation\n\n\n\nSession 14\nFinal project preparation time\n\n\n\nSession 15\nFinal project presentation",
    "crumbs": [
      "Syllabus",
      "Course Schedule"
    ]
  },
  {
    "objectID": "2025/syllabus/schedule.html#important-notes",
    "href": "2025/syllabus/schedule.html#important-notes",
    "title": "Course Schedule",
    "section": "Important Notes",
    "text": "Important Notes\n\nAll times are Japan Standard Time (JST)\nBring your laptop to all sessions\nComplete readings before each day",
    "crumbs": [
      "Syllabus",
      "Course Schedule"
    ]
  },
  {
    "objectID": "2025/slides/session-7.html#learning-objectives",
    "href": "2025/slides/session-7.html#learning-objectives",
    "title": "Session 7: Multiword Units",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nExplain different types of multiword units: collocation, n-grams, lexical bundles\nDemonstrate how major association strengths measures (t-score, Mutual Information, and LogDice) are calculated using examples"
  },
  {
    "objectID": "2025/slides/session-5.html#learning-objectives",
    "href": "2025/slides/session-5.html#learning-objectives",
    "title": "Session 5: Hands-on activity #2",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, you will be able to:\n\n\n\nCompute frequency of a single-word lexical item in reference corpora\nDerive vocabulary frequency list using concordancing software (e.g., AntConc)\nApply tokenization on the Japanese language corpus for frequency analysis\nConduct Lexical Profiling using a web-application or desktop application (e.g., AntWordProfiler)"
  },
  {
    "objectID": "2025/slides/session-5.html#introduction",
    "href": "2025/slides/session-5.html#introduction",
    "title": "Session 5: Hands-on activity #2",
    "section": "Introduction",
    "text": "Introduction\n\nAntConc is free concordancing tool.\nDeveloped by Laurence ANTHONY."
  },
  {
    "objectID": "2025/slides/session-5.html#hands-on-activity",
    "href": "2025/slides/session-5.html#hands-on-activity",
    "title": "Session 5: Hands-on activity #2",
    "section": "Hands-on Activity",
    "text": "Hands-on Activity"
  },
  {
    "objectID": "2025/slides/session-5.html#task-1-loading-a-corpus-to-antconc",
    "href": "2025/slides/session-5.html#task-1-loading-a-corpus-to-antconc",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task 1: Loading a corpus to AntConc",
    "text": "Task 1: Loading a corpus to AntConc"
  },
  {
    "objectID": "2025/slides/session-5.html#open-antconc",
    "href": "2025/slides/session-5.html#open-antconc",
    "title": "Session 5: Hands-on activity #2",
    "section": "Open AntConc",
    "text": "Open AntConc\n\nAntConc"
  },
  {
    "objectID": "2025/slides/session-5.html#antconc-window",
    "href": "2025/slides/session-5.html#antconc-window",
    "title": "Session 5: Hands-on activity #2",
    "section": "AntConc window",
    "text": "AntConc window\n\nAntConc2"
  },
  {
    "objectID": "2025/slides/session-5.html#load-a-corpus",
    "href": "2025/slides/session-5.html#load-a-corpus",
    "title": "Session 5: Hands-on activity #2",
    "section": "Load a corpus",
    "text": "Load a corpus\nNow, letâ€™s load a corpus.\n\nLoad-corpus"
  },
  {
    "objectID": "2025/slides/session-5.html#task-1-creating-a-frequency-list",
    "href": "2025/slides/session-5.html#task-1-creating-a-frequency-list",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task 1: Creating a frequency list",
    "text": "Task 1: Creating a frequency list"
  },
  {
    "objectID": "2025/slides/session-5.html#word",
    "href": "2025/slides/session-5.html#word",
    "title": "Session 5: Hands-on activity #2",
    "section": "Word",
    "text": "Word\nLetâ€™s now create a frequency list\n\nSelect Word analysis option\nSet Min. Freq and Min. Range\n\n\nMin. Freq = the number of times the word should occur in the corpus\nMin. Range = the number of files in which the word should occur\n\n\nHit Start"
  },
  {
    "objectID": "2025/slides/session-5.html#lets-try",
    "href": "2025/slides/session-5.html#lets-try",
    "title": "Session 5: Hands-on activity #2",
    "section": "Letâ€™s try",
    "text": "Letâ€™s try\n\nSet min. frequency = 3; min. range = 3"
  },
  {
    "objectID": "2025/slides/session-5.html#saving-the-frequency-list",
    "href": "2025/slides/session-5.html#saving-the-frequency-list",
    "title": "Session 5: Hands-on activity #2",
    "section": "Saving the frequency list",
    "text": "Saving the frequency list\n\nFrom File hit save the current results\n\n\nsave-list"
  },
  {
    "objectID": "2025/slides/session-5.html#frequency-list",
    "href": "2025/slides/session-5.html#frequency-list",
    "title": "Session 5: Hands-on activity #2",
    "section": "Frequency list",
    "text": "Frequency list\n\nWe will use the BROWN frequency list in the next session.\n\n\nsave-list"
  },
  {
    "objectID": "2025/slides/session-5.html#task-2-plot-frequencies",
    "href": "2025/slides/session-5.html#task-2-plot-frequencies",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task 2: Plot frequencies",
    "text": "Task 2: Plot frequencies\n\nLetâ€™s now understand the distributions of words in language.\nVisit our simple-text-analyzer tool.\nHit Frequency analysis and upload the frequency list.\nWhat did you notice?"
  },
  {
    "objectID": "2025/slides/session-5.html#frequency-plot",
    "href": "2025/slides/session-5.html#frequency-plot",
    "title": "Session 5: Hands-on activity #2",
    "section": "Frequency Plot",
    "text": "Frequency Plot\n\nVery few words occupy most of the corpus.\n\n\nBROWN frequency"
  },
  {
    "objectID": "2025/slides/session-5.html#discussion",
    "href": "2025/slides/session-5.html#discussion",
    "title": "Session 5: Hands-on activity #2",
    "section": "Discussion",
    "text": "Discussion\n\nWhat did you notice in the plot? What are the characteristics of words?\nWhat are the characteristics of frequent words? What about infrequent words?"
  },
  {
    "objectID": "2025/slides/session-5.html#task-3-tokenizing-non-english-languages-for-frequency-analysis",
    "href": "2025/slides/session-5.html#task-3-tokenizing-non-english-languages-for-frequency-analysis",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task 3: Tokenizing non-English languages for frequency analysis",
    "text": "Task 3: Tokenizing non-English languages for frequency analysis\n\nUp to this point, we only dealt with English.\nEnglish is very convenient in corpus analysis because of the white spaces.\nAsian languages have completely different writing system from Indo-European language, and it makes it difficult to tokenize texts in to words.\nI am planning to eat Oysters after this intensive course.\nã“ã®çŸ­æœŸé›†ä¸­è¬›åº§ãŒçµ‚ã‚ã£ãŸã‚‰ã€ã‚«ã‚­ã‚’é£Ÿã¹ãŸã„ã¨æ€ã£ã¦ã„ã¾ã™ã€‚"
  },
  {
    "objectID": "2025/slides/session-5.html#tagant",
    "href": "2025/slides/session-5.html#tagant",
    "title": "Session 5: Hands-on activity #2",
    "section": "TagAnt",
    "text": "TagAnt\n\nTokenization (segmenting running text into words) needs more advanced statistical algorithms.\nTagAnt is a free tool (again developped by Laurence ANTHONY).\nIt uses modern natural language processing tool (called spaCy) to tokenize input texts."
  },
  {
    "objectID": "2025/slides/session-5.html#tokenizing-japanese",
    "href": "2025/slides/session-5.html#tokenizing-japanese",
    "title": "Session 5: Hands-on activity #2",
    "section": "Tokenizing Japanese",
    "text": "Tokenizing Japanese\n\nDownload and open TagAnt."
  },
  {
    "objectID": "2025/slides/session-5.html#tokenizing-japanese-1",
    "href": "2025/slides/session-5.html#tokenizing-japanese-1",
    "title": "Session 5: Hands-on activity #2",
    "section": "Tokenizing Japanese",
    "text": "Tokenizing Japanese\n\nInput text\nSelect language.\nSelect Output format."
  },
  {
    "objectID": "2025/slides/session-5.html#result-of-tagant-segmentation",
    "href": "2025/slides/session-5.html#result-of-tagant-segmentation",
    "title": "Session 5: Hands-on activity #2",
    "section": "Result of TagAnt segmentation",
    "text": "Result of TagAnt segmentation\n\n\n\n\n\nHorizontal display\n\n\n\n\n\n\nVertical display\n\n\n\n\nTagAnt can do more than this.\nWe will come back to POS tagging on Day 4."
  },
  {
    "objectID": "2025/slides/session-5.html#activity-instruction-20-mins",
    "href": "2025/slides/session-5.html#activity-instruction-20-mins",
    "title": "Session 5: Hands-on activity #2",
    "section": "Activity instruction (20 mins)",
    "text": "Activity instruction (20 mins)\nTask\nCompile a Japanese frequency list based on a corpus.\nResource\n\nDownload a Japanese text Aozora 1000 from Google Drive.\nUse AntConc and TagAnt.\n\nSubmission\n\nSubmit a frequency list\nDescription of word frequency pattern in Japanese."
  },
  {
    "objectID": "2025/slides/session-5.html#task-4-vocabulary-profiling",
    "href": "2025/slides/session-5.html#task-4-vocabulary-profiling",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task 4: Vocabulary Profiling",
    "text": "Task 4: Vocabulary Profiling"
  },
  {
    "objectID": "2025/slides/session-5.html#frequency-information",
    "href": "2025/slides/session-5.html#frequency-information",
    "title": "Session 5: Hands-on activity #2",
    "section": "Frequency information",
    "text": "Frequency information\n\nWe are familiar with the notion of frequency in language.\nFrequency is one important driving force of human cognition and language processing.\nProfiling text using corpus frequency information is one important approach to understand textual characteristics."
  },
  {
    "objectID": "2025/slides/session-5.html#vocabulary-profiling",
    "href": "2025/slides/session-5.html#vocabulary-profiling",
    "title": "Session 5: Hands-on activity #2",
    "section": "Vocabulary Profiling",
    "text": "Vocabulary Profiling\n\nVocabulary profiling is a technique to use corpus frequency to understand characteristics of vocabulary use in the input text\nFor lexical sophistication measure\n\nHow much of the learner produced vocabulary is e.g., beyond 2000 word level (LFP; Laufer and Nation, 1995) ?\n\nFor lexical coverage\n\nHow many words do readers/listeners need to know in order to comprehend the text (90, 95, or 98% coverage)"
  },
  {
    "objectID": "2025/slides/session-5.html#vocabulary-profiling-tools",
    "href": "2025/slides/session-5.html#vocabulary-profiling-tools",
    "title": "Session 5: Hands-on activity #2",
    "section": "Vocabulary Profiling tools",
    "text": "Vocabulary Profiling tools\n\nRANGE program\nVocabProfiler in LexTutor\nNew Word Levels Checker\nAntWordProfiler"
  },
  {
    "objectID": "2025/slides/session-5.html#task-1",
    "href": "2025/slides/session-5.html#task-1",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task",
    "text": "Task"
  },
  {
    "objectID": "2025/slides/session-1.html#instructor",
    "href": "2025/slides/session-1.html#instructor",
    "title": "Session 1: Introduction",
    "section": "Instructor",
    "text": "Instructor"
  },
  {
    "objectID": "2025/slides/session-1.html#session-1-agenda",
    "href": "2025/slides/session-1.html#session-1-agenda",
    "title": "Session 1: Introduction",
    "section": "Session 1 Agenda",
    "text": "Session 1 Agenda\n\nIntroduction to Corpus Linguistics\n\nWhat is a corpus?\nWhy use corpora in linguistics?\nTypes of linguistic corpora\n\nGetting Started\n\nCourse tools and resources\nFirst hands-on activity"
  },
  {
    "objectID": "2025/slides/session-1.html#session-1-agenda-contd",
    "href": "2025/slides/session-1.html#session-1-agenda-contd",
    "title": "Session 1: Introduction",
    "section": "Session 1 Agenda (Contâ€™d)",
    "text": "Session 1 Agenda (Contâ€™d)\n\nCourse Introduction\n\nObjectives and learning outcomes\nCourse structure and expectations"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-linguistics",
    "href": "2025/slides/session-1.html#corpus-linguistics",
    "title": "Session 1: Introduction",
    "section": "Corpus linguistics",
    "text": "Corpus linguistics\nCorpus linguistics = the investigation of linguistic research question that have been framed in terms of the conditional distribution of linguisitc phenomena in a linguistic corpus.\n(Stefanowitsch, 2020, p.Â 56)"
  },
  {
    "objectID": "2025/slides/session-1.html#what-does-it-mean",
    "href": "2025/slides/session-1.html#what-does-it-mean",
    "title": "Session 1: Introduction",
    "section": "What does it mean?",
    "text": "What does it mean?"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-linguistics-1",
    "href": "2025/slides/session-1.html#corpus-linguistics-1",
    "title": "Session 1: Introduction",
    "section": "Corpus linguistics =",
    "text": "Corpus linguistics =\n\nthe investigation of linguistic research question\nthat have been framed in terms of the conditional distribution of linguisitc phenomena\nin a linguistic corpus.\n\n(Stefanowitsch, 2020, p.Â 56)"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus",
    "href": "2025/slides/session-1.html#corpus",
    "title": "Session 1: Introduction",
    "section": "Corpus",
    "text": "Corpus"
  },
  {
    "objectID": "2025/slides/session-1.html#linguistic-corpus",
    "href": "2025/slides/session-1.html#linguistic-corpus",
    "title": "Session 1: Introduction",
    "section": "Linguistic Corpus",
    "text": "Linguistic Corpus\n(Linguistic) Corpus =\n\nâ€œa collection of samples of language useâ€ that is:\n\nauthentic\nrepresentative\nlarge\n\n\n(To be explored more in session 2)"
  },
  {
    "objectID": "2025/slides/session-1.html#what-do-corpora-contain",
    "href": "2025/slides/session-1.html#what-do-corpora-contain",
    "title": "Session 1: Introduction",
    "section": "What do corpora contain?",
    "text": "What do corpora contain?\n\n\nlanguage samples produced in the wild for specific communicative purposes\n\nWritten language:\n\nMagazines\nNews Paper\nBlog\n\nSpoken language:\n\ntranscriptions of spoken exchanges\n\nTV or radio shows\nConversations"
  },
  {
    "objectID": "2025/slides/session-1.html#examples",
    "href": "2025/slides/session-1.html#examples",
    "title": "Session 1: Introduction",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "2025/slides/session-1.html#section",
    "href": "2025/slides/session-1.html#section",
    "title": "Session 1: Introduction",
    "section": "",
    "text": "COCA example"
  },
  {
    "objectID": "2025/slides/session-1.html#section-1",
    "href": "2025/slides/session-1.html#section-1",
    "title": "Session 1: Introduction",
    "section": "",
    "text": "COCA example"
  },
  {
    "objectID": "2025/slides/session-1.html#examples-1",
    "href": "2025/slides/session-1.html#examples-1",
    "title": "Session 1: Introduction",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-of-contemporary-american-coca",
    "href": "2025/slides/session-1.html#corpus-of-contemporary-american-coca",
    "title": "Session 1: Introduction",
    "section": "Corpus Of Contemporary American (COCA)",
    "text": "Corpus Of Contemporary American (COCA)\n\n\n\n\n\n\n\n\n\n\nGenre\n# texts\n# words\nExplanation\n\n\n\n\nSpoken\n44,803\n127,396,932\nTranscripts of unscripted conversation from more than 150 different TV and radio programs (examples: All Things Considered (NPR), Newshour (PBS), Good Morning America (ABC), Oprah)\n\n\nFiction\n25,992\n119,505,305\nShort stories and plays from literary magazines, childrenâ€™s magazines, popular magazines, first chapters of first edition books 1990-present, and fan fiction.\n\n\nMagazines\n86,292\n127,352,030\nNearly 100 different magazines, with a good mix between specific domains like news, health, home and gardening, women, financial, religion, sports, etc.\n\n\nNewspapers\n90,243\n122,958,016\nNewspapers from across the US, including: USA Today, New York Times, Atlanta Journal Constitution, San Francisco Chronicle, etc. Good mix between different sections of the newspaper, such as local news, opinion, sports, financial, etc.\n\n\nAcademic\n26,137\n120,988,361\nMore than 200 different peer-reviewed journals. These cover the full range of academic disciplines, with a good balance among education, social sciences, history, humanities, law, medicine, philosophy/religion, science/technology, and business"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-of-contemporary-american-coca-1",
    "href": "2025/slides/session-1.html#corpus-of-contemporary-american-coca-1",
    "title": "Session 1: Introduction",
    "section": "Corpus of Contemporary American (COCA)",
    "text": "Corpus of Contemporary American (COCA)\n\n\n\n\n\n\n\n\n\n\nGenre\n# texts\n# words\nExplanation\n\n\n\n\nWeb (Genl)\n88,989\n129,899,427\nClassified into the web genres of academic, argument, fiction, info, instruction, legal, news, personal, promotion, review web pages (by Serge Sharoff). Taken from the US portion of the GloWbE corpus.\n\n\nWeb (Blog)\n98,748\n125,496,216\nTexts that were classified by Google as being blogs. Further classified into the web genres of academic, argument, fiction, info, instruction, legal, news, personal, promotion, review web pages. Taken from the US portion of the GloWbE corpus.\n\n\nTV/Movies\n23,975\n129,293,467\nSubtitles from OpenSubtitles.org, and later the TV and Movies corpora. Studies have shown that the language from these shows and movies is even more colloquial / core than the data in actual â€œspoken corporaâ€.\n\n\nTotal\n485,179\n1,002,889,754\n\n\n\n\n\nsee more details"
  },
  {
    "objectID": "2025/slides/session-1.html#examples-2",
    "href": "2025/slides/session-1.html#examples-2",
    "title": "Session 1: Introduction",
    "section": "Examples",
    "text": "Examples\nInsert another example"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-linguistics-2",
    "href": "2025/slides/session-1.html#corpus-linguistics-2",
    "title": "Session 1: Introduction",
    "section": "Corpus linguistics =",
    "text": "Corpus linguistics =\n\nthe investigation of linguistic research question\nthat have been framed in terms of the conditional distribution of linguisitc phenomena\n\n\n\nin a linguistic corpus.\n\n\n(Stefanowitsch, 2020, p.Â 56)"
  },
  {
    "objectID": "2025/slides/session-1.html#the-conditional-distribution-of-linguistic-phenomena",
    "href": "2025/slides/session-1.html#the-conditional-distribution-of-linguistic-phenomena",
    "title": "Session 1: Introduction",
    "section": "The conditional distribution of linguistic phenomena",
    "text": "The conditional distribution of linguistic phenomena"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-linguistics-looks-into-conditional-distribution",
    "href": "2025/slides/session-1.html#corpus-linguistics-looks-into-conditional-distribution",
    "title": "Session 1: Introduction",
    "section": "Corpus linguistics looks into (conditional) distribution",
    "text": "Corpus linguistics looks into (conditional) distribution\n\n(roughly speaking) frequency (= occurrence) of a phenomena\nâ†’ How many times X occurs?\nconditional\nâ†’ How does X occur given Y?"
  },
  {
    "objectID": "2025/slides/session-1.html#examples-3",
    "href": "2025/slides/session-1.html#examples-3",
    "title": "Session 1: Introduction",
    "section": "Examples",
    "text": "Examples\n\nFrequency of Dog"
  },
  {
    "objectID": "2025/slides/session-1.html#more-about-conditional-distribution",
    "href": "2025/slides/session-1.html#more-about-conditional-distribution",
    "title": "Session 1: Introduction",
    "section": "More about conditional distribution",
    "text": "More about conditional distribution\nSegment corpus and calculate occurrences by:\n\ngenres (written vs spoken)\nyear (1970s vs 2000s)\ngeographical region (British vs American)\netc."
  },
  {
    "objectID": "2025/slides/session-1.html#conditional-distribution-of-dog-across-genre-and-year",
    "href": "2025/slides/session-1.html#conditional-distribution-of-dog-across-genre-and-year",
    "title": "Session 1: Introduction",
    "section": "Conditional distribution of Dog across genre and year",
    "text": "Conditional distribution of Dog across genre and year\n\nDog by section"
  },
  {
    "objectID": "2025/slides/session-1.html#linguistic-phenomena",
    "href": "2025/slides/session-1.html#linguistic-phenomena",
    "title": "Session 1: Introduction",
    "section": "Linguistic phenomena",
    "text": "Linguistic phenomena\n\nWe have mostly talked about word.\nHowever, corpus linguists are interested in more than words.\n\nExamples:\n\nMultiword Units\nGrammar\nDiscourse"
  },
  {
    "objectID": "2025/slides/session-1.html#multiword-units-more-on-day-3",
    "href": "2025/slides/session-1.html#multiword-units-more-on-day-3",
    "title": "Session 1: Introduction",
    "section": "Multiword Units (More on Day 3)",
    "text": "Multiword Units (More on Day 3)\nWith corpus methods, we can investigate how two or more words co-occur together.\n\nWhat does the word â€œdogâ€ occur together in English?\nAny guesses?"
  },
  {
    "objectID": "2025/slides/session-1.html#collocates",
    "href": "2025/slides/session-1.html#collocates",
    "title": "Session 1: Introduction",
    "section": "Collocates",
    "text": "Collocates\nIn COCA, the following word co-occur with â€œdogâ€: (Collocates = words that frequently co-occur with the node word.)"
  },
  {
    "objectID": "2025/slides/session-1.html#fill-in-the-blank",
    "href": "2025/slides/session-1.html#fill-in-the-blank",
    "title": "Session 1: Introduction",
    "section": "Fill in the blank",
    "text": "Fill in the blank\n\nâ€œOn the other _____â€\nHow do you know?"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-gives-you-some-ideas",
    "href": "2025/slides/session-1.html#corpus-gives-you-some-ideas",
    "title": "Session 1: Introduction",
    "section": "Corpus gives you some ideas",
    "text": "Corpus gives you some ideas\n\nSimple answer: They are used together so often.\nCorpus answer: Given the sequence â€œon the otherâ€, the probability of seeing the word â€œhandâ€ next is very high. â†’ Their Strengths Of Association (SOA) is high.\n\n\n\nWe will cover how to calculate simple SOA measures on Day 3."
  },
  {
    "objectID": "2025/slides/session-1.html#grammar-more-on-day-4",
    "href": "2025/slides/session-1.html#grammar-more-on-day-4",
    "title": "Session 1: Introduction",
    "section": "Grammar (More on Day 4)",
    "text": "Grammar (More on Day 4)\nWe can do similar with grammar (or morpho-syntax).\n\nHow often do we expect â€œby Xâ€ construction in passive construction."
  },
  {
    "objectID": "2025/slides/session-1.html#frequency-of-by-x-in-passive-construction",
    "href": "2025/slides/session-1.html#frequency-of-by-x-in-passive-construction",
    "title": "Session 1: Introduction",
    "section": "Frequency of â€œby Xâ€ in passive construction",
    "text": "Frequency of â€œby Xâ€ in passive construction"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-linguistics-3",
    "href": "2025/slides/session-1.html#corpus-linguistics-3",
    "title": "Session 1: Introduction",
    "section": "Corpus linguistics =",
    "text": "Corpus linguistics =\n\nthe investigation of linguistic research question\nthat have been framed in terms of the conditional distribution of linguisitc phenomena\nin a linguistic corpus."
  },
  {
    "objectID": "2025/slides/session-1.html#linguistic-research-questions",
    "href": "2025/slides/session-1.html#linguistic-research-questions",
    "title": "Session 1: Introduction",
    "section": "Linguistic Research Questions",
    "text": "Linguistic Research Questions\nExamples:\nBasic corpus research\n\nWhat are relationships between processing speed and language frequency\n\nLearner Corpus Research:\n\nWhat kind of vocabulary do second-language learners with varying proficiency levels produce?"
  },
  {
    "objectID": "2025/slides/session-1.html#course-description",
    "href": "2025/slides/session-1.html#course-description",
    "title": "Session 1: Introduction",
    "section": "Course description",
    "text": "Course description\n\nThis 5-day introduction:\n\ncovers key concepts in corpus linguistics and learner corpus research\nteaches you how to conduct simple corpus searches using Concordance software\ngives you an overview of methods to investigate conditional distributions (e.g., frequency, co-occurrences) of vocabulary, multiword units, and grammatical items.\nintroduces foundational methods to identify linguisitic phenomena using corpus and how to know about their distribution\ndiscusses important applications of corpus methods in applied linguistic (second language) research"
  },
  {
    "objectID": "2025/slides/session-1.html#about-the-course-website",
    "href": "2025/slides/session-1.html#about-the-course-website",
    "title": "Session 1: Introduction",
    "section": "About the course website",
    "text": "About the course website\n\nThe course materials are available through the following two:\n\nCourse website\n\nWe use the website to communicate course schedules, plans, and slides.\n\nGoogle Classroom\n\nWe use Google Classroom for assignment submission, discussion forum, and distributing other materials (e.g., Readings)."
  },
  {
    "objectID": "2025/slides/session-1.html#session-overview",
    "href": "2025/slides/session-1.html#session-overview",
    "title": "Session 1: Introduction",
    "section": "Session overview",
    "text": "Session overview\n\n\n\n\nDay\nTheme\nSessions\n\n\n\n\nDay 1\nIntroduction & Corpus Basics\n\n\n\nDay 2\nAnalysis of Vocabulary & Multiword Units (1)\n\n\n\nDay 3\nAnalysis of Vocabulary & Multiword Units (2)\n\n\n\nDay 4\nAnalysis of Grammar\n\n\n\nDay 5\nAdvanced Topics & Projects"
  },
  {
    "objectID": "2025/slides/session-1.html#daily-schedule",
    "href": "2025/slides/session-1.html#daily-schedule",
    "title": "Session 1: Introduction",
    "section": "Daily schedule",
    "text": "Daily schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 1\n\n\n12:00-13:00\nLunch break\n\n\n13:00-14:30\nSession 2\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 3"
  },
  {
    "objectID": "2025/slides/session-1.html#if-we-have-time",
    "href": "2025/slides/session-1.html#if-we-have-time",
    "title": "Session 1: Introduction",
    "section": "If we have timeâ€¦",
    "text": "If we have timeâ€¦\nMove to the next part."
  },
  {
    "objectID": "2025/slides/session-2.html#learning-objectives",
    "href": "2025/slides/session-2.html#learning-objectives",
    "title": "Session 2: Corpus as a scientific method",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nDefine corpus linguistics as an empirical methodology\nExplain key limitations of introspection in linguistic research\nDescribe the role of frequency data and patterns in corpus analysis\nIdentify and explain the basic steps in corpus-based research\nReflect on their own stance toward data, intuition, and linguistic evidence"
  },
  {
    "objectID": "2025/slides/session-2.html#common-criticism",
    "href": "2025/slides/session-2.html#common-criticism",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Common criticism",
    "text": "Common criticism\n\ncorpora are usage data and thus of no use in studying linguistic knowledge;\n\ncorpora and the data derived from them are necessarily incomplete;\ncorpora contain only linguistic forms (represented as graphemic strings), but no information about the semantics, pragmatics, etc. of these forms; and\n\ncorpora do not contain negative evidence, i.e., they can only tell us what is possible in a given language, but not what is not possible."
  },
  {
    "objectID": "2025/slides/session-12.html#learning-objectives",
    "href": "2025/slides/session-12.html#learning-objectives",
    "title": "Session 12: Hands-on Activity",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nConduct linguistic complexity analysis using a template Python code provided by the instructor.\n(Optional) Apply the concept of linguistic complexity to the Japanese language."
  },
  {
    "objectID": "2025/slides/session-12.html#thinking-grammartically",
    "href": "2025/slides/session-12.html#thinking-grammartically",
    "title": "Session 12: Hands-on Activity",
    "section": "Thinking grammartically",
    "text": "Thinking grammartically\nIn group, come up with 3-5 grammatical constructions you would like to identify in learner language.\n\nExplain why it is important\nGive some examples that fall under teh grammatical construction."
  },
  {
    "objectID": "2025/slides/session-12.html#about-google-colab-5-mins",
    "href": "2025/slides/session-12.html#about-google-colab-5-mins",
    "title": "Session 12: Hands-on Activity",
    "section": "About Google Colab (5 mins)",
    "text": "About Google Colab (5 mins)"
  },
  {
    "objectID": "2025/slides/session-12.html#python-basics-15-mins",
    "href": "2025/slides/session-12.html#python-basics-15-mins",
    "title": "Session 12: Hands-on Activity",
    "section": "Python basics (15 mins)",
    "text": "Python basics (15 mins)"
  },
  {
    "objectID": "2025/slides/session-12.html#first-text-analysis",
    "href": "2025/slides/session-12.html#first-text-analysis",
    "title": "Session 12: Hands-on Activity",
    "section": "First text analysis",
    "text": "First text analysis\n\nFirst thing is to load the package.\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\nThen you will define a variable example_text\n\nexample_text = \"Hi. This is my first awesome sentence to analyze.\"\n\nAnalyze this using spacy\n\ndoc = nlp(example_text)"
  },
  {
    "objectID": "2025/slides/session-12.html#result-of-your-first-text-analysis",
    "href": "2025/slides/session-12.html#result-of-your-first-text-analysis",
    "title": "Session 12: Hands-on Activity",
    "section": "Result of your first text analysis",
    "text": "Result of your first text analysis\n\nLetâ€™s print analysis results\n\nfor token in doc:\n    print(token.text, token.pos_, token.tag_, sep=\"\\t\")"
  },
  {
    "objectID": "2025/slides/session-12.html#spacy-token-information",
    "href": "2025/slides/session-12.html#spacy-token-information",
    "title": "Session 12: Hands-on Activity",
    "section": "spaCy token information",
    "text": "spaCy token information\nSome useful token information are following:\n\n\n\ncode\nwhat it does\nexample\n\n\n\n\ntoken.lemma_\nlemmatized form\nbe, child\n\n\ntoken.pos_\nsimple POS (Universal Dependency)\nNOUN, VERB\n\n\ntoken.tag_\nfine-grained POS (PennTag set)\nNN, JJ, VB, BBZ\n\n\ntoken.dep_\ndependency type\namod, advmd\n\n\ntoken.head\ntoken information of the head of the dependency"
  },
  {
    "objectID": "2025/slides/session-11.html#learning-objectives",
    "href": "2025/slides/session-11.html#learning-objectives",
    "title": "Session 11: Hands-on Activity",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nUnderstand NLP tasks such as POS tagging and dependency parsing\nUnderstand how automated parsing works\nConduct multi-lingual Part-Of-Speech (POS) tagging using TagAnt\nConduct POS tagging using spaCy library in Python (through Google Colab)\nConduct Dependency parsing using spaCy library in Python (through Google Colab)"
  },
  {
    "objectID": "2025/slides/session-11.html#hands-on-activity",
    "href": "2025/slides/session-11.html#hands-on-activity",
    "title": "Session 11: Hands-on Activity",
    "section": "Hands-on Activity",
    "text": "Hands-on Activity\nTask 1: POS tagging with TagAnt\nTask 2: POS-sensitive frequency list\nTask 3: Understanding dependency grammar through visualization"
  },
  {
    "objectID": "2025/slides/session-11.html#pos-tagging-with-tagant",
    "href": "2025/slides/session-11.html#pos-tagging-with-tagant",
    "title": "Session 11: Hands-on Activity",
    "section": "POS tagging with TagAnt",
    "text": "POS tagging with TagAnt"
  },
  {
    "objectID": "2025/slides/session-11.html#tagging-with-tagant",
    "href": "2025/slides/session-11.html#tagging-with-tagant",
    "title": "Session 11: Hands-on Activity",
    "section": "Tagging with TagAnt",
    "text": "Tagging with TagAnt\n\nOpen TagAnt\nSelect Input Files\nSelect Language\nSelect Display information (see next)"
  },
  {
    "objectID": "2025/slides/session-11.html#display-setting-info-in-tagant",
    "href": "2025/slides/session-11.html#display-setting-info-in-tagant",
    "title": "Session 11: Hands-on Activity",
    "section": "Display setting info in TagAnt",
    "text": "Display setting info in TagAnt\nFollowings are basic selection in TagAnt.\n\n\n\nMenu\nFunction\nExample\n\n\n\n\nword\ntokenization\ndogs, ran\n\n\npos\nPOS tag (simple)\nNOUN, VERB\n\n\npos_tag\nPOS tag (detailed)\nNNS, VBD\n\n\nlemma\nlemmatized word\ndog, run"
  },
  {
    "objectID": "2025/slides/session-11.html#other-diaplay-settings",
    "href": "2025/slides/session-11.html#other-diaplay-settings",
    "title": "Session 11: Hands-on Activity",
    "section": "Other Diaplay settings",
    "text": "Other Diaplay settings\n\n\n\nMenu\nFunction\nExample\n\n\n\n\nword+pos\ntokenization and POS\ndogs_NOUN, ran_VERB\n\n\nword+lemma +pos_tag\ntoken+lemma+POS\ndogs_dog_NN, ran_run_VERB"
  },
  {
    "objectID": "2025/slides/session-11.html#task-1-annotating-japanese-text-10-mins",
    "href": "2025/slides/session-11.html#task-1-annotating-japanese-text-10-mins",
    "title": "Session 11: Hands-on Activity",
    "section": "Task 1: Annotating Japanese text (10 mins)",
    "text": "Task 1: Annotating Japanese text (10 mins)\n\nAnnotate 50 Japanese text files with TagAnt.\nCreate frequency list for aozora_50"
  },
  {
    "objectID": "2025/slides/session-11.html#task-1-answer",
    "href": "2025/slides/session-11.html#task-1-answer",
    "title": "Session 11: Hands-on Activity",
    "section": "Task 1: Answer",
    "text": "Task 1: Answer\nBefore\nã€Œå¤§æºã€\n\nã€€åƒ•ã¯æœ¬æ‰€ç•Œéšˆã®ã“ã¨ã‚’ã‚¹ã‚±ãƒ„ãƒã—ã‚ã¨ã„ãµç¤¾å‘½ã‚’å—ã‘ã€åŒã˜ç¤¾ã®ï¼¯å›ã¨ä¸€ã—ã‚ˆã«ä¹…æŒ¯ã‚Šã«æœ¬æ‰€ã¸å‡ºã‹ã‘ã¦è¡Œã¤ãŸã€‚\nAfter\nã€Œ_è£œåŠ©è¨˜å·-æ‹¬å¼§é–‹_ã€Œ å¤§æº_åè©-å›ºæœ‰åè©-äººå-å§“_å¤§æº ã€_è£œåŠ©è¨˜å·-æ‹¬å¼§é–‰_ã€\n\nã€€_SPACE_ã€€ åƒ•_ä»£åè©_åƒ• ã¯_åŠ©è©-ä¿‚åŠ©è©_ã¯ æœ¬æ‰€_åè©-å›ºæœ‰åè©-åœ°å-ä¸€èˆ¬_æœ¬æ‰€ ç•Œéšˆ_åè©-æ™®é€šåè©-ä¸€èˆ¬_ç•Œéšˆ ã®_åŠ©è©-æ ¼åŠ©è©_ã® ã“ã¨_åè©-æ™®é€šåè©-ä¸€èˆ¬_ã“ã¨ ã‚’_åŠ©è©-æ ¼åŠ©è©_ã‚’ ã‚¹ã‚±ãƒ„ãƒ_åè©-æ™®é€šåè©-ä¸€èˆ¬_ã‚¹ã‚±ãƒ„ãƒ ã—ã‚_å‹•è©-éè‡ªç«‹å¯èƒ½_ã™ã‚‹ ã¨_åŠ©è©-æ ¼åŠ©è©_ã¨ ã„ãµ_å‹•è©-ä¸€èˆ¬_ã„ãµ ç¤¾å‘½_åè©-æ™®é€šåè©-ä¸€èˆ¬_ç¤¾å‘½ ã‚’_åŠ©è©-æ ¼åŠ©è©_ã‚’ å—ã‘_å‹•è©-ä¸€èˆ¬_å—ã‘ã‚‹ ã€_è£œåŠ©è¨˜å·-èª­ç‚¹_ã€ åŒã˜_é€£ä½“è©_åŒã˜ ç¤¾_åè©-æ™®é€šåè©-åŠ©æ•°è©å¯èƒ½_ç¤¾ ã®_åŠ©è©-æ ¼åŠ©è©_ã® ï¼¯_åè©-æ™®é€šåè©-ä¸€èˆ¬_o å›_æ¥å°¾è¾-åè©çš„-ä¸€èˆ¬_å› ã¨_åŠ©è©-æ ¼åŠ©è©_ã¨ ä¸€ã—ã‚ˆ_åè©-æ™®é€šåè©-ã‚µå¤‰å¯èƒ½_ä¸€ã—ã‚ˆ ã«_åŠ©è©-æ ¼åŠ©è©_ã« ä¹…_å½¢å®¹è©-ä¸€èˆ¬_ä¹…ã„ æŒ¯ã‚Š_æ¥å°¾è¾-åè©çš„-ä¸€èˆ¬_æŒ¯ã‚Š ã«_åŠ©è©-æ ¼åŠ©è©_ã« æœ¬æ‰€_åè©-å›ºæœ‰åè©-åœ°å-ä¸€èˆ¬_æœ¬æ‰€ ã¸_åŠ©è©-æ ¼åŠ©è©_ã¸ å‡º_å‹•è©-ä¸€èˆ¬_å‡ºã‚‹ ã‹ã‘_å‹•è©-éè‡ªç«‹å¯èƒ½_ã‹ã‘ã‚‹ ã¦_åŠ©è©-æ¥ç¶šåŠ©è©_ã¦ è¡Œã¤_å‹•è©-ä¸€èˆ¬_è¡Œãµ ãŸ_åŠ©å‹•è©_ãŸ ã€‚_è£œåŠ©è¨˜å·-å¥ç‚¹_ã€‚"
  },
  {
    "objectID": "2025/slides/session-11.html#task-2-frequency-list-by-pos-tags-10-mins",
    "href": "2025/slides/session-11.html#task-2-frequency-list-by-pos-tags-10-mins",
    "title": "Session 11: Hands-on Activity",
    "section": "Task 2: Frequency-list by POS tags (10 mins)",
    "text": "Task 2: Frequency-list by POS tags (10 mins)\n\nUsing AntConc, create following frequency lists:\n\nCreate a frequency list of å‹•è©-éè‡ªç«‹å¯èƒ½\n\nIf you are done, please create another frequency list with different search terms."
  },
  {
    "objectID": "2025/slides/session-11.html#task-2-key",
    "href": "2025/slides/session-11.html#task-2-key",
    "title": "Session 11: Hands-on Activity",
    "section": "Task 2: Key",
    "text": "Task 2: Key\n\néè‡ªç«‹å¯èƒ½å‹•è©"
  },
  {
    "objectID": "2025/slides/session-11.html#advanced-options-in-tagant",
    "href": "2025/slides/session-11.html#advanced-options-in-tagant",
    "title": "Session 11: Hands-on Activity",
    "section": "Advanced options in TagAnt",
    "text": "Advanced options in TagAnt\n\nIn TagAnt, you can download models for other languages.\n\n\nloading other models"
  },
  {
    "objectID": "2025/slides/session-11.html#any-questions",
    "href": "2025/slides/session-11.html#any-questions",
    "title": "Session 11: Hands-on Activity",
    "section": "Any questions?",
    "text": "Any questions?\n\nNow you can parse multilingual text with TagAnt."
  },
  {
    "objectID": "2025/slides/session-11.html#goals",
    "href": "2025/slides/session-11.html#goals",
    "title": "Session 11: Hands-on Activity",
    "section": "Goals",
    "text": "Goals\n\nDescribe grammatical structure of a simple sentence using terminology such as ROOT,head, dependency type, and dependent."
  },
  {
    "objectID": "2025/slides/session-11.html#dependency-grammar-ä¿‚å—ã‘",
    "href": "2025/slides/session-11.html#dependency-grammar-ä¿‚å—ã‘",
    "title": "Session 11: Hands-on Activity",
    "section": "Dependency grammar (ä¿‚å—ã‘)",
    "text": "Dependency grammar (ä¿‚å—ã‘)\n\nDependency grammar is particular type of syntactic tree.\nforms a tree by defining binary relations between running tokens.\nEach token in the sentence is governed by one token (i.e., head)\nThe highest in the syntactic tree is termed as ROOT"
  },
  {
    "objectID": "2025/slides/session-11.html#dependency-grammar-ä¿‚å—ã‘---2",
    "href": "2025/slides/session-11.html#dependency-grammar-ä¿‚å—ã‘---2",
    "title": "Session 11: Hands-on Activity",
    "section": "Dependency grammar (ä¿‚å—ã‘) - 2",
    "text": "Dependency grammar (ä¿‚å—ã‘) - 2\n\nThere are a few different approaches to formalize dependency\n\nUniversal Dependency\nStanford Dependency\nClearNLP\netc."
  },
  {
    "objectID": "2025/slides/session-11.html#simple-example",
    "href": "2025/slides/session-11.html#simple-example",
    "title": "Session 11: Hands-on Activity",
    "section": "Simple example",
    "text": "Simple example\nThe following is a dependency for I play baseball.\n\nsimple-dependency"
  },
  {
    "objectID": "2025/slides/session-11.html#in-table-format",
    "href": "2025/slides/session-11.html#in-table-format",
    "title": "Session 11: Hands-on Activity",
    "section": "In table format",
    "text": "In table format\nThe same sentence, I play baseball can be expressed in the following format\n\n\n\ntid\ntoken\ndep\nhead\n\n\n\n\n1\nI\nnsubj\n2\n\n\n2\nplay\nROOT\n\n\n\n3\nbaseball\ndobj\n2\n\n\n4\n.\npunct\n2\n\n\n\nThis type of vertical format is often used to represent multi-layered token information."
  },
  {
    "objectID": "2025/slides/session-11.html#some-excercise---problem-1",
    "href": "2025/slides/session-11.html#some-excercise---problem-1",
    "title": "Session 11: Hands-on Activity",
    "section": "Some excercise - Problem 1",
    "text": "Some excercise - Problem 1\nTry filling in the gap in the following table.\n\n\n\ntid\ntoken\ndep\nhead\n\n\n\n\n1\nI\n\n\n\n\n2\nlove\nROOT\n\n\n\n3\nbeef\n\n\n\n\n4\ntongue\n\n\n\n\n5\n.\npunct\n2"
  },
  {
    "objectID": "2025/slides/session-11.html#some-excercise---problem-2",
    "href": "2025/slides/session-11.html#some-excercise---problem-2",
    "title": "Session 11: Hands-on Activity",
    "section": "Some excercise - Problem 2",
    "text": "Some excercise - Problem 2\nTry filling in the gap in the following table.\n\n\n\ntid\ntoken\ndep\nhead\n\n\n\n\n1\nThe\n\n\n\n\n2\ncat\n\n\n\n\n3\nsleeps\nROOT\n\n\n\n4\non\n\n\n\n\n5\nthe\n\n\n\n\n6\nmat\n\n\n\n\n7\n.\npunct\n3"
  },
  {
    "objectID": "2025/slides/session-11.html#some-excercise---problem-3",
    "href": "2025/slides/session-11.html#some-excercise---problem-3",
    "title": "Session 11: Hands-on Activity",
    "section": "Some excercise - Problem 3",
    "text": "Some excercise - Problem 3\nTry filling in the gap in the following table.\n\n\n\ntid\ntoken\ndep\nhead\n\n\n\n\n1\nShe\n\n\n\n\n2\nquickly\n\n\n\n\n3\nreads\nROOT\n\n\n\n4\ninteresting\n\n\n\n\n5\nbooks\n\n\n\n\n6\n.\npunct\n3"
  },
  {
    "objectID": "2025/slides/session-11.html#lets-parse-the-sentence.",
    "href": "2025/slides/session-11.html#lets-parse-the-sentence.",
    "title": "Session 11: Hands-on Activity",
    "section": "Letâ€™s parse the sentence.",
    "text": "Letâ€™s parse the sentence.\n\nVisit our webapp\nTry the sentences above and analyze their dependencies"
  },
  {
    "objectID": "2025/slides/session-11.html#questions",
    "href": "2025/slides/session-11.html#questions",
    "title": "Session 11: Hands-on Activity",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "2025/slides/session-8.html#learning-objectives",
    "href": "2025/slides/session-8.html#learning-objectives",
    "title": "Session 8: Hands-on activity #4",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nSearch for window-based collocations and n-grams in AntConc\nCalculate commonly used strengths of association measures by hand using spreadsheet software\nDiscuss benefits and drawbacks of different strength of association measures"
  },
  {
    "objectID": "2025/slides/session-8.html#recap-what-is-collocation",
    "href": "2025/slides/session-8.html#recap-what-is-collocation",
    "title": "Session 8: Hands-on activity #4",
    "section": "Recap: What is collocation?",
    "text": "Recap: What is collocation?\n\nCollocation"
  },
  {
    "objectID": "2025/slides/session-8.html#collocation-in-antconc",
    "href": "2025/slides/session-8.html#collocation-in-antconc",
    "title": "Session 8: Hands-on activity #4",
    "section": "Collocation in AntConc",
    "text": "Collocation in AntConc\n\nexplain"
  },
  {
    "objectID": "2025/slides/session-8.html#calculating-soa-by-hand",
    "href": "2025/slides/session-8.html#calculating-soa-by-hand",
    "title": "Session 8: Hands-on activity #4",
    "section": "Calculating SOA by hand",
    "text": "Calculating SOA by hand\n\nOpen Google Sreadsheet.\nBuild function to calculate the following SOA measures\n\nT-score\nMutual Information\nMutual Information Squared (\\(MI^2\\))\nLogDice"
  },
  {
    "objectID": "2025/slides/session-8.html#understanding-collocational-characteristics-by-plotting",
    "href": "2025/slides/session-8.html#understanding-collocational-characteristics-by-plotting",
    "title": "Session 8: Hands-on activity #4",
    "section": "Understanding collocational characteristics by plotting",
    "text": "Understanding collocational characteristics by plotting\n\nLetâ€™s make plots based on the calculated\nPlot 1:\n\nX-Axis: T-score\nY-Axis: MI\n\nPlot 2:\n\nX-Axis: LogDice\nY-Axis: MI"
  },
  {
    "objectID": "2025/slides/session-8.html#discussion-describe-the-characteristics-of-each-index",
    "href": "2025/slides/session-8.html#discussion-describe-the-characteristics-of-each-index",
    "title": "Session 8: Hands-on activity #4",
    "section": "Discussion: Describe the characteristics of each index",
    "text": "Discussion: Describe the characteristics of each index\n\nWhat are the characteristics of SOA indices?\n\nT-score\nMI\nLogDice"
  },
  {
    "objectID": "2025/slides/session-8.html#advanced-how-can-i-generate-a-collocation-list",
    "href": "2025/slides/session-8.html#advanced-how-can-i-generate-a-collocation-list",
    "title": "Session 8: Hands-on activity #4",
    "section": "(Advanced) how can I generate a collocation list?",
    "text": "(Advanced) how can I generate a collocation list?\n\nColab notebook"
  },
  {
    "objectID": "2025/assignments/hands-on-4/draft.html",
    "href": "2025/assignments/hands-on-4/draft.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/hands-on-3/draft.html",
    "href": "2025/assignments/hands-on-3/draft.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/hands-on-2/draft.html",
    "href": "2025/assignments/hands-on-2/draft.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html",
    "href": "2025/assignments/hands-on-1/index.html",
    "title": "Hands-on Assignment 1",
    "section": "",
    "text": "Under Construction\n\n\n\nThis course website is currently under construction and will be ready for the class starting August 2nd, 2025. Content is being actively developed and updated.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html#task-1-constructing-corpus-search-research-question-and-hypothesis",
    "href": "2025/assignments/hands-on-1/index.html#task-1-constructing-corpus-search-research-question-and-hypothesis",
    "title": "Hands-on Assignment 1",
    "section": "Task 1: Constructing corpus-search research question and hypothesis",
    "text": "Task 1: Constructing corpus-search research question and hypothesis\nIn this task, you are asked to articulate your research question and hypothesis for your first corpus assignment. Use the following information as guideline.\n\nResearch question:\n\nResearch questions should be answerable. Follow the class material to construct a intruiging research question that you can answer with basic corpus search skills you learned.\nWrite one or two research questions you would like to answer through this assignment.\n\n\n\nHypothesis:\n\nOnce you decided on research question, state your hypothesis.\nUse the course material to help you articulate research hypothesis.\nWrite a short paragraph stating your hypothesis and why you think your research hypotheses may be true.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html#task-2-methods",
    "href": "2025/assignments/hands-on-1/index.html#task-2-methods",
    "title": "Hands-on Assignment 1",
    "section": "Task 2: Methods",
    "text": "Task 2: Methods\nIn this task, you are asked to describe the methods choice of your corpus search. This should include justifications for the corpora used, type of corpus search used.\n\nCorpora\n\nWhich corpora or sub-section of a corpus would you conduct a search on? Why? Justify your choice in a paragraph.\n\n\n\nSearch Methods and Plan\n\nWhat corpus search methods would you choose and why? In this assignment, search methods mainly include methods available through English-Corpora.org, including LIST, Chart, Collocates, Compare and KWIC.\nIn what way are you planning to conduct the search and what kind of information are you expecting from it?",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html#task-3-result",
    "href": "2025/assignments/hands-on-1/index.html#task-3-result",
    "title": "Hands-on Assignment 1",
    "section": "Task 3: Result",
    "text": "Task 3: Result\nIn this part of the assignment, you are asked to describe the search results and provide interpretations on the findings.\n\nCorpus search results: Provide some numbers or discourse samples based on your corpus search. This can be frequency list, table with frequency counts, or a copy of KWIC results.\nInterpretation: Write a paragraph, providing some interpretations of your findings.\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\nincludes one or two research questions\narticulates hypotheses\ninclude a list of corpora used\njustifies the selection of corpus\noutlines the search plan (key phrase, regular expression, sorting, filtering, etc.)\nincludes results of your search\nprovides interpretation of the findings.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html",
    "href": "2025/assignments/hands-on-1/draft.html",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "",
    "text": "How does the frequency of word X change over time, and what do the concordance lines reveal about changes in usage?\nHow does the frequency of word X differ across text types Z, and what patterns appear in the KWIC display?\nWhat is the frequency distribution of grammatical pattern X across different text types, and what contexts does KWIC reveal?\nHow has the usage of word X evolved in the past Y years based on frequency trends and concordance evidence?\nWhat are the typical contexts in which expression X appears based on concordance analysis?\nHow do different varieties of language Y (e.g., British vs.Â American English) differ in frequency and contexts of feature X?\nWhat patterns emerge when comparing the frequency and KWIC results for synonyms X and Y?\nHow does register (formal/informal) affect both the frequency and typical contexts of linguistic feature X?\nWhich words frequently appear near word X based on manual analysis of concordance lines?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-change-over-time-and-what-do-the-concordance-lines-reveal-about-changes-in-usage",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-change-over-time-and-what-do-the-concordance-lines-reveal-about-changes-in-usage",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How does the frequency of word X change over time, and what do the concordance lines reveal about changes in usage?",
    "text": "Template: How does the frequency of word X change over time, and what do the concordance lines reveal about changes in usage?\n\nExample Questions:\n\nHow does the frequency of â€œemailâ€ change from 1990-2020, and what do concordance lines reveal about its usage evolution?\nHow has the frequency of â€œglobal warmingâ€ vs â€œclimate changeâ€ shifted over decades, and what contexts show this shift?\nHow does the frequency of â€œshallâ€ change over time in American English, and what do KWIC results show about its declining contexts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-differ-across-text-types-z-and-what-patterns-appear-in-the-kwic-display",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-differ-across-text-types-z-and-what-patterns-appear-in-the-kwic-display",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How does the frequency of word X differ across text types Z, and what patterns appear in the KWIC display?",
    "text": "Template: How does the frequency of word X differ across text types Z, and what patterns appear in the KWIC display?\n\nExample Questions:\n\nHow does the frequency of â€œthereforeâ€ differ between academic and newspaper texts, and what sentence positions does it occupy in each?\nHow does the frequency of contractions (e.g., â€œdonâ€™tâ€) vary between spoken and written corpora, and what patterns emerge in KWIC?\nHow does the frequency of â€œgetâ€ differ across fiction vs.Â academic writing, and what meanings predominate in each genre?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-what-is-the-frequency-distribution-of-grammatical-pattern-x-across-different-text-types-and-what-contexts-does-kwic-reveal",
    "href": "2025/assignments/hands-on-1/draft.html#template-what-is-the-frequency-distribution-of-grammatical-pattern-x-across-different-text-types-and-what-contexts-does-kwic-reveal",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: What is the frequency distribution of grammatical pattern X across different text types, and what contexts does KWIC reveal?",
    "text": "Template: What is the frequency distribution of grammatical pattern X across different text types, and what contexts does KWIC reveal?\n\nExample Questions:\n\nWhat is the frequency of â€œthere is/areâ€ constructions in spoken vs.Â written English, and what follows this pattern in each?\nHow frequent is the â€œnot onlyâ€¦but alsoâ€ construction across different registers, and what types of elements does it connect?\nWhat is the distribution of passive voice (â€œwas/were + past participleâ€) in news vs.Â academic texts, and what verbs commonly appear?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-has-the-usage-of-word-x-evolved-in-the-past-y-years-based-on-frequency-trends-and-concordance-evidence",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-has-the-usage-of-word-x-evolved-in-the-past-y-years-based-on-frequency-trends-and-concordance-evidence",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How has the usage of word X evolved in the past Y years based on frequency trends and concordance evidence?",
    "text": "Template: How has the usage of word X evolved in the past Y years based on frequency trends and concordance evidence?\n\nExample Questions:\n\nHow has the usage of â€œgayâ€ evolved from 1950-2020 based on frequency and changing contexts in concordance lines?\nHow has â€œliterallyâ€ changed in frequency and usage patterns over the past 30 years?\nHow has the word â€œviralâ€ evolved in meaning from 1990-2020 based on concordance evidence?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-what-are-the-typical-contexts-in-which-expression-x-appears-based-on-concordance-analysis",
    "href": "2025/assignments/hands-on-1/draft.html#template-what-are-the-typical-contexts-in-which-expression-x-appears-based-on-concordance-analysis",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: What are the typical contexts in which expression X appears based on concordance analysis?",
    "text": "Template: What are the typical contexts in which expression X appears based on concordance analysis?\n\nExample Questions:\n\nIn what contexts does the phrase â€œat the end of the dayâ€ typically appear, and is it more common in spoken or written English?\nWhat are the typical contexts for â€œon the other handâ€ and what usually precedes it?\nIn what contexts does â€œfrankly speakingâ€ appear, and what types of statements follow it?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-do-different-varieties-of-language-y-differ-in-frequency-and-contexts-of-feature-x",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-do-different-varieties-of-language-y-differ-in-frequency-and-contexts-of-feature-x",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How do different varieties of language Y differ in frequency and contexts of feature X?",
    "text": "Template: How do different varieties of language Y differ in frequency and contexts of feature X?\n\nExample Questions:\n\nHow do British and American English differ in the frequency and contexts of â€œquiteâ€?\nWhat is the frequency difference of â€œshallâ€ between British and American English, and in what contexts does each variety use it?\nHow do British and American English differ in using â€œat the weekendâ€ vs.Â â€œon the weekendâ€?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-what-patterns-emerge-when-comparing-the-frequency-and-kwic-results-for-synonyms-x-and-y",
    "href": "2025/assignments/hands-on-1/draft.html#template-what-patterns-emerge-when-comparing-the-frequency-and-kwic-results-for-synonyms-x-and-y",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: What patterns emerge when comparing the frequency and KWIC results for synonyms X and Y?",
    "text": "Template: What patterns emerge when comparing the frequency and KWIC results for synonyms X and Y?\n\nExample Questions:\n\nWhat patterns emerge when comparing â€œbigâ€ vs.Â â€œlargeâ€ in terms of frequency and the nouns they modify?\nHow do â€œbeginâ€ and â€œstartâ€ differ in frequency and grammatical patterns (begin to/begin -ing)?\nWhat differences appear between â€œbuyâ€ and â€œpurchaseâ€ in frequency across registers and typical objects?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-does-register-affect-both-the-frequency-and-typical-contexts-of-linguistic-feature-x",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-does-register-affect-both-the-frequency-and-typical-contexts-of-linguistic-feature-x",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How does register affect both the frequency and typical contexts of linguistic feature X?",
    "text": "Template: How does register affect both the frequency and typical contexts of linguistic feature X?\n\nExample Questions:\n\nHow does register (academic vs.Â conversational) affect the frequency and contexts of â€œhoweverâ€?\nHow do formal and informal registers differ in the frequency and usage of phrasal verbs like â€œfind outâ€ vs.Â â€œdiscoverâ€?\nHow does register influence the frequency and positioning of â€œmoreoverâ€ and â€œbesidesâ€?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-which-words-frequently-appear-near-word-x-based-on-manual-analysis-of-concordance-lines",
    "href": "2025/assignments/hands-on-1/draft.html#template-which-words-frequently-appear-near-word-x-based-on-manual-analysis-of-concordance-lines",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: Which words frequently appear near word X based on manual analysis of concordance lines?",
    "text": "Template: Which words frequently appear near word X based on manual analysis of concordance lines?\n\nExample Questions:\n\nWhich words frequently appear immediately before and after â€œdecisionâ€ in business English?\nWhat words commonly appear within 3 words of â€œabsolutelyâ€ in spoken English?\nWhich adjectives most frequently appear before â€œconsequencesâ€ in news texts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#social-media-and-internet-language",
    "href": "2025/assignments/hands-on-1/draft.html#social-media-and-internet-language",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Social Media and Internet Language",
    "text": "Social Media and Internet Language\n\nExample Questions:\n\nHow has the frequency of â€œlolâ€ changed from 2000-2020, and in what contexts does it appear beyond informal communication?\nWhat is the frequency difference of â€œselfieâ€ before and after 2010, and what verbs commonly appear with it?\nHow do â€œemojiâ€ and â€œemoticonâ€ differ in frequency over time, and what contexts show their usage patterns?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#pop-culture-and-entertainment",
    "href": "2025/assignments/hands-on-1/draft.html#pop-culture-and-entertainment",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Pop Culture and Entertainment",
    "text": "Pop Culture and Entertainment\n\nExample Questions:\n\nHow does the frequency of â€œbinge-watchâ€ compare to â€œmarathonâ€ (in TV context) and when did each term become popular?\nWhat patterns emerge when comparing â€œK-popâ€ mentions across different text types and time periods?\nHow has â€œanimeâ€ increased in frequency in English corpora, and what words commonly co-occur with it?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#technology-and-gaming",
    "href": "2025/assignments/hands-on-1/draft.html#technology-and-gaming",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Technology and Gaming",
    "text": "Technology and Gaming\n\nExample Questions:\n\nHow do â€œappâ€ and â€œapplicationâ€ differ in frequency across registers, and which one dominates in informal contexts?\nWhat is the frequency evolution of â€œGoogleâ€ as a verb from 2000-2020, and what objects follow it?\nHow has â€œstream/streamingâ€ changed in meaning from 2000-2020 based on concordance contexts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#language-learning-and-education",
    "href": "2025/assignments/hands-on-1/draft.html#language-learning-and-education",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Language Learning and Education",
    "text": "Language Learning and Education\n\nExample Questions:\n\nHow do native speakers use â€œactuallyâ€ vs how it appears in academic writing by non-native speakers?\nWhat is the frequency of â€œI thinkâ€ vs â€œIn my opinionâ€ across spoken and written English?\nHow do apologetic expressions like â€œsorryâ€ and â€œexcuse meâ€ differ in frequency and contexts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#food-and-lifestyle",
    "href": "2025/assignments/hands-on-1/draft.html#food-and-lifestyle",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Food and Lifestyle",
    "text": "Food and Lifestyle\n\nExample Questions:\n\nHow has â€œbubble teaâ€ entered English usage, and what verbs are associated with it?\nWhat is the frequency difference between â€œsushiâ€ in the 1990s vs 2010s, and how have its contexts changed?\nHow do â€œramenâ€ and â€œnoodlesâ€ compare in frequency and what adjectives modify each?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#common-efl-learner-interests",
    "href": "2025/assignments/hands-on-1/draft.html#common-efl-learner-interests",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Common EFL Learner Interests",
    "text": "Common EFL Learner Interests\n\nExample Questions:\n\nHow does â€œdeadlineâ€ appear in different registers, and what verbs commonly precede it (meet/miss/extend)?\nWhat patterns show the difference between â€œtake a testâ€ vs â€œtake an examâ€ in various English varieties?\nHow do â€œpart-time jobâ€ and â€œinternshipâ€ differ in frequency and contexts across text types?"
  },
  {
    "objectID": "2025/assignments/final-project/index.html",
    "href": "2025/assignments/final-project/index.html",
    "title": "Final Project",
    "section": "",
    "text": "For the final assignment of the current 5-day intensive course, I have two possible plans we can discuss and decide together.\nLetâ€™s decide by the end of Day 2.",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/final-project/index.html#project-guidelines",
    "href": "2025/assignments/final-project/index.html#project-guidelines",
    "title": "Final Project",
    "section": "",
    "text": "For the final assignment of the current 5-day intensive course, I have two possible plans we can discuss and decide together.\nLetâ€™s decide by the end of Day 2.",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/final-project/index.html#option-a---conducting-a-separate-mini-project",
    "href": "2025/assignments/final-project/index.html#option-a---conducting-a-separate-mini-project",
    "title": "Final Project",
    "section": "Option A - Conducting a separate mini-project",
    "text": "Option A - Conducting a separate mini-project\n\nOption A is more extensive in that you will be asked to conduct a new mini-project using the toolkit you have learned throughout the course.\nGiven the limited time, however, this plan requires a lot of commitment to the present course and may not be feasible, (but we can try if youâ€™d like!).\nWe will discuss possible alternatives (like Option B below)",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/final-project/index.html#option-b---revisiting-one-of-the-completed-hands-on-assignments",
    "href": "2025/assignments/final-project/index.html#option-b---revisiting-one-of-the-completed-hands-on-assignments",
    "title": "Final Project",
    "section": "Option B - Revisiting one of the completed hands-on assignments",
    "text": "Option B - Revisiting one of the completed hands-on assignments\n\nAs this 5-day intensive course teaches you a lot of new techniques and approaches to analyze lingusitic data, it is important for us to revisit the already completed assignments and consolidate our skills.\nIn Option B, you will be asked to make a presentation on one of your previously completed hands-on assignments, clearly articulating the thinking process as well as potential extension of your approach.\nMore details will be provided on the first day of the course.",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/answer-key.html",
    "href": "2025/assignments/hands-on-1/answer-key.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/index.html",
    "href": "2025/assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "This section contains all course assignments and the final project.",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#overview",
    "href": "2025/assignments/index.html#overview",
    "title": "Assignments",
    "section": "",
    "text": "This section contains all course assignments and the final project.",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#assignments",
    "href": "2025/assignments/index.html#assignments",
    "title": "Assignments",
    "section": "Assignments",
    "text": "Assignments\n\nHands-on Assignment 1\nHands-on Assignment 2\nHands-on Assignment 3\nHands-on Assignment 4\nFinal Project",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#hands-on-activities",
    "href": "2025/assignments/index.html#hands-on-activities",
    "title": "Assignments",
    "section": "Hands-on activities",
    "text": "Hands-on activities\nEach of the four hands-on assignments are evaluated by two criteria: Submission and Quality.\n\n\n\n\n\n\n\n\nGrading Criteria\nDetails\nScore\n\n\n\n\nSubmission on time\nThe first submission is recieved in time.\n3 points\n\n\nQuality\nThe submission meets criteria. You can resubmit your work by resubmission deadlines.\n4 points Ã— 3 questions",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#hands-on-activity-grading-rubrics",
    "href": "2025/assignments/index.html#hands-on-activity-grading-rubrics",
    "title": "Assignments",
    "section": "Hands-on activity grading rubrics",
    "text": "Hands-on activity grading rubrics\n\n\n\n\n\n\n\nScore\nDescriptor\n\n\n\n\n4\n- All the required/essential components are addressed;  - The write-up is complete and easy to follow.\n\n\n3\n- One or two required/essential components are missing;  - The write-up lacks elaboration.\n\n\n2\n- Half of the required/essential components are missing;  - The write-up lacks critical information.\n\n\n1\n- Most of the required/essential components are missing;  - The write-up is outline based",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html",
    "href": "2025/assignments/hands-on-2/index.html",
    "title": "Hands-on Assignment 2",
    "section": "",
    "text": "Under Construction\n\n\n\nThis course website is currently under construction and will be ready for the class starting August 2nd, 2025. Content is being actively developed and updated.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html#task-1-a-japanese-word-frequency-list",
    "href": "2025/assignments/hands-on-2/index.html#task-1-a-japanese-word-frequency-list",
    "title": "Hands-on Assignment 2",
    "section": "Task 1: A Japanese word frequency List",
    "text": "Task 1: A Japanese word frequency List\n\nGoals\nThe goal of this task is to: - Construct a Japanese word frequency list.\n\n\nInstruction\n\nUse Aozora 1000 corpus.\nCreate frequency list using TagAnt and AntConc\nUnderstand frequency distributions using simple text analyzer.\n\n\n\n\n\n\n\nSubmission:\n\n\n\n\nA Japanese word frequency list (.txt or .tsv format)\nDescriptive paragraphs explaining the frequency distributions of Japanese language.\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\nincludes a frequency list of Japanese words based on Aozora 1000\nprovides a description of word-frequency patterns in Aozora 1000 using example words from each frequency bins",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html#task-2-hand-calculated-lexical-diversity-scores-and-taaled-results",
    "href": "2025/assignments/hands-on-2/index.html#task-2-hand-calculated-lexical-diversity-scores-and-taaled-results",
    "title": "Hands-on Assignment 2",
    "section": "Task 2: Hand-calculated lexical diversity scores and TAALED results",
    "text": "Task 2: Hand-calculated lexical diversity scores and TAALED results\n\nGoals\nThe goals of this task are: - to understand how classical lexical diversity scores behave given the simple textual examples - to compute more recent, robust alternatives to classical indices using TAALED. - to describe how texts from the GiG corpus behave in terms of lexical diversity.\n\n\nInstructions\n\nComplete the hand calculation of lexical diversity indices on the spreadsheet\nCompute recommended lexical diversity indices â€” MATTR and MTLD Original â€” using TAALED.\nReplicate Figure 4.19 in Durrant (2023, p.Â 72) with the two indices (i.e., MATTR and MTLD Original)\nDiscuss implication of the findings.\n\n\n\n\n\n\n\nSubmission:\n\n\n\n\nSpreadsheet file containing hand-calculated lexical diversity scores.\nPlots that contain results of the lexical diversity analysis.\nDescriptive paragraphs explaining the trends of lexical diversity (200-300 words).\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\nincludes a spreatsheet that contains lexical diversity (i.e., TTR, CTTR, MAAS) scores for the sample texts\nincludes a csv file\nprovides two sets of plots that describe trends of lexical diversity across year groups and genre.\nprovides comparison of the trends of lexical diversities you found against Figure 4.19 in Durrant (2023).",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html#task-3-qualitative-analysis-of-lexical-sophistication",
    "href": "2025/assignments/hands-on-2/index.html#task-3-qualitative-analysis-of-lexical-sophistication",
    "title": "Hands-on Assignment 2",
    "section": "Task 3: Qualitative analysis of lexical sophistication",
    "text": "Task 3: Qualitative analysis of lexical sophistication\n\nGoal\nThe goals of this task are to:\n\ncompute several important lexical sophistication indices\ncompare and contrast two texts using the selected indices\ndescribe the use of vocabulary in the two text based on the quantitative and qualitative information\n\n\n\nInstructions\n\nTwo texts from\nUsing the simple text analyzer, compare two texts based on two indices that you select.\nSelect one frequency-based index and aother type of index.\nInterpret the results of the analysis and describe the difference in a (few) paragraph(s).\n\n\n\n\n\n\n\nSubmission:\n\n\n\n\nPlots that contains results of the lexical sophistication analysis.\nDescriptive paragraph(s) contrasting two texts based on lexical sophistication (200-300 words).\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\nincludes plots from one frequency index and one other type of index\nprovides desciption of how two texts differ in terms of the selected lexical sophistication indices.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-3/index.html",
    "href": "2025/assignments/hands-on-3/index.html",
    "title": "Hands-on Assignment 3",
    "section": "",
    "text": "Under Construction\n\n\n\nThis course website is currently under construction and will be ready for the class starting August 2nd, 2025. Content is being actively developed and updated.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 3"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-3/index.html#task-1-describing-statistical-characteristics-of-collocations-4-points",
    "href": "2025/assignments/hands-on-3/index.html#task-1-describing-statistical-characteristics-of-collocations-4-points",
    "title": "Hands-on Assignment 3",
    "section": "Task 1: Describing statistical characteristics of collocations (4 points)",
    "text": "Task 1: Describing statistical characteristics of collocations (4 points)\nIn the first task, I would like you to calculate major Strengths Of Association (SOA) measures to quantify the association between two words (node words and their collocates.)\nThe frequency of node words, their collocates and entire corpus size will be given to you.\nYour task is to calculate T-score, MI, MI^2, and LogDice.\n\n\n\n\n\n\nSubmission\n\n\n\n\nA spreadsheet file with SOA values.\nA word file (.docx) for plots and prose descriptions.\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\ncontains accurate T-score, MI, MI^2 and LogDice scores\nprovides visualization of the relations between SOA indices\ndescribe the relationships among SOA indices and typical collocations",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 3"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-3/index.html#task-2-3-mini-research-project-8-points-altogether",
    "href": "2025/assignments/hands-on-3/index.html#task-2-3-mini-research-project-8-points-altogether",
    "title": "Hands-on Assignment 3",
    "section": "Task 2 & 3: Mini-research project (8 points altogether)",
    "text": "Task 2 & 3: Mini-research project (8 points altogether)\nThe task 2 and 3 are related to the mini-research project.\nIn this part of the assignment, you will conduct a mini-research project to describe uses of single- and multi-word units in a corpus you choose.\nSpecifically, you will:\n\nselect lexical richness or phraseological sophistication indices to answer a set of research questions\nanalyze the chosen corpus with the selected indices\npresent the results and interpretation in a written prose\n\n\nAssignment Guideline\n\n\nStep 1: Understand and choose the corpus\nIn this assignment, please choose one of the following corpora:\n\nGrowth in Grammar (GiG) corpus (Durrant, 2023)\nICNALE corpus (Edited Essay OR GRA)\nSome Japanese corpus here (Ask Masaki about availability).\n\n\n\nStep 2: Construct research questions\nIn this type of research, researchers typically set RQs about the relationships between lexical characteristncs and variables that defines subsection of the corpus (e.g., grade, genre, or proficieincy score).",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 3"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-3/index.html#the-following-information-is-available-through-the-gig-corpus",
    "href": "2025/assignments/hands-on-3/index.html#the-following-information-is-available-through-the-gig-corpus",
    "title": "Hands-on Assignment 3",
    "section": "The following information is available through the GiG corpus:",
    "text": "The following information is available through the GiG corpus:\nThe following information is available through the ICNALE corpus: - Ratings performed by external raters\n\nStep 3: Construct hypothesis\nBased on what youâ€™ve learned about the vocabulary use of learner, state several hypotheses that you expect as the findings for the research question.\nIn other words, what do you expect as the relationship between lexical characteristics X and external variable Y?\n\n\nStep 4: Select index\nBased on the RQs and hypotheses, you will select indices that can capture the lexical characteristics X in your corpus.\n\n\nStep 5: Compute the index\nYou will now use the tools we have covered in this course to derive lexical richness scores for the text.\n\n\nStep 6: Conduct analysis\nTo answer the research questions, you may want to do the followings: - Obtain descriptive statistics of the lexical richness indices - Visualize the relationship between variables - Optionally run statistical analyses\n\n\nStep 7: Interpret and write-up the results\n\n\n\n\n\n\nSubmission\n\n\n\n\nA word file which\n\ndescribe the selected lexical richness and phraseological indices;\nbriefly explain the content of the corpus you investigated;\nprovides analysis procesure\nprovides results of the analysis (e.g., visualization and/or statistical tests)\ndiscusses implication of your analysis\n\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\noutlines research questions and hypotheses\nprovide description of lexical richness measures that you used and how you calculated the measures\nprovides analysis results and their interpretations in relation to the research questions",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 3"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html",
    "href": "2025/assignments/hands-on-4/index.html",
    "title": "Hands-on Assignment 4",
    "section": "",
    "text": "Under Construction\n\n\n\nThis course website is currently under construction and will be ready for the class starting August 2nd, 2025. Content is being actively developed and updated.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 4"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html#task-1-research-questions-hypotheses-and-methods",
    "href": "2025/assignments/hands-on-4/index.html#task-1-research-questions-hypotheses-and-methods",
    "title": "Hands-on Assignment 4",
    "section": "Task 1: Research questions, Hypotheses and Methods",
    "text": "Task 1: Research questions, Hypotheses and Methods\nIn this task you will describe research questions, hypothesis, and methods.\n\nResearch questions\n\nResearch questions should include:\n\ntype of features you are looking at (e.g., adverbial clauses)\nsituational variables that defines your sub-corpora (e.g., grade, genre, proficiency)\n\n\n\n\nHypothesis\n\nYour research hypothesis should:\n\ndescribe your predictions in terms of:\n\nquantitative trends of the feature in relation to the factor you are interested in.\n\n\n\n\n\nDefinitions and operationalization of grammatical features to extract\n\nYou must describe the specific grammatical features that you plan to extract.\nFor example, for clausal features you need to specify if:\n\nyou are interested in subordinate clauses or embedded clauses\nyou are interested in particular type of clauses\n\nDescription of rules to identify desirable linguistic feature.\n\nFor example, you will need to specify amod for dependency label to extract adjective + noun phrase.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 4"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html#task-2-fine-grained-descriptive-grammatical-features",
    "href": "2025/assignments/hands-on-4/index.html#task-2-fine-grained-descriptive-grammatical-features",
    "title": "Hands-on Assignment 4",
    "section": "Task 2: Fine-grained Descriptive grammatical features",
    "text": "Task 2: Fine-grained Descriptive grammatical features\n\nOnce you articulated the information above, you will now conduct a search over the corpus.\nYou should use either simple text analyzer or your own Colab Notebook.\n\nI will specify which option should be used by the time we start working on this assignment (that is, this depends on your progress as a group.)",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 4"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html#task-3-results-and-interpretation",
    "href": "2025/assignments/hands-on-4/index.html#task-3-results-and-interpretation",
    "title": "Hands-on Assignment 4",
    "section": "Task 3: Results and interpretation",
    "text": "Task 3: Results and interpretation\n\nProvide the results of your corpus analysis in a way you think is most effective to address your research questions. Make effective use of tables, plots, or other data presentation technique as you think.\nProvide descriptive paragraphs to walk the reader through the results and how to interprete that results.\n\n\n\n\n\n\n\nSubmission\n\n\n\n\nA word file (.docx file) that addresses requirements in a written format (one or two pages depending on your analysis results.).\n\nScreenshots of your search settings on the simple text analyzer tool.\n\nIF you use colab, Google Colab notebook (.ipynb file) with extraction code and results.\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\noutlines research questions and hypotheses\nprovide description of your approach (algorithms and rules) to identify the desired linguistic structure.\nprovides analysis results and their interpretations in relation to the research questions",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 4"
    ]
  },
  {
    "objectID": "2025/slides/session-9.html#learning-objectives",
    "href": "2025/slides/session-9.html#learning-objectives",
    "title": "Session 9: Research Application",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nJustify choices of lexical richness measures to investigate a research questions\nConduct a simple statistical analysis of selected corpus on small sets of lexical measures using JASP software"
  },
  {
    "objectID": "2025/slides/session-9.html#available-corpus",
    "href": "2025/slides/session-9.html#available-corpus",
    "title": "Session 9: Research Application",
    "section": "Available corpus",
    "text": "Available corpus\n\nLetâ€™s use GiG corpus by Durrant (2023)."
  },
  {
    "objectID": "2025/slides/session-10.html#learning-objectives",
    "href": "2025/slides/session-10.html#learning-objectives",
    "title": "Session 10: Analyzing Grammar",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nProvide historical overview of the syntactic complexity research\nDescribe different approaches to grammatical features:\n\nGrammatical complexity strand\nFine-grained grammatical complexity strand\nDescriptive (register-based analysis) strand\nVerb Argument Construction (VAC) strand\n\nUnderstand current trends of syntactic complexity research"
  },
  {
    "objectID": "2025/slides/session-10.html#grammatical-complexity",
    "href": "2025/slides/session-10.html#grammatical-complexity",
    "title": "Session 10: Analyzing Grammar",
    "section": "Grammatical complexity",
    "text": "Grammatical complexity\n\nIndicators of\n\nproficiency\ndevelopment"
  },
  {
    "objectID": "2025/slides/session-10.html#grammatical-complexity-strand",
    "href": "2025/slides/session-10.html#grammatical-complexity-strand",
    "title": "Session 10: Analyzing Grammar",
    "section": "Grammatical complexity strand",
    "text": "Grammatical complexity strand\n\nLength-based indices\n\nNumber of words per T-unit\n\nSubordination"
  },
  {
    "objectID": "2025/slides/session-10.html#t-unit",
    "href": "2025/slides/session-10.html#t-unit",
    "title": "Session 10: Analyzing Grammar",
    "section": "T-unit",
    "text": "T-unit\n\nWhen it comes to measuring grammatical complexity, one important consideration is how to define linguistic unit.\nWriting researchers use T-Unit (Hunt, 1966)\n\nT-unit: â€œIndependent clause plus any subordinate clauses attached to itâ€"
  },
  {
    "objectID": "2025/slides/session-10.html#typical-grammatical-complexity-measures",
    "href": "2025/slides/session-10.html#typical-grammatical-complexity-measures",
    "title": "Session 10: Analyzing Grammar",
    "section": "Typical grammatical complexity measures",
    "text": "Typical grammatical complexity measures\n\ncomplexity measures reported in Kyle & Crossley (2018)"
  },
  {
    "objectID": "2025/slides/session-10.html#findings",
    "href": "2025/slides/session-10.html#findings",
    "title": "Session 10: Analyzing Grammar",
    "section": "Findings",
    "text": "Findings"
  },
  {
    "objectID": "2025/slides/session-10.html#organic-approach-norris-ortega-2009",
    "href": "2025/slides/session-10.html#organic-approach-norris-ortega-2009",
    "title": "Session 10: Analyzing Grammar",
    "section": "Organic approach (Norris & Ortega, 2009)",
    "text": "Organic approach (Norris & Ortega, 2009)"
  },
  {
    "objectID": "2025/slides/session-10.html#fine-grained-grammatical-complexity-strand",
    "href": "2025/slides/session-10.html#fine-grained-grammatical-complexity-strand",
    "title": "Session 10: Analyzing Grammar",
    "section": "Fine-grained grammatical complexity strand",
    "text": "Fine-grained grammatical complexity strand\nCriticism on the (largely) length-based grammatical complexity:\n\nDoes not tell us about how sentence structures are elaborated"
  },
  {
    "objectID": "2025/slides/session-10.html#length-based-indices-does-not-tell-elaboration-strategy",
    "href": "2025/slides/session-10.html#length-based-indices-does-not-tell-elaboration-strategy",
    "title": "Session 10: Analyzing Grammar",
    "section": "Length-based indices does not tell elaboration strategy",
    "text": "Length-based indices does not tell elaboration strategy\n\n\nThe athletic man in the jersey kicked the ball over the fence.\n\n\nElaborated by phrases (adjectival modification; prepositional phrases)\n\n\nBecause he wanted to score a goal, the man kicked the ball.\n\n\nElaborated by subordinate clause.\n\n\nExample from Kyle & Crossley (2018)"
  },
  {
    "objectID": "2025/slides/session-10.html#kyle-crossley-2018",
    "href": "2025/slides/session-10.html#kyle-crossley-2018",
    "title": "Session 10: Analyzing Grammar",
    "section": "Kyle & Crossley (2018)",
    "text": "Kyle & Crossley (2018)\n\nKyle & Crossley (2018) proposed fine-grained clausal & phrasal complexity indices\nThey used dependency parsing to identify fine-grained features of grammar."
  },
  {
    "objectID": "2025/slides/session-10.html#clausal-indices",
    "href": "2025/slides/session-10.html#clausal-indices",
    "title": "Session 10: Analyzing Grammar",
    "section": "Clausal indices",
    "text": "Clausal indices\nThe followings are example:\n\n\n\n\nStructure\nDependency tag\nExample of Structure\n\n\n\n\nNominal subject\nnsubj\nThe athlete ran quickly.\n\n\nDirect object\ndobj\nHe plays soccer.\n\n\nIndirect object\niobj\nHe teaches me soccer.\n\n\nClausal complement\nccomp\nI am certain that he did it.\n\n\nAdjectival complement\nacomp\nHe looks fine.\n\n\nNominal complement\nncomp\nShe is a teacher."
  },
  {
    "objectID": "2025/slides/session-10.html#clausal-indices-oblique",
    "href": "2025/slides/session-10.html#clausal-indices-oblique",
    "title": "Session 10: Analyzing Grammar",
    "section": "Clausal indices (Oblique)",
    "text": "Clausal indices (Oblique)\nThe followings are example:\n\n\n\n\nStructure\nAbbreviation\nExample of Structure\n\n\n\n\nAdverbial modifier\nadvmod\nAccordingly, I ate pizza.\n\n\nPrepositional modifier\nprep\nThey went into the score.\n\n\nTemporal modifier\ntmod\nLast night, we had fun.\n\n\nAdverbial clause\nadvcl\nThe accident happened as night fell.\n\n\nOpen clausal complement\nxcomp\nI am ready to leave."
  },
  {
    "objectID": "2025/slides/session-10.html#phrasal-indices",
    "href": "2025/slides/session-10.html#phrasal-indices",
    "title": "Session 10: Analyzing Grammar",
    "section": "Phrasal indices",
    "text": "Phrasal indices\n\nPhrasal indices counts how many dependents there are for each of the following structure: nsubj, nsubj_pass, agent, ncomp, dobj, iobj, pobj.\n\n\n\n\n\nStructure\nAbbreviation\nExample of Structure\n\n\n\n\nDeterminers\ndet\nThe man went into the store.\n\n\nPrepositional phrases\nprep\nthe man in the red hat.\n\n\nAdjective modifier\namod\nThe man in the red hat\n\n\nPossessives\nposs\nTomâ€™s store; his intention\n\n\nRelative clause modifiers\nrecmod\nthe plan I thought\n\n\nAdverbial modifiers\nadvmod\nItâ€™s a really good idea."
  },
  {
    "objectID": "2025/slides/session-10.html#kyle-crossley-2018-1",
    "href": "2025/slides/session-10.html#kyle-crossley-2018-1",
    "title": "Session 10: Analyzing Grammar",
    "section": "Kyle & Crossley (2018)",
    "text": "Kyle & Crossley (2018)\n\nThey counted fine-grained clausal and phrasal indices.\n# dependents per clause/phrase\nThey examined correlations between TOEFL score and the fine-grained indices"
  },
  {
    "objectID": "2025/slides/session-10.html#results",
    "href": "2025/slides/session-10.html#results",
    "title": "Session 10: Analyzing Grammar",
    "section": "Results",
    "text": "Results\n\nFinal regression model"
  },
  {
    "objectID": "2025/slides/session-10.html#example-sentences",
    "href": "2025/slides/session-10.html#example-sentences",
    "title": "Session 10: Analyzing Grammar",
    "section": "Example sentences",
    "text": "Example sentences\n\n\n\nExample from score 1 essay\n\n\n\n\n\n\n\nExample from score 5 essay\n\n\n\n\n\n\nExample from score 5 essay"
  },
  {
    "objectID": "2025/slides/session-10.html#kyle-crossley-2018-2",
    "href": "2025/slides/session-10.html#kyle-crossley-2018-2",
    "title": "Session 10: Analyzing Grammar",
    "section": "Kyle & Crossley (2018)",
    "text": "Kyle & Crossley (2018)\n\nThe fine-grained indices:\n\nexplained more variances (~ 20%) than traditional complexity measures (~ 5%)\nprovides more insights into what structure the learners tend to use\n\nIn hands-on activity, we will talk more about how to identify fine-grained grammatical features."
  },
  {
    "objectID": "2025/slides/session-10.html#syntactic-sophistication-or-vac-strand",
    "href": "2025/slides/session-10.html#syntactic-sophistication-or-vac-strand",
    "title": "Session 10: Analyzing Grammar",
    "section": "Syntactic sophistication (or VAC) strand",
    "text": "Syntactic sophistication (or VAC) strand\nAccording to construction grammar (Goldberg, 1995, 2006), grammatical construction (structure) convey abstract linguistic meaning.\nIn turn,\n\nIntransitive:\nTransitive:"
  },
  {
    "objectID": "2025/slides/session-10.html#descriptive-register-based-feature-strand",
    "href": "2025/slides/session-10.html#descriptive-register-based-feature-strand",
    "title": "Session 10: Analyzing Grammar",
    "section": "Descriptive (register-based feature) strand",
    "text": "Descriptive (register-based feature) strand"
  },
  {
    "objectID": "2025/slides/session-13.html#learning-objectives",
    "href": "2025/slides/session-13.html#learning-objectives",
    "title": "Session 12: Hands-on Activity",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nDescribe how LLMs are trained generally and what LLMs do to produce language.\nExplain the benefits and drawbacks of using LLMs for linguistic annotation.\nDemonstrate/discuss potential impacts of prompts on the LLMs performance on linguistic annotation.\nDesign an experiment to investigate LLMs output accuracy on a given annotation task."
  },
  {
    "objectID": "2025/slides/session-3.html#learning-objectives",
    "href": "2025/slides/session-3.html#learning-objectives",
    "title": "Session 3: Hands-on #1",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nConduct KWIC searches on English-Corpora.org\nSort KWIC search results to obtain qualitative observation about language use\nUse advanced search strings such as regular expression to fine-tune the search results"
  },
  {
    "objectID": "2025/slides/session-3.html#task-a---simple-word-search",
    "href": "2025/slides/session-3.html#task-a---simple-word-search",
    "title": "Session 3: Hands-on #1",
    "section": "Task A - Simple word search",
    "text": "Task A - Simple word search"
  },
  {
    "objectID": "2025/slides/session-3.html#first-corpus-search",
    "href": "2025/slides/session-3.html#first-corpus-search",
    "title": "Session 3: Hands-on #1",
    "section": "First corpus search",
    "text": "First corpus search\n\nLetâ€™s start our journey.\nFirst of all, letâ€™s search the following word in COCA.\n\nrun"
  },
  {
    "objectID": "2025/slides/session-3.html#result-of-run",
    "href": "2025/slides/session-3.html#result-of-run",
    "title": "Session 3: Hands-on #1",
    "section": "Result of run",
    "text": "Result of run\n\nYou might get slightly different results.\n\n\nSearch - run"
  },
  {
    "objectID": "2025/slides/session-3.html#lemma-search",
    "href": "2025/slides/session-3.html#lemma-search",
    "title": "Session 3: Hands-on #1",
    "section": "LEMMA search",
    "text": "LEMMA search\n\nLemma is a group of word form for the headword with grammatical inflection.\nYou can search LEMMA in English-Corpora.org through Capital letters.\nThe search methods will depend on the corpus tool you use."
  },
  {
    "objectID": "2025/slides/session-3.html#lemma-search-1",
    "href": "2025/slides/session-3.html#lemma-search-1",
    "title": "Session 3: Hands-on #1",
    "section": "LEMMA search",
    "text": "LEMMA search\n\nSearch - consider"
  },
  {
    "objectID": "2025/slides/session-3.html#task-a.2---pos-search",
    "href": "2025/slides/session-3.html#task-a.2---pos-search",
    "title": "Session 3: Hands-on #1",
    "section": "Task A.2 - POS search",
    "text": "Task A.2 - POS search"
  },
  {
    "objectID": "2025/slides/session-3.html#specifying-part-of-speech-pos",
    "href": "2025/slides/session-3.html#specifying-part-of-speech-pos",
    "title": "Session 3: Hands-on #1",
    "section": "Specifying Part Of Speech (POS)",
    "text": "Specifying Part Of Speech (POS)\n\nWhen you want to specify POS, you will need to add the following tags.\n\n\n\n\nCategory\nSimple tag\nSymbol\nExample\n\n\n\n\nCommon noun\nNOUN\nN\nsun\n\n\nProper Nouns\nNAME\nNP\nSendai\n\n\nAll nouns\nNOUN+\nN+\nsun, Sendai\n\n\nLexical verbs\nVERB\nV\nrun\n\n\nAll verbs\nVERB+\nV\nrun, do"
  },
  {
    "objectID": "2025/slides/session-3.html#specifying-part-of-speech-pos-1",
    "href": "2025/slides/session-3.html#specifying-part-of-speech-pos-1",
    "title": "Session 3: Hands-on #1",
    "section": "Specifying Part Of Speech (POS)",
    "text": "Specifying Part Of Speech (POS)\n\n\n\nCategory\nSimple tag\nSymbol\nExample\n\n\n\n\nAdjectives\nADJ\nJ\nsimple\n\n\nAdverbs\nADV\nR\nclear\n\n\nPreposition\nPREP\n-\nin\n\n\nArticles\nART\n-\nher\n\n\nDeterminers\nDET\n-\nthese\n\n\nConjunctions\nCONJ\n-\nthese\n\n\nConjunctions\nCONJ\n-\nthese"
  },
  {
    "objectID": "2025/slides/session-3.html#the-word-run-used-as-noun",
    "href": "2025/slides/session-3.html#the-word-run-used-as-noun",
    "title": "Session 3: Hands-on #1",
    "section": "the word run used as noun",
    "text": "the word run used as noun\n\nSearch occurrences of run that are used as nouns."
  },
  {
    "objectID": "2025/slides/session-3.html#the-word-run-used-as-noun-1",
    "href": "2025/slides/session-3.html#the-word-run-used-as-noun-1",
    "title": "Session 3: Hands-on #1",
    "section": "the word run used as noun",
    "text": "the word run used as noun\n\nSearch occurrences of run that are used as nouns.\n\n\nrun - as noun"
  },
  {
    "objectID": "2025/slides/session-3.html#concordances-or-kwic",
    "href": "2025/slides/session-3.html#concordances-or-kwic",
    "title": "Session 3: Hands-on #1",
    "section": "Concordances (or KWIC)",
    "text": "Concordances (or KWIC)\n\nNow you know how many times XX occurs in COCA, you want to see the context in which XX occur.\nThis goal can be accomplished with KWIC search."
  },
  {
    "objectID": "2025/slides/session-3.html#kwic-view",
    "href": "2025/slides/session-3.html#kwic-view",
    "title": "Session 3: Hands-on #1",
    "section": "KWIC view",
    "text": "KWIC view\n\nGo back to SEARCH window.\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#kwic-view-1",
    "href": "2025/slides/session-3.html#kwic-view-1",
    "title": "Session 3: Hands-on #1",
    "section": "KWIC view",
    "text": "KWIC view\n\nClick on the + button in the search menu and enter your search word.\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#kwic-view-2",
    "href": "2025/slides/session-3.html#kwic-view-2",
    "title": "Session 3: Hands-on #1",
    "section": "KWIC view",
    "text": "KWIC view\n\nThe result is displayed. Default sort : R1 &gt; R2 &gt; R3\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#sorting-kwic-window",
    "href": "2025/slides/session-3.html#sorting-kwic-window",
    "title": "Session 3: Hands-on #1",
    "section": "Sorting KWIC window",
    "text": "Sorting KWIC window\n\nNow letâ€™s sort the results according to the position in context.\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#sorting-words",
    "href": "2025/slides/session-3.html#sorting-words",
    "title": "Session 3: Hands-on #1",
    "section": "Sorting words",
    "text": "Sorting words\nYou can sort the words.\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#lets-try-kwic",
    "href": "2025/slides/session-3.html#lets-try-kwic",
    "title": "Session 3: Hands-on #1",
    "section": "Letâ€™s Try: KWIC",
    "text": "Letâ€™s Try: KWIC\n\nChoose a word that you want to see the context for.\nSearch the word with KWIC.\nSort the word in the following way.\n\nDefault: R1 &gt; R2 &gt; R3\nCustom 1: L1 &gt; L2 &gt; L3\nCustom 2: L1 &gt; R1 &gt; R2"
  },
  {
    "objectID": "2025/slides/session-3.html#how-to-get-custom-1-and-2",
    "href": "2025/slides/session-3.html#how-to-get-custom-1-and-2",
    "title": "Session 3: Hands-on #1",
    "section": "How to get Custom 1 and 2",
    "text": "How to get Custom 1 and 2\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#chart-function",
    "href": "2025/slides/session-3.html#chart-function",
    "title": "Session 3: Hands-on #1",
    "section": "Chart function",
    "text": "Chart function\n\nNow that we learned how to conduct concordance search, letâ€™s experiment with some CHART function.\nThis allows you to return frequency by sections of of COCA (= conditional frequency)."
  },
  {
    "objectID": "2025/slides/session-3.html#search-lol-using-chart",
    "href": "2025/slides/session-3.html#search-lol-using-chart",
    "title": "Session 3: Hands-on #1",
    "section": "Search lol using CHART",
    "text": "Search lol using CHART\n\nGo to Search Tab, select, CHART and enter lol"
  },
  {
    "objectID": "2025/slides/session-3.html#search-lol-using-chart-1",
    "href": "2025/slides/session-3.html#search-lol-using-chart-1",
    "title": "Session 3: Hands-on #1",
    "section": "Search lol using CHART",
    "text": "Search lol using CHART\n\nYou will get the following:\n\n\nSearch - lol"
  },
  {
    "objectID": "2025/slides/session-3.html#regular-expressions",
    "href": "2025/slides/session-3.html#regular-expressions",
    "title": "Session 3: Hands-on #1",
    "section": "Regular expressions",
    "text": "Regular expressions\nRegular expression (æ­£è¦è¡¨ç¾) allows you to search corpus through â€œpattern matchingâ€.\n\nHave you ever used (*) asterisk in your web search?\nThis is one of the regular expression (= wild card)"
  },
  {
    "objectID": "2025/slides/session-3.html#using-wild-card-in-search",
    "href": "2025/slides/session-3.html#using-wild-card-in-search",
    "title": "Session 3: Hands-on #1",
    "section": "Using wild card in search",
    "text": "Using wild card in search\n\nLetâ€™s go back to List search.\nEnter â€œa * of theâ€\nWhat result do you expect with this search?\nDonâ€™t turn to the next page YET!!!"
  },
  {
    "objectID": "2025/slides/session-3.html#result-with-a-of-the",
    "href": "2025/slides/session-3.html#result-with-a-of-the",
    "title": "Session 3: Hands-on #1",
    "section": "Result with a * of the",
    "text": "Result with a * of the\n\n\n\n\na * of the"
  },
  {
    "objectID": "2025/slides/session-3.html#more-search-methods",
    "href": "2025/slides/session-3.html#more-search-methods",
    "title": "Session 3: Hands-on #1",
    "section": "More search methods",
    "text": "More search methods\n\nYou can check how to use other search methods in English-Corpora.org here"
  },
  {
    "objectID": "2025/slides/session-3.html#examples",
    "href": "2025/slides/session-3.html#examples",
    "title": "Session 3: Hands-on #1",
    "section": "Examples",
    "text": "Examples\n\nWhich of the following adjectives are more frequent â€” exciting or excited?\nIs there any reletitive phrase with three adjectives?"
  },
  {
    "objectID": "2025/slides/session-3.html#adj-x-3",
    "href": "2025/slides/session-3.html#adj-x-3",
    "title": "Session 3: Hands-on #1",
    "section": "ADJ x 3",
    "text": "ADJ x 3\n\nthree adjectives\nNote: Sometimes the search results will contain parsing errors. (We will come back to this topic on Day 4.)"
  },
  {
    "objectID": "2025/slides/session-3.html#constructing-questions",
    "href": "2025/slides/session-3.html#constructing-questions",
    "title": "Session 3: Hands-on #1",
    "section": "Constructing questions",
    "text": "Constructing questions\nAre there any questions you would like to conduct?"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search",
    "href": "2025/slides/session-3.html#collocation-search",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nThis is main topic for Day 3.\nBriefly, collocates search allows us to search for co-occurring words within specified window."
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-1",
    "href": "2025/slides/session-3.html#collocation-search-1",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nSelect collocates\n\n\ncollocation"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-2",
    "href": "2025/slides/session-3.html#collocation-search-2",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nEnter search word\n\n\nentering words"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-3",
    "href": "2025/slides/session-3.html#collocation-search-3",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nOptional enter the word to look for\n\n\nenter collocates"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-4",
    "href": "2025/slides/session-3.html#collocation-search-4",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nSpecify window\n\n\nHow distant do you search for the collocates\n\n\nwindow"
  },
  {
    "objectID": "2025/slides/session-4.html#learning-objectives",
    "href": "2025/slides/session-4.html#learning-objectives",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nExplain the purposes of linguistic measures\nList commonly used lexical measures in second language acquisition research\nExplain sub-constructs of lexical richness measures\n\nLexical Diversity\nLexical Sophistication"
  },
  {
    "objectID": "2025/slides/coca_table.html",
    "href": "2025/slides/coca_table.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenre\n# texts\n# words\nExplanation\n\n\n\n\nSpoken\n44,803\n127,396,932\nTranscripts of unscripted conversation from more than 150 different TV and radio programs (examples: All Things Considered (NPR), Newshour (PBS), Good Morning America (ABC), Oprah)\n\n\nFiction\n25,992\n119,505,305\nShort stories and plays from literary magazines, childrenâ€™s magazines, popular magazines, first chapters of first edition books 1990-present, and fan fiction.\n\n\nMagazines\n86,292\n127,352,030\nNearly 100 different magazines, with a good mix between specific domains like news, health, home and gardening, women, financial, religion, sports, etc.\n\n\nNewspapers\n90,243\n122,958,016\nNewspapers from across the US, including: USA Today, New York Times, Atlanta Journal Constitution, San Francisco Chronicle, etc. Good mix between different sections of the newspaper, such as local news, opinion, sports, financial, etc.\n\n\nAcademic\n26,137\n120,988,361\nMore than 200 different peer-reviewed journals. These cover the full range of academic disciplines, with a good balance among education, social sciences, history, humanities, law, medicine, philosophy/religion, science/technology, and business\n\n\nWeb (Genl)\n88,989\n129,899,427\nClassified into the web genres of academic, argument, fiction, info, instruction, legal, news, personal, promotion, review web pages (by Serge Sharoff). Taken from the US portion of the GloWbE corpus.\n\n\nWeb (Blog)\n98,748\n125,496,216\nTexts that were classified by Google as being blogs. Further classified into the web genres of academic, argument, fiction, info, instruction, legal, news, personal, promotion, review web pages. Taken from the US portion of the GloWbE corpus.\n\n\nTV/Movies\n23,975\n129,293,467\nSubtitles from OpenSubtitles.org, and later the TV and Movies corpora. Studies have shown that the language from these shows and movies is even more colloquial / core than the data in actual â€œspoken corporaâ€.\n\n\nTotal\n485,179\n1,002,889,754"
  },
  {
    "objectID": "2025/slides/session-6.html#learning-objectives",
    "href": "2025/slides/session-6.html#learning-objectives",
    "title": "Session 6: Hands-on activity #3",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, you will be able to:\n\n\n\nCompute simple lexical diversity measures using spreadsheet software\nCompute advanced lexical diversity measures using TAALED\nExplain how modern lexical diversity measures are calculated\nCalculate simple lexical sophistication measures using dedicated web application\nDescribe how lexical sophistication measures behave on a single input text.\nDiscuss benefits and drawbacks of lexical richness measures."
  },
  {
    "objectID": "2025/slides/session-6.html#terminology",
    "href": "2025/slides/session-6.html#terminology",
    "title": "Session 6: Hands-on activity #3",
    "section": "Terminology",
    "text": "Terminology\nIn this presentation, I will use the following terms:\n\nInput text: The text you want to analyze (e.g., learner produce text)."
  },
  {
    "objectID": "2025/slides/session-6.html#lexical-diversity",
    "href": "2025/slides/session-6.html#lexical-diversity",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lexical Diversity",
    "text": "Lexical Diversity\n\nLexical Diversity is computed internally to text.\ne.g., Type-Token Ratio:\n\nCount the number of unique word (Type) in the input text\nCount the number of total word (Token) in the input text\nDevide Type by token."
  },
  {
    "objectID": "2025/slides/session-6.html#lexical-sophistication",
    "href": "2025/slides/session-6.html#lexical-sophistication",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lexical Sophistication",
    "text": "Lexical Sophistication\n\nLexical Sophistication requires external resources to compute.\nTo derive a frequency index:\n\nCompile a frequency list\nFor each word in the input text, retrieve frequency score from the list\n\ne.g., tree â€“&gt; 1,0000\n\nAverage the frequency scores (out of item awarded the score)"
  },
  {
    "objectID": "2025/slides/session-6.html#computing-simple-lexical-diversity-measures-by-hand",
    "href": "2025/slides/session-6.html#computing-simple-lexical-diversity-measures-by-hand",
    "title": "Session 6: Hands-on activity #3",
    "section": "Computing simple Lexical Diversity measures by hand",
    "text": "Computing simple Lexical Diversity measures by hand"
  },
  {
    "objectID": "2025/slides/session-6.html#simple-text-example",
    "href": "2025/slides/session-6.html#simple-text-example",
    "title": "Session 6: Hands-on activity #3",
    "section": "Simple Text Example",
    "text": "Simple Text Example\nCound the type and token of the following texts.\n\n\n\n\n\n\n\nID\nText\n\n\n\n\nText 1\nâ€œThe dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.â€\n\n\nText 2\nâ€œA curious fox trotted briskly through the meadow, leaping over mossy logs, sniffing wildflowers, and vanishing into golden twilight.â€\n\n\n\nNote : Texts were generated by GPT for illustration purposes."
  },
  {
    "objectID": "2025/slides/session-6.html#simple-text-example-1",
    "href": "2025/slides/session-6.html#simple-text-example-1",
    "title": "Session 6: Hands-on activity #3",
    "section": "Simple Text Example",
    "text": "Simple Text Example\n\n\n\n\n\n\n\n\n\nID\nText\nToken\nType\n\n\n\n\nText 1\nâ€œThe dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.â€\n19\n8\n\n\nText 2\nâ€œA curious fox trotted briskly through the meadow, leaping over mossy logs, sniffing wildflowers, and vanishing into golden twilight.â€\n19\n19\n\n\n\nNote : Texts were generated by GPT for illustration purposes."
  },
  {
    "objectID": "2025/slides/session-6.html#impact-of-text-lengths",
    "href": "2025/slides/session-6.html#impact-of-text-lengths",
    "title": "Session 6: Hands-on activity #3",
    "section": "Impact of Text lengths",
    "text": "Impact of Text lengths\n\n\n\n\n\n\n\n\n\n\nID\nText\nToken\nType\n\n\n\n\nText 1a\nâ€œThe dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.â€\n19\n8\n\n\nText 1b\nâ€œThe dog ran. The dog jumped. The dog barked. The dog played. The dog ran quickly. The dog jumped so high. The dog barked very loudly. The dog played, sat, and rolled. The dog sneezed. The dog ate the food.â€\n40\n18\n\n\nText 1c\nâ€œThe parrot squawked loudly. The parrot chirped again. A toucan perched nearby. The parrot fluttered. Wings flapped softly. The parrot chirped again. Feathers shimmered under sunlight. The crow cawed. The parrot glided low. The air shimmered. The owl blinked slowly. The parrot perched again. The owl blinked slowly. The parrot shrieked. The parrot chirped nearby again. The parrot squawked again.â€\n60\n27\n\n\n\nNote : Texts were generated by GPT for illustration purposes."
  },
  {
    "objectID": "2025/slides/session-6.html#lets-calculate-some-classical-lexical-diversity-indices",
    "href": "2025/slides/session-6.html#lets-calculate-some-classical-lexical-diversity-indices",
    "title": "Session 6: Hands-on activity #3",
    "section": "Letâ€™s calculate some classical Lexical Diversity indices",
    "text": "Letâ€™s calculate some classical Lexical Diversity indices\n\nOpen Google Spreadsheet\nCalculate the lexical diversity indices on the next page."
  },
  {
    "objectID": "2025/slides/session-6.html#some-classic-lexical-diversity",
    "href": "2025/slides/session-6.html#some-classic-lexical-diversity",
    "title": "Session 6: Hands-on activity #3",
    "section": "Some classic lexical diversity",
    "text": "Some classic lexical diversity\nWe calculate this for illustration but NEVER use these in your study.\n\n\\(TTR = {nType \\over nToken}\\)\n\\(RootTTR = {nType \\over \\sqrt{nToken}}\\)\n\\(LogTTR = {\\log(nType) \\over \\log(nToken)}\\)\n\\(Maas = {\\log(nTokens) - \\log(nTypes) \\over \\log(nToken)^2}\\)"
  },
  {
    "objectID": "2025/slides/session-6.html#what-should-we-actually-use-then",
    "href": "2025/slides/session-6.html#what-should-we-actually-use-then",
    "title": "Session 6: Hands-on activity #3",
    "section": "What should we actually use then?",
    "text": "What should we actually use then?\n\nThe Measure of Textual Lexical Diversity (MTLD)\nMoving-Average Type Token Ratio (MATTR)\n\nâ†’ These are shown as more robust indices of LD."
  },
  {
    "objectID": "2025/slides/session-6.html#using-taaled-desktop-version",
    "href": "2025/slides/session-6.html#using-taaled-desktop-version",
    "title": "Session 6: Hands-on activity #3",
    "section": "Using TAALED desktop version",
    "text": "Using TAALED desktop version\n\nWe can use Tool for the Automatic Analysis of Lexical Diversity (TAALED)\nDownload it to your computer and we will use it to compute modern LD measures\n\n\nTAALED"
  },
  {
    "objectID": "2025/slides/session-6.html#setting-up",
    "href": "2025/slides/session-6.html#setting-up",
    "title": "Session 6: Hands-on activity #3",
    "section": "Setting up",
    "text": "Setting up\n\nClick the software icon after download\nFor mac users, the system will issue warning, you must follow the following step:\n\nGo to setting and select Privacy & Security\nIf you have already attempted to open the software, there will be Open Anyway button.\nClick Open Anyway and that will allow Mac to open the software."
  },
  {
    "objectID": "2025/slides/session-6.html#selecting-indics",
    "href": "2025/slides/session-6.html#selecting-indics",
    "title": "Session 6: Hands-on activity #3",
    "section": "Selecting indics",
    "text": "Selecting indics\nYou can then wait for the TAALED app to start up.\n\nTAALED"
  },
  {
    "objectID": "2025/slides/session-6.html#options",
    "href": "2025/slides/session-6.html#options",
    "title": "Session 6: Hands-on activity #3",
    "section": "Options",
    "text": "Options\nWord analysis options\n\nAll words: Conduct analysis including all words.\nContent words: Conduct analysis with content words only.\nFunction words: Conduct analysis with function words only."
  },
  {
    "objectID": "2025/slides/session-6.html#options-1",
    "href": "2025/slides/session-6.html#options-1",
    "title": "Session 6: Hands-on activity #3",
    "section": "Options",
    "text": "Options\nIndex selection\nSelect the indices you need in the results. Three variants of MTLD are available.\n\nMTLD Original:\nMTLD MA Bi: Moving Average Bidirectional\nMTLD MA Wrap: If there is words left in the final factor, come back to the first part and complete the analysis."
  },
  {
    "objectID": "2025/slides/session-6.html#options-2",
    "href": "2025/slides/session-6.html#options-2",
    "title": "Session 6: Hands-on activity #3",
    "section": "Options",
    "text": "Options\nInput and output options\n\nYou can choose the input folder by selection\n\nOutput option\n\nTicking the Individual Item Output button allows you to have POS analysis\n\nparent_cw_nn\nand_fw\nteacher_cw_nn\ndisagree_cw_vb\nthat_fw"
  },
  {
    "objectID": "2025/slides/session-6.html#run-the-analysis",
    "href": "2025/slides/session-6.html#run-the-analysis",
    "title": "Session 6: Hands-on activity #3",
    "section": "Run the analysis",
    "text": "Run the analysis\n\nPress Process Texts and wait the following display.\n\n\nAnalysis complete"
  },
  {
    "objectID": "2025/slides/session-6.html#lets-take-a-look-at-the-csv-file",
    "href": "2025/slides/session-6.html#lets-take-a-look-at-the-csv-file",
    "title": "Session 6: Hands-on activity #3",
    "section": "Letâ€™s take a look at the csv file",
    "text": "Letâ€™s take a look at the csv file\n\nWhatâ€™s CSV?\n\nCSV (Comma Separated Values) file is a file extension like others (txt, docx).\nIt allows table like representation of data (like excel) separated by comma.\n\n\nThe Raw data (if you open it with text editor) should look like the following:\nfilename,basic_ntokens,basic_ntypes,basic_ncontent_tokens,basic_ncontent_types,basic_nfunction_tokens,basic_nfunction_types,lexical_density_types,lexical_density_tokens,maas_ttr_aw,mattr50_aw,hdd42_aw,mtld_original_aw,mtld_ma_bi_aw,mtld_ma_wrap_aw\nW_CHN_PTJ0_004_B1_2_ORIG.txt,267,124,135,75,132,49,0.6048387096774194,0.5056179775280899,0.056571333957257205,0.7793577981651377,0.7981161136859327,68.68659119235562,65.84622666144406,61.50561797752809\nW_JPN_SMK0_015_B1_2_ORIG.txt,302,138,129,82,173,56,0.5942028985507246,0.4271523178807947,0.05530143602594381,0.777865612648221,0.7974803670481457,68.38677597714803,67.6645170484911,65.22185430463576"
  },
  {
    "objectID": "2025/slides/session-6.html#opening-csv-file-in-a-spreadsheet-software",
    "href": "2025/slides/session-6.html#opening-csv-file-in-a-spreadsheet-software",
    "title": "Session 6: Hands-on activity #3",
    "section": "Opening csv file in a spreadsheet software",
    "text": "Opening csv file in a spreadsheet software\n\nYou can open csv file in Excel (or any other spreadsheet software)"
  },
  {
    "objectID": "2025/slides/session-6.html#questions",
    "href": "2025/slides/session-6.html#questions",
    "title": "Session 6: Hands-on activity #3",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "2025/slides/session-6.html#lemmatize",
    "href": "2025/slides/session-6.html#lemmatize",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lemmatize?",
    "text": "Lemmatize?\n\nWhat do you think is the effect of lemmatization on the lexical diversity measures?\nShould we lemmatize? Why or why not?"
  },
  {
    "objectID": "2025/slides/session-6.html#computing-simple-lexical-sophisitcation-measures",
    "href": "2025/slides/session-6.html#computing-simple-lexical-sophisitcation-measures",
    "title": "Session 6: Hands-on activity #3",
    "section": "Computing simple Lexical Sophisitcation measures",
    "text": "Computing simple Lexical Sophisitcation measures"
  },
  {
    "objectID": "2025/slides/session-6.html#lexical-sophistication-1",
    "href": "2025/slides/session-6.html#lexical-sophistication-1",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lexical sophistication",
    "text": "Lexical sophistication\nThere are a number of lexical sophistication measures for English (+ 300).\n\n12 categories of measures (Eguchi & Kyle, 2020)\n\nFrequency\nRange\nPsycholinguistic Norm\nHypernymy\nN-gram Frequency/Range/SOA\netc."
  },
  {
    "objectID": "2025/slides/session-6.html#typical-operationalization",
    "href": "2025/slides/session-6.html#typical-operationalization",
    "title": "Session 6: Hands-on activity #3",
    "section": "Typical operationalization",
    "text": "Typical operationalization\n\nTypically, lexical sophistication (LS) is calculated as an average:\nTypical LS score = \\[Total \\; LS \\; score \\over nToken \\; with \\; LS \\; score\\]\nAverage is just a convenient choice."
  },
  {
    "objectID": "2025/slides/session-6.html#using-an-emulation-of-taales",
    "href": "2025/slides/session-6.html#using-an-emulation-of-taales",
    "title": "Session 6: Hands-on activity #3",
    "section": "Using an emulation of TAALES",
    "text": "Using an emulation of TAALES\n\nSince the desktop version of TAALES is unstable these days, we will use a simple web application.\nLetâ€™s use simple text analyzer"
  },
  {
    "objectID": "2025/slides/session-6.html#simple-text-analyzer",
    "href": "2025/slides/session-6.html#simple-text-analyzer",
    "title": "Session 6: Hands-on activity #3",
    "section": "Simple Text Analyzer",
    "text": "Simple Text Analyzer\n!"
  },
  {
    "objectID": "2025/slides/session-6.html#analyzing-single-text-in-simple-text-analzer",
    "href": "2025/slides/session-6.html#analyzing-single-text-in-simple-text-analzer",
    "title": "Session 6: Hands-on activity #3",
    "section": "Analyzing single-text in simple text analzer",
    "text": "Analyzing single-text in simple text analzer\n!"
  },
  {
    "objectID": "2025/slides/session-6.html#analyzing-single-text-in-simple-text-analzer-1",
    "href": "2025/slides/session-6.html#analyzing-single-text-in-simple-text-analzer-1",
    "title": "Session 6: Hands-on activity #3",
    "section": "Analyzing single-text in simple text analzer",
    "text": "Analyzing single-text in simple text analzer\n\nChoose single text mode\nChoose Paste text\nChoose Reference list (e.g., COCA Spoken Frequency)\nChoose if you apply log transformation to frequency values\nHit Analyze Text"
  },
  {
    "objectID": "2025/slides/session-6.html#check-impact-of-log-transformation",
    "href": "2025/slides/session-6.html#check-impact-of-log-transformation",
    "title": "Session 6: Hands-on activity #3",
    "section": "Check impact of log transformation",
    "text": "Check impact of log transformation\n\nHow would you explain the results of the log transformation?\nWhich one do you recommend?\n\n\n\n\n\n\nNot log Transformed\n\n\n\n\n\n\nLog Transformed"
  },
  {
    "objectID": "2025/slides/session-6.html#on-log-transformation",
    "href": "2025/slides/session-6.html#on-log-transformation",
    "title": "Session 6: Hands-on activity #3",
    "section": "On log transformation",
    "text": "On log transformation\n\nLog transformation allows the text-internal frequency distribution to approach normal distribution.\nThis is generally recommended because the â€œmeanâ€ score behaves well on normally distributed data."
  },
  {
    "objectID": "2025/slides/session-6.html#comparing-two-texts",
    "href": "2025/slides/session-6.html#comparing-two-texts",
    "title": "Session 6: Hands-on activity #3",
    "section": "Comparing two texts",
    "text": "Comparing two texts\nWe can also compare two texts in simple text analyzer.\n!"
  },
  {
    "objectID": "2025/slides/session-6.html#plots-that-compares-two-lists",
    "href": "2025/slides/session-6.html#plots-that-compares-two-lists",
    "title": "Session 6: Hands-on activity #3",
    "section": "Plots that compares two lists",
    "text": "Plots that compares two lists\n!"
  },
  {
    "objectID": "2025/slides/session-6.html#uploading-your-frequency-list",
    "href": "2025/slides/session-6.html#uploading-your-frequency-list",
    "title": "Session 6: Hands-on activity #3",
    "section": "Uploading your frequency list",
    "text": "Uploading your frequency list\nThe simple text analyzer can accept a word frequency list.\n!TODO\n[] fix the application then think about"
  },
  {
    "objectID": "2025/slides/session-6.html#batch-analysis",
    "href": "2025/slides/session-6.html#batch-analysis",
    "title": "Session 6: Hands-on activity #3",
    "section": "Batch analysis",
    "text": "Batch analysis\n[] fix the application"
  },
  {
    "objectID": "2025/syllabus/index.html",
    "href": "2025/syllabus/index.html",
    "title": "Course Syllabus",
    "section": "",
    "text": "Course Title: Linguistic Data Analysis I\nCredits: 2\nFormat: Intensive 5-day course (15 sessions)\nLanguage: English\n Classroom: 113 Lecture room â‘£",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#course-information",
    "href": "2025/syllabus/index.html#course-information",
    "title": "Course Syllabus",
    "section": "",
    "text": "Course Title: Linguistic Data Analysis I\nCredits: 2\nFormat: Intensive 5-day course (15 sessions)\nLanguage: English\n Classroom: 113 Lecture room â‘£",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#instructor-information",
    "href": "2025/syllabus/index.html#instructor-information",
    "title": "Course Syllabus",
    "section": "Instructor Information",
    "text": "Instructor Information\nInstructor: Masaki Eguchi, Ph.D.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#course-description",
    "href": "2025/syllabus/index.html#course-description",
    "title": "Course Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces the foundations of corpus linguistics and the analysis of learner language through corpus linguistic approaches. It covers key concepts in corpus linguistics, including what corpora are, how they are used to answer (applied) linguistic research questions, and how to design corpus-based analyses to address substantive research questions in second language research. The primary language of analysis in this course is English, but students are encouraged to apply the concepts introduced to the languages they work with in their own research.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#learning-objectives",
    "href": "2025/syllabus/index.html#learning-objectives",
    "title": "Course Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this course, students will be able to:\n\nExplain what corpus linguistics is and how corpus linguistics can help learn linguistic phenomena\nSearch for and select available corpora relevant to their own research\nDiscuss design issues related to language corpora for specific research purposes\nApply introductory corpus linguistic analyses (e.g., frequency analysis, concordancing, POS tagging) to preprocessed corpora\nEvaluate the benefits and drawbacks of a corpus linguistic approach to linguistic analysis",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#course-components",
    "href": "2025/syllabus/index.html#course-components",
    "title": "Course Syllabus",
    "section": "Course Components",
    "text": "Course Components\n\nLectures and tutorials\nDaily Hands-on activities\nMini-corpus labs\nFinal project\n\n\n\n\n\n\n\nNavigation\n\n\n\n\nğŸ“… Detailed Schedule",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#required-materials",
    "href": "2025/syllabus/index.html#required-materials",
    "title": "Course Syllabus",
    "section": "Required Materials",
    "text": "Required Materials\n\nTextbook\n\nDurrant, P. (2023). Corpus linguistics for writing development: A guide for research. Routledge. https://doi.org/10.4324/9781003152682\nStefanowitsch, A. (2020). Corpus linguistics: A guide to the methodology. Zenodo. https://doi.org/10.5281/ZENODO.3735822 (This is an open source textbook, so itâ€™s freely available online)\n\nOther required/Optional readings are provided through Google Classroom.\n\n\nSoftwares (Free)\n\nWeb application for simple text analyses\n\nSimple Text Analyzer: A web app created for you.\n\n\n\nConcordancing Software\n\nAntConc: Corpus analysis toolkit for Concordancing\n\n\n\nLexical Profiling Software\n\nAntWordProfiler: Corpus analysis toolkit for Lexical Profiling\nLexTutor: Corpus analysis toolkit online\n\n\n\nStatistics\n\nJASP: Statistical analysis software\n\n\n\nOthers\n\nGoogle Colaboratory: Follow the instruction here to enable the tool.\nText Editor: VS Code recommended",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#assignments-and-grading",
    "href": "2025/syllabus/index.html#assignments-and-grading",
    "title": "Course Syllabus",
    "section": "Assignments and Grading",
    "text": "Assignments and Grading\nYou can find detailed information about each assignment in assignments page (under construction).\nWe have two possible choices for the grade distribution for this course. I will explain the details of each choice in the first day of the course. The class as a whole decides which one we will incorporate in this course. Once decided, all students will follow the decided plan.\n\nGrade Distribution â€“ Option A\n\nFor option A, pairs of students will develop their own mini-research and present their final project on the final day.\n\n\n\n\nAssignment\n\nPercent\n\n\n\n\nHands-on Assignments\n(4 Ã— 15%)\n60%\n\n\nClass Participation\n\n20%\n\n\nFinal Project\n\n20%\n\n\n\n\n\nGrade Distribution â€“ Option B\n\nFor option B, pairs of students will present on hands-on assignments on the final day.\n\n\n\n\nAssignment\n\nPercent\n\n\n\n\nHands-on Assignments\n(4 Ã— 15%)\n60%\n\n\nClass Participation\n\n20%\n\n\nFinal Presentation on Selected Hands-on Assignment\n\n20%\n\n\n\n\n\nGrading Scale\nWe follow the grading system at Tohoku University.\n\n\n\nGrade\nRange\nGrade Point\n\n\n\n\nAA\n100-90%\n4.0\n\n\nA\n89-80%\n3.0\n\n\nB\n79-70%\n2.0\n\n\nC\n69-60%\n1.0\n\n\nD\n59-0%\n0.0",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#daily-structure",
    "href": "2025/syllabus/index.html#daily-structure",
    "title": "Course Syllabus",
    "section": "Daily Structure",
    "text": "Daily Structure\nEach day follows this general pattern:\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 1\n\n\n12:00-13:00\nLunch break\n\n\n13:00-14:30\nSession 2\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 3\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#attendance-policy",
    "href": "2025/syllabus/index.html#attendance-policy",
    "title": "Course Syllabus",
    "section": "Attendance Policy",
    "text": "Attendance Policy\n\nDue to the intensive nature of the course, attendance and participation are crucial to your success in this course.\nHowever, in case of emergency, do not hesitate to reach out to the instructor for possible accomodation. I may be able to accommodate depending on the situation.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#assignment-submission",
    "href": "2025/syllabus/index.html#assignment-submission",
    "title": "Course Syllabus",
    "section": "Assignment Submission",
    "text": "Assignment Submission\n\nDeadlines\n\nAll assignments are due at 10:30 AM on the specified day\nLate submissions will receive a 10% penalty per day\nExtensions may be granted for documented emergencies.\n\n\n\nSubmission Format\n\nSubmit all assignments via the course management system (Google Classroom)\nUse the provided templates when available\nFile naming convention: LastName_Assignment#.ext\nAcceptable formats: .docx, .pdf, .ipynb (for Python notebooks)\n\n\n\nPlagiarism\n\n\nCollaboration",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#technology-policy",
    "href": "2025/syllabus/index.html#technology-policy",
    "title": "Course Syllabus",
    "section": "Technology Policy",
    "text": "Technology Policy\n\nRequired Technology\n\nBring a laptop to every session.\nEnsure all required software is installed.\n\n\n\nClassroom Etiquette\n\nLaptops should be used for course activities only",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#communication",
    "href": "2025/syllabus/index.html#communication",
    "title": "Course Syllabus",
    "section": "Communication",
    "text": "Communication\n\nCourse related communications will happen via Google Classroom.\n\n\nCourse Announcements\n\nCourse announcements are made through Google Classroom.\n\n\n\nMaterials Sharing\n\nMaterials (e.g., slides) are shared through this website.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#accommodations",
    "href": "2025/syllabus/index.html#accommodations",
    "title": "Course Syllabus",
    "section": "Accommodations",
    "text": "Accommodations",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#learning-objectives",
    "href": "2025/sessions/day4/session11.html#learning-objectives",
    "title": "Session 11",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nUnderstand NLP tasks such as POS tagging and dependency parsing\nUnderstand how automated parsing works\nConduct POS tagging using spaCy library in Python (through Google Colab)\nConduct Dependency parsing using spaCy library in Python (through Google Colab)\nConduct multi-lingual Part-Of-Speech (POS) tagging using TagAnt",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#key-concepts",
    "href": "2025/sessions/day4/session11.html#key-concepts",
    "title": "Session 11",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nPOS tagging\nDependency parsing\nPrecision, Recall, and F1 score",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#required-readings",
    "href": "2025/sessions/day4/session11.html#required-readings",
    "title": "Session 11",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nSkim Durrant Ch 6 (Ignore R codes if you are not familiar)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#recommended-readings",
    "href": "2025/sessions/day4/session11.html#recommended-readings",
    "title": "Session 11",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nKyle, K., & Eguchi, M. (2024). Evaluating NLP models with written and spoken L2 samples. Research Methods in Applied Linguistics, 3(2), 100120. https://doi.org/10.1016/j.rmal.2024.100120",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#notes",
    "href": "2025/sessions/day4/session11.html#notes",
    "title": "Session 11",
    "section": "ğŸ“ Notes",
    "text": "ğŸ“ Notes\n\nAll the python codes are prepared by the instructor and shared with the students. This session does not require ability to code.\nThe decision to use Python programming language rather than already available software is based on consideration that there is very little tools which provide stable access to the language analysis described here.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#tools-used",
    "href": "2025/sessions/day4/session11.html#tools-used",
    "title": "Session 11",
    "section": "ğŸ› ï¸ Tools Used",
    "text": "ğŸ› ï¸ Tools Used\n\nTagAnt\nSimple Text Analyzer: A web app created for you.\nGoogle Colaboratory",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day4/session11.html#dive-deeper---recommended-readings",
    "title": "Session 11",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#materials",
    "href": "2025/sessions/day4/session11.html#materials",
    "title": "Session 11",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#reflection",
    "href": "2025/sessions/day4/session11.html#reflection",
    "title": "Session 11",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html",
    "href": "2025/sessions/day4/index.html",
    "title": "Day 4: Analyzing Grammar",
    "section": "",
    "text": "Day 4 introduces grammatical analysis in corpus linguistics, exploring complexity measures and computational tools for parsing and analysis.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#overview",
    "href": "2025/sessions/day4/index.html#overview",
    "title": "Day 4: Analyzing Grammar",
    "section": "",
    "text": "Day 4 introduces grammatical analysis in corpus linguistics, exploring complexity measures and computational tools for parsing and analysis.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#key-concepts",
    "href": "2025/sessions/day4/index.html#key-concepts",
    "title": "Day 4: Analyzing Grammar",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nGrammatical complexity\nPredictive measures versus Descriptive measures\nPOS tagging\nDependency parsing\nPrecision, Recall, and F1 score\nSyntactic sophistication and fine-grained measures",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#preparation",
    "href": "2025/sessions/day4/index.html#preparation",
    "title": "Day 4: Analyzing Grammar",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 4:\n\nRead:\n\nDurrant Ch. 5\nKyle, K., & Crossley, S. A. (2018). Measuring Syntactic Complexity in L2 Writing Using Fineâ€Grained Clausal and Phrasal Indices. The Modern Language Journal, 102(2), 333â€“349.\n\nSkim:\n\nDurrant Ch. 6 (Ignore R codes if you are not familiar)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#schedule",
    "href": "2025/sessions/day4/index.html#schedule",
    "title": "Day 4: Analyzing Grammar",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 10: Grammar â€” Overview\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 11: POS Tagging and Parsing\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 12: Linguistic Complexity Analysis\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#assignments",
    "href": "2025/sessions/day4/index.html#assignments",
    "title": "Day 4: Analyzing Grammar",
    "section": "Assignments",
    "text": "Assignments\n\nDue Tomorrow: Hands-on Assignment 4\nComplete grammatical analysis exercises using Python notebooks and TagAnt",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#reflection",
    "href": "2025/sessions/day4/index.html#reflection",
    "title": "Day 4: Analyzing Grammar",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#learning-objectives",
    "href": "2025/sessions/day3/session9.html#learning-objectives",
    "title": "Session 9",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\nJustify choices of lexical richness measures to investigate a research questions\nConduct a simple statistical analysis of selected corpus on small sets of lexical measures using JASP software",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#key-concepts",
    "href": "2025/sessions/day3/session9.html#key-concepts",
    "title": "Session 9",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nLinear regression analysis (group comparison or prediction)\nIntroduction to in-class mini-project",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#required-readings",
    "href": "2025/sessions/day3/session9.html#required-readings",
    "title": "Session 9",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nNo new reading! (please re-read Eguchi & Kyle, 2020)",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day3/session9.html#dive-deeper---recommended-readings",
    "title": "Session 9",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings\n\nPaquot, M. (2019). The phraseological dimension in interlanguage complexity research. Second Language Research, 35(1), 121â€“145. https://doi.org/10.1177/0267658317694221\nEguchi, M., & Kyle, K. (2023). L2 collocation profiles and their relationship with vocabulary proficiency: A learner corpus approach. Journal of Second Language Writing, 60, 100975. https://doi.org/10.1016/j.jslw.2023.100975",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#materials",
    "href": "2025/sessions/day3/session9.html#materials",
    "title": "Session 9",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#reflection",
    "href": "2025/sessions/day3/session9.html#reflection",
    "title": "Session 9",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now:\n\nProvide reasons for your choice of lexical richness measures.\nConduct preliminary analysis to understand how text differ from one aother in relation to learnerâ€™s proficiency, learner groups, etc.\nBrainstorm your ideas for your final project.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#learning-objectives",
    "href": "2025/sessions/day3/session7.html#learning-objectives",
    "title": "Session 7",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\nExplain different types of multiword units: collocation, n-grams, lexical bundles\nDemonstrate how major association strengths measures (t-score, Mutual Information, and LogDice) are calculated using examples",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#key-concepts",
    "href": "2025/sessions/day3/session7.html#key-concepts",
    "title": "Session 7",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nTypes of multiword units\nAssociation strengths\nThree approaches:\n\nContext window\nDependency bigram",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#required-readings",
    "href": "2025/sessions/day3/session7.html#required-readings",
    "title": "Session 7",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nDurrant (2023) Ch. 7\nGablasova, D., Brezina, V., & McEnery, T. (2017). Collocations in Corpusâ€Based Language Learning Research: Identifying, Comparing, and Interpreting the Evidence. Language Learning, 67(S1), 155â€“179. https://doi.org/10.1111/lang.12225",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day3/session7.html#dive-deeper---recommended-readings",
    "title": "Session 7",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings\n\nPaquot, M. (2019). The phraseological dimension in interlanguage complexity research. Second Language Research, 35(1), 121â€“145. https://doi.org/10.1177/0267658317694221\nStephanie Evertâ€™s website on collocation measures\n\nThis webpage provides formulas to calculate various Strengths Of Association measures.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#materials",
    "href": "2025/sessions/day3/session7.html#materials",
    "title": "Session 7",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#reflection",
    "href": "2025/sessions/day3/session7.html#reflection",
    "title": "Session 7",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now:\n\nDescribe major types of multiword units and how they differ from each other\n\nN-gram, Lexical Collocations, Colligations, lexical bundle\n\nDescribe benefits and drawbacks of major Strengths Of Association (SOA) measures\nDiscuss two approaches to identify collocation from the text: window-based approach and dependency-based approach.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#learning-objectives",
    "href": "2025/sessions/day2/session4.html#learning-objectives",
    "title": "Session 4",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nExplain the purposes of linguistic measures\nList commonly used lexical measures in second language acquisition research\nExplain sub-constructs of lexical richness measures\n\nLexical Diversity\nLexical Sophistication",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#key-concepts",
    "href": "2025/sessions/day2/session4.html#key-concepts",
    "title": "Session 4",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nLexical Richness\n\nDistinction between text internal vs external measures\n\nLexical Diversity\n\nType-Token Ratio\nMeasure of Textual Lexical Diversity (MTLD)\n\nLexical Sophistication\n\nFrequency\nConcreteness\nPhonological neighbors",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#required-readings",
    "href": "2025/sessions/day2/session4.html#required-readings",
    "title": "Session 4",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nDurrant Ch. 3\n(Skim) Eguchi, M., & Kyle, K. (2020). Continuing to Explore the Multidimensional Nature of Lexical Sophistication: The Case of Oral Proficiency Interviews. The Modern Language Journal, 104(2), 381â€“400. https://doi.org/10.1111/modl.12637",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day2/session4.html#dive-deeper---recommended-readings",
    "title": "Session 4",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#materials",
    "href": "2025/sessions/day2/session4.html#materials",
    "title": "Session 4",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#reflection",
    "href": "2025/sessions/day2/session4.html#reflection",
    "title": "Session 4",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#learning-objectives",
    "href": "2025/sessions/day2/session6.html#learning-objectives",
    "title": "Session 6",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, you will be able to:\n\n\nCompute simple lexical diversity measures using spreadsheet software\nCompute advanced lexical diversity measures using TAALED\nExplain how modern lexical diversity measures are calculated\nCalculate simple lexical sophistication measures using dedicated web application\nDescribe how lexical sophistication measures behave on a single input text.\nDiscuss benefits and drawbacks of lexical richness measures.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#key-concepts",
    "href": "2025/sessions/day2/session6.html#key-concepts",
    "title": "Session 6",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nlexical diversity\nlexical sophistication\nLearner corpus research",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#tools-used",
    "href": "2025/sessions/day2/session6.html#tools-used",
    "title": "Session 6",
    "section": "ğŸ› ï¸ Tools Used",
    "text": "ğŸ› ï¸ Tools Used\n\nSimple Text Analyzer: A web app created for you.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#required-readings",
    "href": "2025/sessions/day2/session6.html#required-readings",
    "title": "Session 6",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\n(Skim) Durrant Ch. 4 (Ignore R codes if you are not familiar)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day2/session6.html#dive-deeper---recommended-readings",
    "title": "Session 6",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings\n\nBestgen, Y. (2025). Estimating lexical diversity using the moving average type-token ratio (MATTR): Pros and cons. Research Methods in Applied Linguistics, 4(1), 100168. https://doi.org/10.1016/j.rmal.2024.100168\nKyle, K., Crossley, S., & Berger, C. (2018). The tool for the automatic analysis of lexical sophistication (TAALES): Version 2.0. Behavior Research Methods, 50(3), 1030â€“1046. https://doi.org/10.3758/s13428-017-0924-4\nKyle, K., Crossley, S. A., & Jarvis, S. (2021). Assessing the Validity of Lexical Diversity Indices Using Direct Judgements. Language Assessment Quarterly, 18(2), 154â€“170. https://doi.org/10.1080/15434303.2020.1844205\nKyle, K., Sung, H., Eguchi, M., & Zenker, F. (2024). Evaluating evidence for the reliability and validity of lexical diversity indices in L2 oral task responses. Studies in Second Language Acquisition, 46(1), 278â€“299. https://doi.org/10.1017/S0272263123000402",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#materials",
    "href": "2025/sessions/day2/session6.html#materials",
    "title": "Session 6",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#reflection",
    "href": "2025/sessions/day2/session6.html#reflection",
    "title": "Session 6",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session15.html",
    "href": "2025/sessions/day5/session15.html",
    "title": "Session 15",
    "section": "",
    "text": "Final project time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 15"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session15.html#one-liner",
    "href": "2025/sessions/day5/session15.html#one-liner",
    "title": "Session 15",
    "section": "",
    "text": "Final project time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 15"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html",
    "href": "2025/sessions/day5/session13.html",
    "title": "Session 13",
    "section": "",
    "text": "We will explore how powerful and limited Large Language Models (LLMs) can be at the same time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#one-liner",
    "href": "2025/sessions/day5/session13.html#one-liner",
    "title": "Session 13",
    "section": "",
    "text": "We will explore how powerful and limited Large Language Models (LLMs) can be at the same time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#learning-objectives",
    "href": "2025/sessions/day5/session13.html#learning-objectives",
    "title": "Session 13",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nDescribe how LLMs are trained generally and what LLMs do to produce language.\nExplain the benefits and drawbacks of using LLMs for linguistic annotation.\nDemonstrate/discuss potential impacts of prompts on the LLMs performance on linguistic annotation.\nDesign an experiment to investigate LLMs output accuracy on a given annotation task.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#key-concepts",
    "href": "2025/sessions/day5/session13.html#key-concepts",
    "title": "Session 13",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nLarge Language Models (LLMs) and Language Generation\nPrompt engineering\nFine-tuning",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#required-readings",
    "href": "2025/sessions/day5/session13.html#required-readings",
    "title": "Session 13",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nMizumoto, A., Shintani, N., Sasaki, M., & Teng, M. F. (2024). Testing the viability of ChatGPT as a companion in L2 writing accuracy assessment. Research Methods in Applied Linguistics, 3(2), 100116. https://doi.org/10.1016/j.rmal.2024.100116\n(Skim) Kim, M., & Lu, X. (2024). Exploring the potential of using ChatGPT for rhetorical move-step analysis: The impact of prompt refinement, few-shot learning, and fine-tuning. Journal of English for Academic Purposes, 71, 101422. https://doi.org/10.1016/j.jeap.2024.101422",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day5/session13.html#dive-deeper---recommended-readings",
    "title": "Session 13",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings\n\nMizumoto, A. (2025). Automated analysis of common errors in L2 learner production: Prototype web application development. Studies in Second Language Acquisition, 1â€“18. https://doi.org/10.1017/S0272263125100934\nYu, D., Li, L., Su, H., & Fuoli, M. (2024). Assessing the potential of LLM-assisted annotation for corpus-based pragmatics and discourse analysis: The case of apology. International Journal of Corpus Linguistics, 29(4), 534â€“561. https://doi.org/10.1075/ijcl.23087.yu",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#materials",
    "href": "2025/sessions/day5/session13.html#materials",
    "title": "Session 13",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\n\n\nOther resources\n\nPrompting engineering guide\nPost by Mark Davies on how similar LLMâ€™s â€œintrospectionsâ€ are to corpus data",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#reflection",
    "href": "2025/sessions/day5/session13.html#reflection",
    "title": "Session 13",
    "section": "Reflection",
    "text": "Reflection\nYou can now:",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html",
    "href": "2025/sessions/day1/index.html",
    "title": "Day 1: Introduction and Foundations",
    "section": "",
    "text": "Day 1 introduces basic concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#overview",
    "href": "2025/sessions/day1/index.html#overview",
    "title": "Day 1: Introduction and Foundations",
    "section": "",
    "text": "Day 1 introduces basic concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#key-concepts",
    "href": "2025/sessions/day1/index.html#key-concepts",
    "title": "Day 1: Introduction and Foundations",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nlinguistic intuition\nscientific hypothesis and data\nKWIC\nConcordancing",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#preparation",
    "href": "2025/sessions/day1/index.html#preparation",
    "title": "Day 1: Introduction and Foundations",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 1:\n\nRead:\n\nStefanowitsch (2020) Ch. 1. Freely available online\nStefanowitsch (2020) Ch. 2. Freely available online\n\nSkim:\n\nDurrant (2023) Ch. 1.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#schedule",
    "href": "2025/sessions/day1/index.html#schedule",
    "title": "Day 1: Introduction and Foundations",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 1: Introduction\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 2: Corpus foundation\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 3: First Hands-on Activity\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#assignments",
    "href": "2025/sessions/day1/index.html#assignments",
    "title": "Day 1: Introduction and Foundations",
    "section": "Assignments",
    "text": "Assignments\n\nDue Tomorrow: Hands-on Assignment 1\nComplete basic corpus search exercise",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#reflection",
    "href": "2025/sessions/day1/index.html#reflection",
    "title": "Day 1: Introduction and Foundations",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html",
    "href": "2025/sessions/day1/session2.html",
    "title": "Session 2",
    "section": "",
    "text": "Session 2 covers foundational concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#one-liner",
    "href": "2025/sessions/day1/session2.html#one-liner",
    "title": "Session 2",
    "section": "",
    "text": "Session 2 covers foundational concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#learning-objectives",
    "href": "2025/sessions/day1/session2.html#learning-objectives",
    "title": "Session 2",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nDefine corpus linguistics as an empirical methodology\nExplain key limitations of introspection in linguistic research\nDescribe the role of frequency data and patterns in corpus analysis\nIdentify and explain the basic steps in corpus-based research\nReflect on their own stance toward data, intuition, and linguistic evidence",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#key-concepts",
    "href": "2025/sessions/day1/session2.html#key-concepts",
    "title": "Session 2",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nCorpus linguistics\nBalanced Corpus\nReference Corpus\nLearner Corpus\nCorpus representativeness\nLinguistic intuition vs data",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#required-readings",
    "href": "2025/sessions/day1/session2.html#required-readings",
    "title": "Session 2",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nStefanowitsch (2020) Ch. 1 Freely available online\nStefanowitsch (2020) Ch. 2 Freely available online\n(Skim) Durrant (2023) Ch. 1",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day1/session2.html#dive-deeper---recommended-readings",
    "title": "Session 2",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#materials",
    "href": "2025/sessions/day1/session2.html#materials",
    "title": "Session 2",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#reflection",
    "href": "2025/sessions/day1/session2.html#reflection",
    "title": "Session 2",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/notebooks/session-6.html",
    "href": "2025/notebooks/session-6.html",
    "title": "Session 6 â€” Computing simple lexical diversity and sophistication index",
    "section": "",
    "text": "Show code\nlow_diversity = \"The dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.\"\nShow code\n\nhigh_diversity = \"A curious fox trotted briskly through the meadow, leaping over mossy logs, sniffing wildflowers, and vanishing into golden twilight.\"\nShow code\ndef count_token_type(text: str):\n    # delete punctuation\n    text = text.replace(\".\", \"\")\n    text = text.replace(\",\", \"\")\n    text = text.replace(\"?\", \"\")\n\n    \n    token_list = text.strip()\n    token_list = text.split(\" \")\n\n    token = len(token_list)\n    type = len(set(token_list))\n    return (token, type)\nShow code\ncount_token_type(low_diversity)\n\n\n(19, 8)\nShow code\ncount_token_type(high_diversity)\n\n\n(19, 19)\nShow code\nlow_diversity2 = \"The dog ran. The dog jumped. The dog barked. The dog played. The dog ran quickly. The dog jumped so high. The dog barked very loudly. The dog played, sat, and rolled. The dog sneezed. The dog ate the food.\"\nShow code\ncount_token_type(low_diversity2)\n\n\n(40, 18)\nShow code\nlow_diversity3 = \"The parrot squawked loudly. The parrot chirped again. A toucan perched nearby. The parrot fluttered. Wings flapped softly. The parrot chirped again. Feathers shimmered under sunlight. The crow cawed. The parrot glided low. The air shimmered. The owl blinked slowly. The parrot perched again. The owl blinked slowly. The parrot shrieked. The parrot chirped nearby again. The parrot squawked again.\"\nShow code\ncount_token_type(low_diversity3)\n\n\n(60, 27)"
  },
  {
    "objectID": "2025/notebooks/session-6.html#using-lexical-diversity-package",
    "href": "2025/notebooks/session-6.html#using-lexical-diversity-package",
    "title": "Session 6 â€” Computing simple lexical diversity and sophistication index",
    "section": "Using Lexical Diversity package",
    "text": "Using Lexical Diversity package\nThere is a package called TAALED maintained by Dr.Â Kris Kyle.\n\n\nShow code\nfrom taaled import ld\nfrom pylats import lats"
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html",
    "href": "2025/notebooks/japanese-nlp-test.html",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "",
    "text": "This notebook implements and compares two approaches for Japanese morphological analysis with BCCWJ frequency matching:\nEach approach is designed for reproducible setup, implementation, validation, and operational use."
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html#environment-setup-verification",
    "href": "2025/notebooks/japanese-nlp-test.html#environment-setup-verification",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "1. Environment Setup & Verification",
    "text": "1. Environment Setup & Verification\nFirst, letâ€™s verify and set up our environment with all required packages.\n\n\nShow code\n# Environment verification and setup\nimport sys\nimport subprocess\nfrom pathlib import Path\n\nprint(f\"Python version: {sys.version}\")\nprint(f\"Working directory: {Path.cwd()}\")\n\n# Required packages\nrequired_packages = [\n    'fugashi', 'unidic', 'unidic-lite', 'spacy', 'ginza', \n    'ja-ginza', 'sudachipy', 'pandas', 'numpy', 'matplotlib', 'collections'\n]\n\nprint(\"\\nChecking package availability:\")\nfor package in required_packages:\n    try:\n        if package == 'collections':\n            import collections\n            print(f\"âœ“ {package} (built-in)\")\n        else:\n            __import__(package)\n            print(f\"âœ“ {package}\")\n    except ImportError:\n        print(f\"âœ— {package} - NOT FOUND\")\n\n\nPython version: 3.12.2 (main, Feb 25 2024, 03:55:42) [Clang 17.0.6 ]\nWorking directory: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/2025/notebooks\n\nChecking package availability:\nâœ“ fugashi\nâœ“ unidic\nâœ— unidic-lite - NOT FOUND\nâœ“ spacy\nâœ“ ginza\nâœ— ja-ginza - NOT FOUND\nâœ“ sudachipy\nâœ“ pandas\nâœ“ numpy\nâœ“ matplotlib\nâœ“ collections (built-in)\n\n\n\n\nShow code\n# Import all necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter, defaultdict\nimport time\nimport warnings\nfrom typing import List, Tuple, Dict, Optional\n\n# Japanese NLP libraries\nimport fugashi\nimport unidic\nimport spacy\nfrom spacy.tokens import Token, Doc\n\n# Statistical analysis\ntry:\n    from scipy.stats import spearmanr\n    scipy_available = True\nexcept ImportError:\n    print(\"scipy not available - will use numpy for correlation\")\n    scipy_available = False\n\nprint(\"All imports successful!\")\nwarnings.filterwarnings('ignore')\n\n\nscipy not available - will use numpy for correlation\nAll imports successful!\n\n\n\n\nShow code\n# Check UniDic installation and download if needed\ntry:\n    print(f\"UniDic directory: {unidic.DICDIR}\")\n    print(\"UniDic is properly installed\")\nexcept Exception as e:\n    print(f\"UniDic issue: {e}\")\n    print(\"You may need to run: python -m unidic download\")\n\n# Test basic fugashi functionality\ntry:\n    tagger = fugashi.Tagger(f'-d \"{unidic.DICDIR}\"')\n    test_result = list(tagger(\"ãƒ†ã‚¹ãƒˆ\"))\n    print(f\"Fugashi + UniDic test successful: {test_result[0].surface}\")\nexcept Exception as e:\n    print(f\"Fugashi test failed: {e}\")\n\n\nUniDic directory: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/unidic/dicdir\nUniDic is properly installed\nFugashi + UniDic test successful: ãƒ†ã‚¹ãƒˆ"
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html#sample-data-preparation",
    "href": "2025/notebooks/japanese-nlp-test.html#sample-data-preparation",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "2. Sample Data Preparation",
    "text": "2. Sample Data Preparation\nLetâ€™s create realistic Japanese text samples for testing our pipelines.\n\n\nShow code\n# Sample Japanese texts for testing\nsample_texts = [\n    \"å½¼ã¯æ—¥ã”ã‚ã‹ã‚‰æœ¬ã‚’èª­ã‚€ã®ãŒå¥½ãã§ã™ã€‚\",\n    \"ã²ã”ã‚ã®å‹‰å¼·ãŒå¤§åˆ‡ã ã¨æ€ã„ã¾ã™ã€‚\",\n    \"æ—¥é ƒã®åŠªåŠ›ãŒå®Ÿã‚’çµã¶ã§ã—ã‚‡ã†ã€‚\",\n    \"å½¼å¥³ã¯æ›¸ãã‚ã‚‰ã‚ã™ã“ã¨ãŒå¾—æ„ã§ã™ã€‚\",\n    \"ãã®å•é¡Œã‚’æ›¸ãè¡¨ã™ã®ã¯é›£ã—ã„ã€‚\",\n    \"ä»Šæ—¥ã¯æ±äº¬ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦è©±ã—ã¾ã—ã‚‡ã†ã€‚\",\n    \"ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’é£²ã‚“ã§ã€å‘‘ã¿è¾¼ã‚“ã§ã€ã¾ãŸé£²ã‚“ã§ã—ã¾ã£ãŸã€‚\",\n    \"å›½éš›çš„ãªå”åŠ›ãŒå¿…è¦ä¸å¯æ¬ ã§ã™ã€‚\",\n    \"æ©Ÿæ¢°å­¦ç¿’ã®æŠ€è¡“ãŒé€²æ­©ã—ã¦ã„ã‚‹ã€‚\",\n    \"è‡ªç„¶è¨€èªå‡¦ç†ã¯èˆˆå‘³æ·±ã„åˆ†é‡ã ã€‚\"\n]\n\nprint(\"Sample texts prepared:\")\nfor i, text in enumerate(sample_texts, 1):\n    print(f\"{i:2d}. {text}\")\n\n# Create a larger corpus by repeating and slightly modifying texts\nextended_corpus = sample_texts * 3  # Simulate frequency variations\nprint(f\"\\nExtended corpus: {len(extended_corpus)} texts\")\n\n\nSample texts prepared:\n 1. å½¼ã¯æ—¥ã”ã‚ã‹ã‚‰æœ¬ã‚’èª­ã‚€ã®ãŒå¥½ãã§ã™ã€‚\n 2. ã²ã”ã‚ã®å‹‰å¼·ãŒå¤§åˆ‡ã ã¨æ€ã„ã¾ã™ã€‚\n 3. æ—¥é ƒã®åŠªåŠ›ãŒå®Ÿã‚’çµã¶ã§ã—ã‚‡ã†ã€‚\n 4. å½¼å¥³ã¯æ›¸ãã‚ã‚‰ã‚ã™ã“ã¨ãŒå¾—æ„ã§ã™ã€‚\n 5. ãã®å•é¡Œã‚’æ›¸ãè¡¨ã™ã®ã¯é›£ã—ã„ã€‚\n 6. ä»Šæ—¥ã¯æ±äº¬ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦è©±ã—ã¾ã—ã‚‡ã†ã€‚\n 7. ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’é£²ã‚“ã§ã€å‘‘ã¿è¾¼ã‚“ã§ã€ã¾ãŸé£²ã‚“ã§ã—ã¾ã£ãŸã€‚\n 8. å›½éš›çš„ãªå”åŠ›ãŒå¿…è¦ä¸å¯æ¬ ã§ã™ã€‚\n 9. æ©Ÿæ¢°å­¦ç¿’ã®æŠ€è¡“ãŒé€²æ­©ã—ã¦ã„ã‚‹ã€‚\n10. è‡ªç„¶è¨€èªå‡¦ç†ã¯èˆˆå‘³æ·±ã„åˆ†é‡ã ã€‚\n\nExtended corpus: 30 texts\n\n\n\n\nShow code\n# Create mock BCCWJ frequency data for testing\n# In real usage, this would be loaded from an actual BCCWJ frequency file\n\nmock_bccwj_data = [\n    ('æ—¥é ƒ', 'ãƒ’ã‚´ãƒ­', 'åè©', 1250),\n    ('æœ¬', 'ãƒ›ãƒ³', 'åè©', 8500),\n    ('èª­ã‚€', 'ãƒ¨ãƒ ', 'å‹•è©', 3200),\n    ('å¥½ã', 'ã‚¹ã‚­', 'å½¢å®¹å‹•è©', 2100),\n    ('å‹‰å¼·', 'ãƒ™ãƒ³ã‚­ãƒ§ã‚¦', 'åè©', 4200),\n    ('å¤§åˆ‡', 'ã‚¿ã‚¤ã‚»ãƒ„', 'å½¢å®¹å‹•è©', 1800),\n    ('æ€ã†', 'ã‚ªãƒ¢ã‚¦', 'å‹•è©', 9500),\n    ('åŠªåŠ›', 'ãƒ‰ãƒªãƒ§ã‚¯', 'åè©', 2200),\n    ('å®Ÿ', 'ãƒŸ', 'åè©', 1100),\n    ('çµã¶', 'ãƒ ã‚¹ãƒ–', 'å‹•è©', 800),\n    ('æ›¸ã', 'ã‚«ã‚¯', 'å‹•è©', 4100),\n    ('è¡¨ã™', 'ã‚¢ãƒ©ãƒ¯ã‚¹', 'å‹•è©', 1500),\n    ('å¾—æ„', 'ãƒˆã‚¯ã‚¤', 'å½¢å®¹å‹•è©', 1300),\n    ('å•é¡Œ', 'ãƒ¢ãƒ³ãƒ€ã‚¤', 'åè©', 6200),\n    ('é›£ã—ã„', 'ãƒ ã‚ºã‚«ã‚·ã‚¤', 'å½¢å®¹è©', 3800),\n    ('ä»Šæ—¥', 'ã‚­ãƒ§ã‚¦', 'åè©', 5500),\n    ('æ±äº¬', 'ãƒˆã‚¦ã‚­ãƒ§ã‚¦', 'åè©', 4800),\n    ('è©±ã™', 'ãƒãƒŠã‚¹', 'å‹•è©', 3600),\n    ('é£²ã‚€', 'ãƒãƒ ', 'å‹•è©', 2400),\n    ('å‘‘ã‚€', 'ãƒãƒ ', 'å‹•è©', 150),\n    ('å›½éš›', 'ã‚³ã‚¯ã‚µã‚¤', 'åè©', 2800),\n    ('å”åŠ›', 'ã‚­ãƒ§ã‚¦ãƒªãƒ§ã‚¯', 'åè©', 1900),\n    ('å¿…è¦', 'ãƒ’ãƒ„ãƒ¨ã‚¦', 'å½¢å®¹å‹•è©', 4500),\n    ('æŠ€è¡“', 'ã‚®ã‚¸ãƒ¥ãƒ„', 'åè©', 3900),\n    ('é€²æ­©', 'ã‚·ãƒ³ãƒ', 'åè©', 1100)\n]\n\n# Create DataFrame\ndf_bccwj = pd.DataFrame(mock_bccwj_data, columns=['lemma', 'reading', 'pos', 'freq_bccwj'])\ndf_bccwj['key'] = list(zip(df_bccwj.lemma, df_bccwj.reading, df_bccwj.pos))\n\nprint(\"Mock BCCWJ frequency data:\")\nprint(df_bccwj.head(10))\nprint(f\"\\nTotal entries: {len(df_bccwj)}\")\n\n\nMock BCCWJ frequency data:\n  lemma reading   pos  freq_bccwj               key\n0    æ—¥é ƒ     ãƒ’ã‚´ãƒ­    åè©        1250     (æ—¥é ƒ, ãƒ’ã‚´ãƒ­, åè©)\n1     æœ¬      ãƒ›ãƒ³    åè©        8500       (æœ¬, ãƒ›ãƒ³, åè©)\n2    èª­ã‚€      ãƒ¨ãƒ     å‹•è©        3200      (èª­ã‚€, ãƒ¨ãƒ , å‹•è©)\n3    å¥½ã      ã‚¹ã‚­  å½¢å®¹å‹•è©        2100    (å¥½ã, ã‚¹ã‚­, å½¢å®¹å‹•è©)\n4    å‹‰å¼·   ãƒ™ãƒ³ã‚­ãƒ§ã‚¦    åè©        4200   (å‹‰å¼·, ãƒ™ãƒ³ã‚­ãƒ§ã‚¦, åè©)\n5    å¤§åˆ‡    ã‚¿ã‚¤ã‚»ãƒ„  å½¢å®¹å‹•è©        1800  (å¤§åˆ‡, ã‚¿ã‚¤ã‚»ãƒ„, å½¢å®¹å‹•è©)\n6    æ€ã†     ã‚ªãƒ¢ã‚¦    å‹•è©        9500     (æ€ã†, ã‚ªãƒ¢ã‚¦, å‹•è©)\n7    åŠªåŠ›    ãƒ‰ãƒªãƒ§ã‚¯    åè©        2200    (åŠªåŠ›, ãƒ‰ãƒªãƒ§ã‚¯, åè©)\n8     å®Ÿ       ãƒŸ    åè©        1100        (å®Ÿ, ãƒŸ, åè©)\n9    çµã¶     ãƒ ã‚¹ãƒ–    å‹•è©         800     (çµã¶, ãƒ ã‚¹ãƒ–, å‹•è©)\n\nTotal entries: 25"
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html#plan-a-mecab-fugashi-unidic-direct-pipeline",
    "href": "2025/notebooks/japanese-nlp-test.html#plan-a-mecab-fugashi-unidic-direct-pipeline",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "3. Plan A: MeCab (fugashi) + UniDic Direct Pipeline",
    "text": "3. Plan A: MeCab (fugashi) + UniDic Direct Pipeline\n\nA-1 to A-3: Setup and Configuration\nUniDic provides the morphological analysis system used in BCCWJ, making it ideal for frequency matching.\n\n\nShow code\n# A-3: Initialize fugashi with UniDic\nprint(\"Initializing Plan A: fugashi + UniDic pipeline\")\n\n# Initialize tagger with explicit UniDic path\ntagger_a = fugashi.Tagger(f'-d \"{unidic.DICDIR}\"')\nprint(f\"Tagger initialized with UniDic dictionary: {unidic.DICDIR}\")\n\n# Test the tagger\ntest_text = \"æ—¥ã”ã‚ã‹ã‚‰å‹‰å¼·ã—ã¦ã„ã‚‹ã€‚\"\ntokens = list(tagger_a(test_text))\nprint(f\"\\nTest analysis of '{test_text}':\")\nfor token in tokens:\n    print(f\"  {token.surface} -&gt; {token.feature.lemma} [{','.join(token.pos)}]\")\n\n\nInitializing Plan A: fugashi + UniDic pipeline\nTagger initialized with UniDic dictionary: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/unidic/dicdir\n\nTest analysis of 'æ—¥ã”ã‚ã‹ã‚‰å‹‰å¼·ã—ã¦ã„ã‚‹ã€‚':\n  æ—¥ã”ã‚ -&gt; æ—¥é ƒ [å,è©,,,æ™®,é€š,å,è©,,,å‰¯,è©,å¯,èƒ½,,,*]\n  ã‹ã‚‰ -&gt; ã‹ã‚‰ [åŠ©,è©,,,æ ¼,åŠ©,è©,,,*,,,*]\n  å‹‰å¼· -&gt; å‹‰å¼· [å,è©,,,æ™®,é€š,å,è©,,,ã‚µ,å¤‰,å¯,èƒ½,,,*]\n  ã— -&gt; ç‚ºã‚‹ [å‹•,è©,,,é,è‡ª,ç«‹,å¯,èƒ½,,,*,,,*]\n  ã¦ -&gt; ã¦ [åŠ©,è©,,,æ¥,ç¶š,åŠ©,è©,,,*,,,*]\n  ã„ã‚‹ -&gt; å±…ã‚‹ [å‹•,è©,,,é,è‡ª,ç«‹,å¯,èƒ½,,,*,,,*]\n  ã€‚ -&gt; ã€‚ [è£œ,åŠ©,è¨˜,å·,,,å¥,ç‚¹,,,*,,,*]\n\n\n\n\nShow code\n# A-4: Morphological field extraction function\ndef iter_lemma_keys_plan_a(text: str, tagger) -&gt; List[Tuple[str, str, str]]:\n    \"\"\"\n    Extract (lemma, reading, pos_major) tuples from text using UniDic.\n    \n    Args:\n        text: Input Japanese text\n        tagger: fugashi Tagger instance\n    \n    Returns:\n        List of (dictionary_form, reading, pos_major) tuples\n    \"\"\"\n    keys = []\n    for m in tagger(text):\n        if m.surface.strip():  # Skip empty tokens\n            # UniDic POS is hierarchical; use major category (pos[0])\n            pos_major = m.pos[0] if m.pos else 'UNKNOWN'\n            lemma = m.feature[10] if m.feature[10] else m.surface\n            reading = m.feature[11] if m.feature[11] else ''\n            keys.append((lemma, reading, pos_major))\n    return keys\n\n# Test the extraction function\ntest_keys = iter_lemma_keys_plan_a(test_text, tagger_a)\nprint(f\"Extracted keys from '{test_text}':\")\nfor lemma, reading, pos in test_keys:\n    print(f\"  ({lemma}, {reading}, {pos})\")\n\n\nExtracted keys from 'æ—¥ã”ã‚ã‹ã‚‰å‹‰å¼·ã—ã¦ã„ã‚‹ã€‚':\n  (æ—¥ã”ã‚, ãƒ’ã‚´ãƒ­, å)\n  (ã‹ã‚‰, ã‚«ãƒ©, åŠ©)\n  (å‹‰å¼·, ãƒ™ãƒ³ã‚­ãƒ§ãƒ¼, å)\n  (ã™ã‚‹, ã‚¹ãƒ«, å‹•)\n  (ã¦, ãƒ†, åŠ©)\n  (ã„ã‚‹, ã‚¤ãƒ«, å‹•)\n  (ã€‚, *, è£œ)\n\n\n\n\nShow code\n# Fixed version with proper fugashi/UniDic attribute handling\ndef iter_lemma_keys_fixed(text: str, tagger) -&gt; List[Tuple[str, str, str]]:\n    \"\"\"\n    Extract (lemma, reading, pos_major) tuples from text using UniDic.\n    Fixed version that handles fugashi attribute variations.\n    \"\"\"\n    keys = []\n    for m in tagger(text):\n        if m.surface.strip():  # Skip empty tokens\n            # UniDic POS is hierarchical; use major category (pos[0])\n            pos_major = m.pos[0] if m.pos else 'UNKNOWN'\n            \n            # Handle different attribute names for lemma\n            try:\n                lemma = m.lemma if hasattr(m, 'lemma') else m.feature[10]\n            except:\n                lemma = m.surface  # fallback\n            \n            # Handle different attribute names for reading\n            try:\n                reading = m.feature[9] if len(m.feature) &gt; 9 else ''\n            except:\n                reading = ''  # fallback\n            \n            keys.append((lemma, reading, pos_major))\n    return keys\n\n# Use the fixed function\niter_lemma_keys_plan_a = iter_lemma_keys_fixed\n\n# Test the fixed function\ntest_keys = iter_lemma_keys_plan_a(test_text, tagger_a)\nprint(f\"Extracted keys from '{test_text}' (fixed version):\")\nfor lemma, reading, pos in test_keys:\n    print(f\"  ({lemma}, {reading}, {pos})\")\n\n\nExtracted keys from 'æ—¥ã”ã‚ã‹ã‚‰å‹‰å¼·ã—ã¦ã„ã‚‹ã€‚' (fixed version):\n  (æ—¥ã”ã‚, ãƒ’ã‚´ãƒ­, å)\n  (ã‹ã‚‰, ã‚«ãƒ©, åŠ©)\n  (å‹‰å¼·, ãƒ™ãƒ³ã‚­ãƒ§ãƒ¼, å)\n  (ã™ã‚‹, ã‚·, å‹•)\n  (ã¦, ãƒ†, åŠ©)\n  (ã„ã‚‹, ã‚¤ãƒ«, å‹•)\n  (ã€‚, *, è£œ)\n\n\n\n\nShow code\n# A-5: Frequency analysis with BCCWJ matching\ndef analyze_corpus_plan_a(corpus: List[str], tagger, bccwj_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Analyze corpus using Plan A and match with BCCWJ frequencies.\"\"\"\n    freq = Counter()\n    \n    print(f\"Analyzing {len(corpus)} texts with Plan A...\")\n    for text in corpus:\n        for key in iter_lemma_keys_plan_a(text, tagger):\n            freq[key] += 1\n    \n    # Convert to DataFrame\n    rows = []\n    for (lemma, reading, pos), count in freq.items():\n        rows.append((lemma, reading, pos, count))\n    \n    df_local = pd.DataFrame(rows, columns=['lemma', 'reading', 'pos', 'freq_local'])\n    df_local['key'] = list(zip(df_local.lemma, df_local.reading, df_local.pos))\n    \n    # Merge with BCCWJ data\n    merged = df_local.merge(bccwj_df[['key', 'freq_bccwj']], on='key', how='left')\n    \n    return merged.sort_values('freq_local', ascending=False)\n\n# Run Plan A analysis\nresults_a = analyze_corpus_plan_a(extended_corpus, tagger_a, df_bccwj)\nprint(f\"\\nPlan A Results (top 15):\")\nprint(results_a.head(15)[['lemma', 'reading', 'pos', 'freq_local', 'freq_bccwj']])\n\n\nAnalyzing 30 texts with Plan A...\n\nPlan A Results (top 15):\n   lemma reading pos  freq_local  freq_bccwj\n11     ã€‚       *   è£œ          30         NaN\n8      ãŒ       ã‚¬   åŠ©          18         NaN\n1      ã¯       ãƒ¯   åŠ©          15         NaN\n7      ã®       ãƒ   åŠ©          15         NaN\n5      ã‚’       ã‚ª   åŠ©          12         NaN\n10    ã§ã™      ãƒ‡ã‚¹   åŠ©           9         NaN\n42     ã§       ãƒ‡   åŠ©           9         NaN\n15     ã        ãƒ€   åŠ©           6         NaN\n37     ã¦       ãƒ†   åŠ©           6         NaN\n41    é£²ã‚€      ãƒãƒ³   å‹•           6         NaN\n43     ã€       *   è£œ           6         NaN\n48    å›½éš›    ã‚³ã‚¯ã‚µã‚¤   å           3         NaN\n47     ãŸ       ã‚¿   åŠ©           3         NaN\n46   ã—ã¾ã†     ã‚·ãƒãƒƒ   å‹•           3         NaN\n0      å½¼      ã‚«ãƒ¬   ä»£           3         NaN\n\n\n\n\nShow code\n# A-6: Evaluation metrics for Plan A\ndef calculate_metrics(df: pd.DataFrame) -&gt; Dict[str, float]:\n    \"\"\"Calculate coverage and correlation metrics.\"\"\"\n    # Coverage: percentage of local tokens found in BCCWJ\n    matched = df.dropna(subset=['freq_bccwj'])\n    coverage = len(matched) / len(df) * 100\n    \n    # Token coverage (by frequency)\n    total_tokens = df['freq_local'].sum()\n    matched_tokens = matched['freq_local'].sum()\n    token_coverage = matched_tokens / total_tokens * 100\n    \n    # Spearman correlation for matched items\n    if len(matched) &gt; 1:\n        if scipy_available:\n            correlation, p_value = spearmanr(matched['freq_local'], matched['freq_bccwj'])\n        else:\n            correlation = np.corrcoef(matched['freq_local'].rank(), matched['freq_bccwj'].rank())[0,1]\n            p_value = None\n    else:\n        correlation, p_value = None, None\n    \n    return {\n        'type_coverage': coverage,\n        'token_coverage': token_coverage,\n        'correlation': correlation,\n        'p_value': p_value,\n        'total_types': len(df),\n        'matched_types': len(matched),\n        'total_tokens': total_tokens,\n        'matched_tokens': matched_tokens\n    }\n\nmetrics_a = calculate_metrics(results_a)\nprint(\"Plan A Evaluation Metrics:\")\nfor key, value in metrics_a.items():\n    if isinstance(value, float) and value is not None:\n        print(f\"  {key}: {value:.3f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\n\nPlan A Evaluation Metrics:\n  type_coverage: 0.000\n  token_coverage: 0.000\n  correlation: None\n  p_value: None\n  total_types: 66\n  matched_types: 0\n  total_tokens: 297\n  matched_tokens: 0"
  },
  {
    "objectID": "resources/code-examples/python/index.html",
    "href": "resources/code-examples/python/index.html",
    "title": "Python Notebooks",
    "section": "",
    "text": "Python notebooks for corpus analysis tasks."
  },
  {
    "objectID": "resources/code-examples/python/index.html#available-notebooks",
    "href": "resources/code-examples/python/index.html#available-notebooks",
    "title": "Python Notebooks",
    "section": "",
    "text": "Python notebooks for corpus analysis tasks."
  },
  {
    "objectID": "resources/code-examples/python/index.html#how-to-use",
    "href": "resources/code-examples/python/index.html#how-to-use",
    "title": "Python Notebooks",
    "section": "How to Use",
    "text": "How to Use\n\nClick on any notebook link\nOpen in Google Colab\nMake a copy to your Google Drive\nRun cells sequentially"
  },
  {
    "objectID": "resources/code-examples/index.html",
    "href": "resources/code-examples/index.html",
    "title": "Code Examples",
    "section": "",
    "text": "Python Notebooks\nSpreadsheet Templates",
    "crumbs": [
      "Resources",
      "Code Examples"
    ]
  },
  {
    "objectID": "resources/code-examples/index.html#available-resources",
    "href": "resources/code-examples/index.html#available-resources",
    "title": "Code Examples",
    "section": "",
    "text": "Python Notebooks\nSpreadsheet Templates",
    "crumbs": [
      "Resources",
      "Code Examples"
    ]
  },
  {
    "objectID": "resources/tools/japanese-nlp.html",
    "href": "resources/tools/japanese-nlp.html",
    "title": "Japanese NLP",
    "section": "",
    "text": "This page provides some advanced resources on Japanese NLP."
  },
  {
    "objectID": "resources/tools/japanese-nlp.html#demo-pages",
    "href": "resources/tools/japanese-nlp.html#demo-pages",
    "title": "Japanese NLP",
    "section": "Demo pages",
    "text": "Demo pages\n\nSpacy GiNZA morphological analysis\nWeb app for UniDic morphological analysis"
  },
  {
    "objectID": "resources/tools/japanese-nlp.html#using-unidic-with-fugashi",
    "href": "resources/tools/japanese-nlp.html#using-unidic-with-fugashi",
    "title": "Japanese NLP",
    "section": "Using Unidic with Fugashi",
    "text": "Using Unidic with Fugashi\n\n\nFugashi\n\n# Core tools\npip install fugashi unidic-lite  # quick start (smaller)\n# OR for full UniDic (larger, closer to BCCWJ)\npip install fugashi unidic\npython -m unidic download  # downloads the full UniDic dictionary"
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html",
    "href": "resources/tools/byu-corpora-guide.html",
    "title": "English Corpora Guide",
    "section": "",
    "text": "The EnglishCorpora.org, formally BYU (Brigham Young University) corpora, provide web-based interfaces to some of the largest and most widely-used corpora in the world. These include COCA (Corpus of Contemporary American English), BNC (British National Corpus), and many others.\nYou will need a usable account for english-corpora.org on on Day 1.",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html#overview",
    "href": "resources/tools/byu-corpora-guide.html#overview",
    "title": "English Corpora Guide",
    "section": "",
    "text": "The EnglishCorpora.org, formally BYU (Brigham Young University) corpora, provide web-based interfaces to some of the largest and most widely-used corpora in the world. These include COCA (Corpus of Contemporary American English), BNC (British National Corpus), and many others.\nYou will need a usable account for english-corpora.org on on Day 1.",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html#available-corpora",
    "href": "resources/tools/byu-corpora-guide.html#available-corpora",
    "title": "English Corpora Guide",
    "section": "Available Corpora",
    "text": "Available Corpora\n\nMajor English Corpora\n\nCOCA (Corpus of Contemporary American English): 1 billion words, 1990-2019\nBNC (British National Corpus): 100 million words\nGloWbE (Global Web-Based English): 1.9 billion words\nNOW (News on the Web): 14+ billion words, updated daily\nCOHA (Corpus of Historical American English): 400 million words, 1810-2009\n\n\n\nSpecialized Corpora\n\nSOAP (Corpus of American Soap Operas): 100 million words\nTIME (TIME Magazine Corpus): 100 million words, 1920s-2000s\nWikipedia Corpus: 1.9 billion words",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html#registration-and-access",
    "href": "resources/tools/byu-corpora-guide.html#registration-and-access",
    "title": "English Corpora Guide",
    "section": "Registration and Access",
    "text": "Registration and Access\n\nFree Access\n\nVisit english-corpora.org\nClick on desired corpus\nRegister for free account\nLimited to 20 queries per day\n\n\n\nAcademic License (Do not purchase for this class)\n\nExtended query limits\nDownload capabilities\nAvailable through institution",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html",
    "href": "resources/tools/python-setup.html",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "",
    "text": "In this 5-day intensive course, we will use Python through Google colaboratory, browser based environment that requires no installation on your local computer.\nYou can follow the step here to enable Colaboratory through your gmail account.\nThe following steps should be completed before we start grammar analysis on Day 4.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#overview",
    "href": "resources/tools/python-setup.html#overview",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "",
    "text": "In this 5-day intensive course, we will use Python through Google colaboratory, browser based environment that requires no installation on your local computer.\nYou can follow the step here to enable Colaboratory through your gmail account.\nThe following steps should be completed before we start grammar analysis on Day 4.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#log-in-to-your-google-account.",
    "href": "resources/tools/python-setup.html#log-in-to-your-google-account.",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Log in to your google account.",
    "text": "Log in to your google account.\nYou will need a google account, so log in to your google account.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#go-to-google-drive",
    "href": "resources/tools/python-setup.html#go-to-google-drive",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Go to Google Drive",
    "text": "Go to Google Drive\nGo to google drive.\n\n\n\ngoogle drive",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#hit-new-and-find-connect-more-apps",
    "href": "resources/tools/python-setup.html#hit-new-and-find-connect-more-apps",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Hit new and find connect more apps",
    "text": "Hit new and find connect more apps\n\n\n\nconnect-more-app",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#search-colaboratory-on-the-marketplace",
    "href": "resources/tools/python-setup.html#search-colaboratory-on-the-marketplace",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Search Colaboratory on the marketplace",
    "text": "Search Colaboratory on the marketplace\n\n\n\nsearch-market-place",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#install-colaboratory",
    "href": "resources/tools/python-setup.html#install-colaboratory",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Install Colaboratory",
    "text": "Install Colaboratory\nClick on Colaboratory, and hit install button. \nHit continue when it prompts permission.\n\n\n\nsearch-market-place\n\n\nFollow the instruction of the pop-up instruction.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#installation-success",
    "href": "resources/tools/python-setup.html#installation-success",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Installation success!!",
    "text": "Installation success!!\n\n\n\nsuccess",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#now-you-can-create-google-colab-notebook-from-google-drive.",
    "href": "resources/tools/python-setup.html#now-you-can-create-google-colab-notebook-from-google-drive.",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Now you can create google colab notebook from Google Drive.",
    "text": "Now you can create google colab notebook from Google Drive.\nNow you can create a new google colab notebook.\n\n\n\ncreate a note",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/corpora/index.html",
    "href": "resources/corpora/index.html",
    "title": "Corpora Resources",
    "section": "",
    "text": "Available Corpora\nLearner Corpora",
    "crumbs": [
      "Resources",
      "Corpora",
      "Corpora Resources"
    ]
  },
  {
    "objectID": "resources/corpora/index.html#resources",
    "href": "resources/corpora/index.html#resources",
    "title": "Corpora Resources",
    "section": "",
    "text": "Available Corpora\nLearner Corpora",
    "crumbs": [
      "Resources",
      "Corpora",
      "Corpora Resources"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html",
    "href": "resources/corpora/available-corpora.html",
    "title": "Available Corpora",
    "section": "",
    "text": "Content to be added.\n\nCorpusMate\nMontclair State University - CORAL lab",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#placeholder",
    "href": "resources/corpora/available-corpora.html#placeholder",
    "title": "Available Corpora",
    "section": "",
    "text": "Content to be added.\n\nCorpusMate\nMontclair State University - CORAL lab",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#japanese-corpus",
    "href": "resources/corpora/available-corpora.html#japanese-corpus",
    "title": "Available Corpora",
    "section": "Japanese corpus",
    "text": "Japanese corpus\n\nNINJAL-LWP for TWC\nåå¤§ä¼šè©±ã‚³ãƒ¼ãƒ‘ã‚¹\næ—¥æœ¬èªå¯¾è©±ã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§\né–¢è¥¿å¼ã‚³ãƒ¼ãƒ‘ã‚¹\nAozorabunko-data",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#vocabulary-list",
    "href": "resources/corpora/available-corpora.html#vocabulary-list",
    "title": "Available Corpora",
    "section": "Vocabulary list",
    "text": "Vocabulary list\n\nç¾ä»£æ—¥æœ¬èªæ›¸ãè¨€è‘‰å‡è¡¡ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆBCCWJï¼‰ã€€å…¬é–‹ãƒ‡ãƒ¼ã‚¿\næ—¥æœ¬èªè©±ã—è¨€è‘‰ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆCSJ)\nå›½èªç ”æ—¥æœ¬èªã‚¦ã‚§ãƒ–ã‚³ãƒ¼ãƒ‘ã‚¹",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#corpus-data",
    "href": "resources/corpora/available-corpora.html#corpus-data",
    "title": "Available Corpora",
    "section": "Corpus data",
    "text": "Corpus data\n\nwortschatz corpus",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#databases",
    "href": "resources/corpora/available-corpora.html#databases",
    "title": "Available Corpora",
    "section": "Databases",
    "text": "Databases\n\nCollection of age of acquisition ratings for over 5,000 Japanese words\nJALEX: Japanese Version of Lexical Decision Database\nAWD-J: AWD-J: Abstractness of Word Database for Japanese common words",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  }
]
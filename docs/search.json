[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linguistic Data Analysis I",
    "section": "",
    "text": "Under Construction\n\n\n\nThis course website is currently under construction and will be ready for the class starting August 2nd, 2025. Content is being actively developed and updated."
  },
  {
    "objectID": "index.html#welcome-to-linguistic-data-analysis-i",
    "href": "index.html#welcome-to-linguistic-data-analysis-i",
    "title": "Linguistic Data Analysis I",
    "section": "Welcome to Linguistic Data Analysis I",
    "text": "Welcome to Linguistic Data Analysis I\nThis intensive 5-day graduate course introduces students to corpus linguistics and learner language analysis. Through hands-on activities and practical applications, youâ€™ll learn to use computational tools to analyze linguistic data, with a special focus on learner corpora.\n\n\n\n\n\n\nQuick Links\n\n\n\n\nğŸ“‹ Course Syllabus\nğŸ“… Schedule\nğŸ’» Sessions\nğŸ“ Assignments\nğŸ”§ Resources"
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Linguistic Data Analysis I",
    "section": "Course Overview",
    "text": "Course Overview\n\n\n\nWhat Youâ€™ll Learn\n\nCorpus analysis techniques\nLearner language analysis methods\nPractical applications with real corpora\nResearch methodology in corpus linguistics\n\n\n\n\nKey Tools\n\nAntConc - Corpus analysis software\nBYU Corpora - Online corpus interfaces\nPython - Text processing (via Google Colab)\nJASP - Statistical analysis"
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Linguistic Data Analysis I",
    "section": "Course Structure",
    "text": "Course Structure\nThe course is organized into 5 days:\n\n\n\nDay\nTheme\nSessions\n\n\n\n\nDay 1\nIntroduction & Corpus Basics\n\n\n\nDay 2\nAnalysis of Vocabulary & Multiword Units (1)\n\n\n\nDay 3\nAnalysis of Vocabulary & Multiword Units (2)\n\n\n\nDay 4\nAnalysis of Grammar\n\n\n\nDay 5\nAdvanced Topics & Projects"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Linguistic Data Analysis I",
    "section": "Getting Started",
    "text": "Getting Started\n\nReview the syllabus for course policies and expectations\nCheck the detailed schedule for session timings\nInstall required software using our setup guides\nBrowse the resources section for helpful materials"
  },
  {
    "objectID": "index.html#instructor-information",
    "href": "index.html#instructor-information",
    "title": "Linguistic Data Analysis I",
    "section": "Instructor Information",
    "text": "Instructor Information\nInstructor: Masaki Eguchi, Ph.D.\nEmail: You can contact me through Google Classroom"
  },
  {
    "objectID": "index.html#course-communication",
    "href": "index.html#course-communication",
    "title": "Linguistic Data Analysis I",
    "section": "Course Communication",
    "text": "Course Communication\n\n\n\n\n\n\nStay Connected\n\n\n\n\nCourse Website: This site\nCommunication: Google Classroom\nAssignment Submission: Google Classroom"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Linguistic Data Analysis I",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis course builds on materials and approaches from:\n\nDr.Â Kris Kyle (University of Oregon) for his previous corpus linguistics/NLP classes from University of Hawaiâ€™i and Oregon.\nDr.Â Andrew Heiss (Georgia State University) for his Quarto-based materials and website settings, which significantly enhanced the accessibility of the course content."
  },
  {
    "objectID": "resources/corpora/learner-corpora.html",
    "href": "resources/corpora/learner-corpora.html",
    "title": "Learner Corpora",
    "section": "",
    "text": "Content to be added.\n\nJapanese Learner corpus Natane\nInternational Center for Japanese Studies\næ—¥æœ¬èªå­¦ç¿’è€…ä¼šè©±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
    "crumbs": [
      "Resources",
      "Corpora",
      "Learner Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/learner-corpora.html#placeholder",
    "href": "resources/corpora/learner-corpora.html#placeholder",
    "title": "Learner Corpora",
    "section": "",
    "text": "Content to be added.\n\nJapanese Learner corpus Natane\nInternational Center for Japanese Studies\næ—¥æœ¬èªå­¦ç¿’è€…ä¼šè©±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
    "crumbs": [
      "Resources",
      "Corpora",
      "Learner Corpora"
    ]
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Course resources organized by category.",
    "crumbs": [
      "Resources",
      "Resources"
    ]
  },
  {
    "objectID": "resources/index.html#overview",
    "href": "resources/index.html#overview",
    "title": "Resources",
    "section": "",
    "text": "Course resources organized by category.",
    "crumbs": [
      "Resources",
      "Resources"
    ]
  },
  {
    "objectID": "resources/index.html#categories",
    "href": "resources/index.html#categories",
    "title": "Resources",
    "section": "Categories",
    "text": "Categories\n\nTools and Software\nCorpora\nCode Examples",
    "crumbs": [
      "Resources",
      "Resources"
    ]
  },
  {
    "objectID": "resources/tools/index.html",
    "href": "resources/tools/index.html",
    "title": "Tools and Software",
    "section": "",
    "text": "BYU Corpora Guide\nAntConc Guide\nPython (google colab) Setup\nJASP Guide",
    "crumbs": [
      "Resources",
      "Tools",
      "Tools and Software"
    ]
  },
  {
    "objectID": "resources/tools/index.html#available-guides",
    "href": "resources/tools/index.html#available-guides",
    "title": "Tools and Software",
    "section": "",
    "text": "BYU Corpora Guide\nAntConc Guide\nPython (google colab) Setup\nJASP Guide",
    "crumbs": [
      "Resources",
      "Tools",
      "Tools and Software"
    ]
  },
  {
    "objectID": "resources/tools/index.html#other-useful-corpus-resources",
    "href": "resources/tools/index.html#other-useful-corpus-resources",
    "title": "Tools and Software",
    "section": "Other useful corpus resources",
    "text": "Other useful corpus resources\n\nCIABATTTA\nCorpus and Repository of Writing",
    "crumbs": [
      "Resources",
      "Tools",
      "Tools and Software"
    ]
  },
  {
    "objectID": "resources/tools/antconc-guide.html",
    "href": "resources/tools/antconc-guide.html",
    "title": "AntConc Guide",
    "section": "",
    "text": "We will use AntConc, a one of the most widely used corpus tool developed by Laurence ANTHONY (Waseda University).\nThanksfully, Laurence has shared tutorials on basic features of AntConc on Youtube.\nYou will need AntConc on Day 2 and 3.\nPlease complete the following steps before Day 2.\n\nDownload AntConc\n\n\nVisit AntConc Website; download the software to your computer.\n\n\nWatch the following tutorial videos\n\n\nLaurence Anthonyâ€™s intro to AntConc\n\nGetting started (10 mins)\nCorpus manager Basics (18 mins)\nKWIC tool basics (17 mins)\n\n\nIf you are unsure about these steps, do not hesitate to reach out to me through google classroom or through email.",
    "crumbs": [
      "Resources",
      "Tools",
      "AntConc Guide"
    ]
  },
  {
    "objectID": "resources/tools/antconc-guide.html#overview",
    "href": "resources/tools/antconc-guide.html#overview",
    "title": "AntConc Guide",
    "section": "",
    "text": "We will use AntConc, a one of the most widely used corpus tool developed by Laurence ANTHONY (Waseda University).\nThanksfully, Laurence has shared tutorials on basic features of AntConc on Youtube.\nYou will need AntConc on Day 2 and 3.\nPlease complete the following steps before Day 2.\n\nDownload AntConc\n\n\nVisit AntConc Website; download the software to your computer.\n\n\nWatch the following tutorial videos\n\n\nLaurence Anthonyâ€™s intro to AntConc\n\nGetting started (10 mins)\nCorpus manager Basics (18 mins)\nKWIC tool basics (17 mins)\n\n\nIf you are unsure about these steps, do not hesitate to reach out to me through google classroom or through email.",
    "crumbs": [
      "Resources",
      "Tools",
      "AntConc Guide"
    ]
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html",
    "href": "2025/notebooks/japanese-nlp-test.html",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "",
    "text": "This notebook implements and compares two approaches for Japanese morphological analysis with BCCWJ frequency matching:\nEach approach is designed for reproducible setup, implementation, validation, and operational use."
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html#environment-setup-verification",
    "href": "2025/notebooks/japanese-nlp-test.html#environment-setup-verification",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "1. Environment Setup & Verification",
    "text": "1. Environment Setup & Verification\nFirst, letâ€™s verify and set up our environment with all required packages.\n\n\nShow code\n# Environment verification and setup\nimport sys\nimport subprocess\nfrom pathlib import Path\n\nprint(f\"Python version: {sys.version}\")\nprint(f\"Working directory: {Path.cwd()}\")\n\n# Required packages\nrequired_packages = [\n    'fugashi', 'unidic', 'unidic-lite', 'spacy', 'ginza', \n    'ja-ginza', 'sudachipy', 'pandas', 'numpy', 'matplotlib', 'collections'\n]\n\nprint(\"\\nChecking package availability:\")\nfor package in required_packages:\n    try:\n        if package == 'collections':\n            import collections\n            print(f\"âœ“ {package} (built-in)\")\n        else:\n            __import__(package)\n            print(f\"âœ“ {package}\")\n    except ImportError:\n        print(f\"âœ— {package} - NOT FOUND\")\n\n\nPython version: 3.12.2 (main, Feb 25 2024, 03:55:42) [Clang 17.0.6 ]\nWorking directory: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/2025/notebooks\n\nChecking package availability:\nâœ“ fugashi\nâœ“ unidic\nâœ— unidic-lite - NOT FOUND\nâœ“ spacy\nâœ“ ginza\nâœ— ja-ginza - NOT FOUND\nâœ“ sudachipy\nâœ“ pandas\nâœ“ numpy\nâœ“ matplotlib\nâœ“ collections (built-in)\n\n\n\n\nShow code\n# Import all necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter, defaultdict\nimport time\nimport warnings\nfrom typing import List, Tuple, Dict, Optional\n\n# Japanese NLP libraries\nimport fugashi\nimport unidic\nimport spacy\nfrom spacy.tokens import Token, Doc\n\n# Statistical analysis\ntry:\n    from scipy.stats import spearmanr\n    scipy_available = True\nexcept ImportError:\n    print(\"scipy not available - will use numpy for correlation\")\n    scipy_available = False\n\nprint(\"All imports successful!\")\nwarnings.filterwarnings('ignore')\n\n\nscipy not available - will use numpy for correlation\nAll imports successful!\n\n\n\n\nShow code\n# Check UniDic installation and download if needed\ntry:\n    print(f\"UniDic directory: {unidic.DICDIR}\")\n    print(\"UniDic is properly installed\")\nexcept Exception as e:\n    print(f\"UniDic issue: {e}\")\n    print(\"You may need to run: python -m unidic download\")\n\n# Test basic fugashi functionality\ntry:\n    tagger = fugashi.Tagger(f'-d \"{unidic.DICDIR}\"')\n    test_result = list(tagger(\"ãƒ†ã‚¹ãƒˆ\"))\n    print(f\"Fugashi + UniDic test successful: {test_result[0].surface}\")\nexcept Exception as e:\n    print(f\"Fugashi test failed: {e}\")\n\n\nUniDic directory: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/unidic/dicdir\nUniDic is properly installed\nFugashi + UniDic test successful: ãƒ†ã‚¹ãƒˆ"
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html#sample-data-preparation",
    "href": "2025/notebooks/japanese-nlp-test.html#sample-data-preparation",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "2. Sample Data Preparation",
    "text": "2. Sample Data Preparation\nLetâ€™s create realistic Japanese text samples for testing our pipelines.\n\n\nShow code\n# Sample Japanese texts for testing\nsample_texts = [\n    \"å½¼ã¯æ—¥ã”ã‚ã‹ã‚‰æœ¬ã‚’èª­ã‚€ã®ãŒå¥½ãã§ã™ã€‚\",\n    \"ã²ã”ã‚ã®å‹‰å¼·ãŒå¤§åˆ‡ã ã¨æ€ã„ã¾ã™ã€‚\",\n    \"æ—¥é ƒã®åŠªåŠ›ãŒå®Ÿã‚’çµã¶ã§ã—ã‚‡ã†ã€‚\",\n    \"å½¼å¥³ã¯æ›¸ãã‚ã‚‰ã‚ã™ã“ã¨ãŒå¾—æ„ã§ã™ã€‚\",\n    \"ãã®å•é¡Œã‚’æ›¸ãè¡¨ã™ã®ã¯é›£ã—ã„ã€‚\",\n    \"ä»Šæ—¥ã¯æ±äº¬ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦è©±ã—ã¾ã—ã‚‡ã†ã€‚\",\n    \"ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’é£²ã‚“ã§ã€å‘‘ã¿è¾¼ã‚“ã§ã€ã¾ãŸé£²ã‚“ã§ã—ã¾ã£ãŸã€‚\",\n    \"å›½éš›çš„ãªå”åŠ›ãŒå¿…è¦ä¸å¯æ¬ ã§ã™ã€‚\",\n    \"æ©Ÿæ¢°å­¦ç¿’ã®æŠ€è¡“ãŒé€²æ­©ã—ã¦ã„ã‚‹ã€‚\",\n    \"è‡ªç„¶è¨€èªå‡¦ç†ã¯èˆˆå‘³æ·±ã„åˆ†é‡ã ã€‚\"\n]\n\nprint(\"Sample texts prepared:\")\nfor i, text in enumerate(sample_texts, 1):\n    print(f\"{i:2d}. {text}\")\n\n# Create a larger corpus by repeating and slightly modifying texts\nextended_corpus = sample_texts * 3  # Simulate frequency variations\nprint(f\"\\nExtended corpus: {len(extended_corpus)} texts\")\n\n\nSample texts prepared:\n 1. å½¼ã¯æ—¥ã”ã‚ã‹ã‚‰æœ¬ã‚’èª­ã‚€ã®ãŒå¥½ãã§ã™ã€‚\n 2. ã²ã”ã‚ã®å‹‰å¼·ãŒå¤§åˆ‡ã ã¨æ€ã„ã¾ã™ã€‚\n 3. æ—¥é ƒã®åŠªåŠ›ãŒå®Ÿã‚’çµã¶ã§ã—ã‚‡ã†ã€‚\n 4. å½¼å¥³ã¯æ›¸ãã‚ã‚‰ã‚ã™ã“ã¨ãŒå¾—æ„ã§ã™ã€‚\n 5. ãã®å•é¡Œã‚’æ›¸ãè¡¨ã™ã®ã¯é›£ã—ã„ã€‚\n 6. ä»Šæ—¥ã¯æ±äº¬ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦è©±ã—ã¾ã—ã‚‡ã†ã€‚\n 7. ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’é£²ã‚“ã§ã€å‘‘ã¿è¾¼ã‚“ã§ã€ã¾ãŸé£²ã‚“ã§ã—ã¾ã£ãŸã€‚\n 8. å›½éš›çš„ãªå”åŠ›ãŒå¿…è¦ä¸å¯æ¬ ã§ã™ã€‚\n 9. æ©Ÿæ¢°å­¦ç¿’ã®æŠ€è¡“ãŒé€²æ­©ã—ã¦ã„ã‚‹ã€‚\n10. è‡ªç„¶è¨€èªå‡¦ç†ã¯èˆˆå‘³æ·±ã„åˆ†é‡ã ã€‚\n\nExtended corpus: 30 texts\n\n\n\n\nShow code\n# Create mock BCCWJ frequency data for testing\n# In real usage, this would be loaded from an actual BCCWJ frequency file\n\nmock_bccwj_data = [\n    ('æ—¥é ƒ', 'ãƒ’ã‚´ãƒ­', 'åè©', 1250),\n    ('æœ¬', 'ãƒ›ãƒ³', 'åè©', 8500),\n    ('èª­ã‚€', 'ãƒ¨ãƒ ', 'å‹•è©', 3200),\n    ('å¥½ã', 'ã‚¹ã‚­', 'å½¢å®¹å‹•è©', 2100),\n    ('å‹‰å¼·', 'ãƒ™ãƒ³ã‚­ãƒ§ã‚¦', 'åè©', 4200),\n    ('å¤§åˆ‡', 'ã‚¿ã‚¤ã‚»ãƒ„', 'å½¢å®¹å‹•è©', 1800),\n    ('æ€ã†', 'ã‚ªãƒ¢ã‚¦', 'å‹•è©', 9500),\n    ('åŠªåŠ›', 'ãƒ‰ãƒªãƒ§ã‚¯', 'åè©', 2200),\n    ('å®Ÿ', 'ãƒŸ', 'åè©', 1100),\n    ('çµã¶', 'ãƒ ã‚¹ãƒ–', 'å‹•è©', 800),\n    ('æ›¸ã', 'ã‚«ã‚¯', 'å‹•è©', 4100),\n    ('è¡¨ã™', 'ã‚¢ãƒ©ãƒ¯ã‚¹', 'å‹•è©', 1500),\n    ('å¾—æ„', 'ãƒˆã‚¯ã‚¤', 'å½¢å®¹å‹•è©', 1300),\n    ('å•é¡Œ', 'ãƒ¢ãƒ³ãƒ€ã‚¤', 'åè©', 6200),\n    ('é›£ã—ã„', 'ãƒ ã‚ºã‚«ã‚·ã‚¤', 'å½¢å®¹è©', 3800),\n    ('ä»Šæ—¥', 'ã‚­ãƒ§ã‚¦', 'åè©', 5500),\n    ('æ±äº¬', 'ãƒˆã‚¦ã‚­ãƒ§ã‚¦', 'åè©', 4800),\n    ('è©±ã™', 'ãƒãƒŠã‚¹', 'å‹•è©', 3600),\n    ('é£²ã‚€', 'ãƒãƒ ', 'å‹•è©', 2400),\n    ('å‘‘ã‚€', 'ãƒãƒ ', 'å‹•è©', 150),\n    ('å›½éš›', 'ã‚³ã‚¯ã‚µã‚¤', 'åè©', 2800),\n    ('å”åŠ›', 'ã‚­ãƒ§ã‚¦ãƒªãƒ§ã‚¯', 'åè©', 1900),\n    ('å¿…è¦', 'ãƒ’ãƒ„ãƒ¨ã‚¦', 'å½¢å®¹å‹•è©', 4500),\n    ('æŠ€è¡“', 'ã‚®ã‚¸ãƒ¥ãƒ„', 'åè©', 3900),\n    ('é€²æ­©', 'ã‚·ãƒ³ãƒ', 'åè©', 1100)\n]\n\n# Create DataFrame\ndf_bccwj = pd.DataFrame(mock_bccwj_data, columns=['lemma', 'reading', 'pos', 'freq_bccwj'])\ndf_bccwj['key'] = list(zip(df_bccwj.lemma, df_bccwj.reading, df_bccwj.pos))\n\nprint(\"Mock BCCWJ frequency data:\")\nprint(df_bccwj.head(10))\nprint(f\"\\nTotal entries: {len(df_bccwj)}\")\n\n\nMock BCCWJ frequency data:\n  lemma reading   pos  freq_bccwj               key\n0    æ—¥é ƒ     ãƒ’ã‚´ãƒ­    åè©        1250     (æ—¥é ƒ, ãƒ’ã‚´ãƒ­, åè©)\n1     æœ¬      ãƒ›ãƒ³    åè©        8500       (æœ¬, ãƒ›ãƒ³, åè©)\n2    èª­ã‚€      ãƒ¨ãƒ     å‹•è©        3200      (èª­ã‚€, ãƒ¨ãƒ , å‹•è©)\n3    å¥½ã      ã‚¹ã‚­  å½¢å®¹å‹•è©        2100    (å¥½ã, ã‚¹ã‚­, å½¢å®¹å‹•è©)\n4    å‹‰å¼·   ãƒ™ãƒ³ã‚­ãƒ§ã‚¦    åè©        4200   (å‹‰å¼·, ãƒ™ãƒ³ã‚­ãƒ§ã‚¦, åè©)\n5    å¤§åˆ‡    ã‚¿ã‚¤ã‚»ãƒ„  å½¢å®¹å‹•è©        1800  (å¤§åˆ‡, ã‚¿ã‚¤ã‚»ãƒ„, å½¢å®¹å‹•è©)\n6    æ€ã†     ã‚ªãƒ¢ã‚¦    å‹•è©        9500     (æ€ã†, ã‚ªãƒ¢ã‚¦, å‹•è©)\n7    åŠªåŠ›    ãƒ‰ãƒªãƒ§ã‚¯    åè©        2200    (åŠªåŠ›, ãƒ‰ãƒªãƒ§ã‚¯, åè©)\n8     å®Ÿ       ãƒŸ    åè©        1100        (å®Ÿ, ãƒŸ, åè©)\n9    çµã¶     ãƒ ã‚¹ãƒ–    å‹•è©         800     (çµã¶, ãƒ ã‚¹ãƒ–, å‹•è©)\n\nTotal entries: 25"
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html#plan-a-mecab-fugashi-unidic-direct-pipeline",
    "href": "2025/notebooks/japanese-nlp-test.html#plan-a-mecab-fugashi-unidic-direct-pipeline",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "3. Plan A: MeCab (fugashi) + UniDic Direct Pipeline",
    "text": "3. Plan A: MeCab (fugashi) + UniDic Direct Pipeline\n\nA-1 to A-3: Setup and Configuration\nUniDic provides the morphological analysis system used in BCCWJ, making it ideal for frequency matching.\n\n\nShow code\n# A-3: Initialize fugashi with UniDic\nprint(\"Initializing Plan A: fugashi + UniDic pipeline\")\n\n# Initialize tagger with explicit UniDic path\ntagger_a = fugashi.Tagger(f'-d \"{unidic.DICDIR}\"')\nprint(f\"Tagger initialized with UniDic dictionary: {unidic.DICDIR}\")\n\n# Test the tagger\ntest_text = \"æ—¥ã”ã‚ã‹ã‚‰å‹‰å¼·ã—ã¦ã„ã‚‹ã€‚\"\ntokens = list(tagger_a(test_text))\nprint(f\"\\nTest analysis of '{test_text}':\")\nfor token in tokens:\n    print(f\"  {token.surface} -&gt; {token.feature.lemma} [{','.join(token.pos)}]\")\n\n\nInitializing Plan A: fugashi + UniDic pipeline\nTagger initialized with UniDic dictionary: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/unidic/dicdir\n\nTest analysis of 'æ—¥ã”ã‚ã‹ã‚‰å‹‰å¼·ã—ã¦ã„ã‚‹ã€‚':\n  æ—¥ã”ã‚ -&gt; æ—¥é ƒ [å,è©,,,æ™®,é€š,å,è©,,,å‰¯,è©,å¯,èƒ½,,,*]\n  ã‹ã‚‰ -&gt; ã‹ã‚‰ [åŠ©,è©,,,æ ¼,åŠ©,è©,,,*,,,*]\n  å‹‰å¼· -&gt; å‹‰å¼· [å,è©,,,æ™®,é€š,å,è©,,,ã‚µ,å¤‰,å¯,èƒ½,,,*]\n  ã— -&gt; ç‚ºã‚‹ [å‹•,è©,,,é,è‡ª,ç«‹,å¯,èƒ½,,,*,,,*]\n  ã¦ -&gt; ã¦ [åŠ©,è©,,,æ¥,ç¶š,åŠ©,è©,,,*,,,*]\n  ã„ã‚‹ -&gt; å±…ã‚‹ [å‹•,è©,,,é,è‡ª,ç«‹,å¯,èƒ½,,,*,,,*]\n  ã€‚ -&gt; ã€‚ [è£œ,åŠ©,è¨˜,å·,,,å¥,ç‚¹,,,*,,,*]\n\n\n\n\nShow code\n# A-4: Morphological field extraction function\ndef iter_lemma_keys_plan_a(text: str, tagger) -&gt; List[Tuple[str, str, str]]:\n    \"\"\"\n    Extract (lemma, reading, pos_major) tuples from text using UniDic.\n    \n    Args:\n        text: Input Japanese text\n        tagger: fugashi Tagger instance\n    \n    Returns:\n        List of (dictionary_form, reading, pos_major) tuples\n    \"\"\"\n    keys = []\n    for m in tagger(text):\n        if m.surface.strip():  # Skip empty tokens\n            # UniDic POS is hierarchical; use major category (pos[0])\n            pos_major = m.pos[0] if m.pos else 'UNKNOWN'\n            lemma = m.feature[10] if m.feature[10] else m.surface\n            reading = m.feature[11] if m.feature[11] else ''\n            keys.append((lemma, reading, pos_major))\n    return keys\n\n# Test the extraction function\ntest_keys = iter_lemma_keys_plan_a(test_text, tagger_a)\nprint(f\"Extracted keys from '{test_text}':\")\nfor lemma, reading, pos in test_keys:\n    print(f\"  ({lemma}, {reading}, {pos})\")\n\n\nExtracted keys from 'æ—¥ã”ã‚ã‹ã‚‰å‹‰å¼·ã—ã¦ã„ã‚‹ã€‚':\n  (æ—¥ã”ã‚, ãƒ’ã‚´ãƒ­, å)\n  (ã‹ã‚‰, ã‚«ãƒ©, åŠ©)\n  (å‹‰å¼·, ãƒ™ãƒ³ã‚­ãƒ§ãƒ¼, å)\n  (ã™ã‚‹, ã‚¹ãƒ«, å‹•)\n  (ã¦, ãƒ†, åŠ©)\n  (ã„ã‚‹, ã‚¤ãƒ«, å‹•)\n  (ã€‚, *, è£œ)\n\n\n\n\nShow code\n# Fixed version with proper fugashi/UniDic attribute handling\ndef iter_lemma_keys_fixed(text: str, tagger) -&gt; List[Tuple[str, str, str]]:\n    \"\"\"\n    Extract (lemma, reading, pos_major) tuples from text using UniDic.\n    Fixed version that handles fugashi attribute variations.\n    \"\"\"\n    keys = []\n    for m in tagger(text):\n        if m.surface.strip():  # Skip empty tokens\n            # UniDic POS is hierarchical; use major category (pos[0])\n            pos_major = m.pos[0] if m.pos else 'UNKNOWN'\n            \n            # Handle different attribute names for lemma\n            try:\n                lemma = m.lemma if hasattr(m, 'lemma') else m.feature[10]\n            except:\n                lemma = m.surface  # fallback\n            \n            # Handle different attribute names for reading\n            try:\n                reading = m.feature[9] if len(m.feature) &gt; 9 else ''\n            except:\n                reading = ''  # fallback\n            \n            keys.append((lemma, reading, pos_major))\n    return keys\n\n# Use the fixed function\niter_lemma_keys_plan_a = iter_lemma_keys_fixed\n\n# Test the fixed function\ntest_keys = iter_lemma_keys_plan_a(test_text, tagger_a)\nprint(f\"Extracted keys from '{test_text}' (fixed version):\")\nfor lemma, reading, pos in test_keys:\n    print(f\"  ({lemma}, {reading}, {pos})\")\n\n\nExtracted keys from 'æ—¥ã”ã‚ã‹ã‚‰å‹‰å¼·ã—ã¦ã„ã‚‹ã€‚' (fixed version):\n  (æ—¥ã”ã‚, ãƒ’ã‚´ãƒ­, å)\n  (ã‹ã‚‰, ã‚«ãƒ©, åŠ©)\n  (å‹‰å¼·, ãƒ™ãƒ³ã‚­ãƒ§ãƒ¼, å)\n  (ã™ã‚‹, ã‚·, å‹•)\n  (ã¦, ãƒ†, åŠ©)\n  (ã„ã‚‹, ã‚¤ãƒ«, å‹•)\n  (ã€‚, *, è£œ)\n\n\n\n\nShow code\n# A-5: Frequency analysis with BCCWJ matching\ndef analyze_corpus_plan_a(corpus: List[str], tagger, bccwj_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Analyze corpus using Plan A and match with BCCWJ frequencies.\"\"\"\n    freq = Counter()\n    \n    print(f\"Analyzing {len(corpus)} texts with Plan A...\")\n    for text in corpus:\n        for key in iter_lemma_keys_plan_a(text, tagger):\n            freq[key] += 1\n    \n    # Convert to DataFrame\n    rows = []\n    for (lemma, reading, pos), count in freq.items():\n        rows.append((lemma, reading, pos, count))\n    \n    df_local = pd.DataFrame(rows, columns=['lemma', 'reading', 'pos', 'freq_local'])\n    df_local['key'] = list(zip(df_local.lemma, df_local.reading, df_local.pos))\n    \n    # Merge with BCCWJ data\n    merged = df_local.merge(bccwj_df[['key', 'freq_bccwj']], on='key', how='left')\n    \n    return merged.sort_values('freq_local', ascending=False)\n\n# Run Plan A analysis\nresults_a = analyze_corpus_plan_a(extended_corpus, tagger_a, df_bccwj)\nprint(f\"\\nPlan A Results (top 15):\")\nprint(results_a.head(15)[['lemma', 'reading', 'pos', 'freq_local', 'freq_bccwj']])\n\n\nAnalyzing 30 texts with Plan A...\n\nPlan A Results (top 15):\n   lemma reading pos  freq_local  freq_bccwj\n11     ã€‚       *   è£œ          30         NaN\n8      ãŒ       ã‚¬   åŠ©          18         NaN\n1      ã¯       ãƒ¯   åŠ©          15         NaN\n7      ã®       ãƒ   åŠ©          15         NaN\n5      ã‚’       ã‚ª   åŠ©          12         NaN\n10    ã§ã™      ãƒ‡ã‚¹   åŠ©           9         NaN\n42     ã§       ãƒ‡   åŠ©           9         NaN\n15     ã        ãƒ€   åŠ©           6         NaN\n37     ã¦       ãƒ†   åŠ©           6         NaN\n41    é£²ã‚€      ãƒãƒ³   å‹•           6         NaN\n43     ã€       *   è£œ           6         NaN\n48    å›½éš›    ã‚³ã‚¯ã‚µã‚¤   å           3         NaN\n47     ãŸ       ã‚¿   åŠ©           3         NaN\n46   ã—ã¾ã†     ã‚·ãƒãƒƒ   å‹•           3         NaN\n0      å½¼      ã‚«ãƒ¬   ä»£           3         NaN\n\n\n\n\nShow code\n# A-6: Evaluation metrics for Plan A\ndef calculate_metrics(df: pd.DataFrame) -&gt; Dict[str, float]:\n    \"\"\"Calculate coverage and correlation metrics.\"\"\"\n    # Coverage: percentage of local tokens found in BCCWJ\n    matched = df.dropna(subset=['freq_bccwj'])\n    coverage = len(matched) / len(df) * 100\n    \n    # Token coverage (by frequency)\n    total_tokens = df['freq_local'].sum()\n    matched_tokens = matched['freq_local'].sum()\n    token_coverage = matched_tokens / total_tokens * 100\n    \n    # Spearman correlation for matched items\n    if len(matched) &gt; 1:\n        if scipy_available:\n            correlation, p_value = spearmanr(matched['freq_local'], matched['freq_bccwj'])\n        else:\n            correlation = np.corrcoef(matched['freq_local'].rank(), matched['freq_bccwj'].rank())[0,1]\n            p_value = None\n    else:\n        correlation, p_value = None, None\n    \n    return {\n        'type_coverage': coverage,\n        'token_coverage': token_coverage,\n        'correlation': correlation,\n        'p_value': p_value,\n        'total_types': len(df),\n        'matched_types': len(matched),\n        'total_tokens': total_tokens,\n        'matched_tokens': matched_tokens\n    }\n\nmetrics_a = calculate_metrics(results_a)\nprint(\"Plan A Evaluation Metrics:\")\nfor key, value in metrics_a.items():\n    if isinstance(value, float) and value is not None:\n        print(f\"  {key}: {value:.3f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\n\nPlan A Evaluation Metrics:\n  type_coverage: 0.000\n  token_coverage: 0.000\n  correlation: None\n  p_value: None\n  total_types: 66\n  matched_types: 0\n  total_tokens: 297\n  matched_tokens: 0"
  },
  {
    "objectID": "2025/notebooks/session-6.html",
    "href": "2025/notebooks/session-6.html",
    "title": "Session 6 â€” Computing simple lexical diversity and sophistication index",
    "section": "",
    "text": "Show code\nlow_diversity = \"The dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.\"\nShow code\n\nhigh_diversity = \"A curious fox trotted briskly through the meadow, leaping over mossy logs, sniffing wildflowers, and vanishing into golden twilight.\"\nShow code\ndef count_token_type(text: str):\n    # delete punctuation\n    text = text.replace(\".\", \"\")\n    text = text.replace(\",\", \"\")\n    text = text.replace(\"?\", \"\")\n\n    \n    token_list = text.strip()\n    token_list = text.split(\" \")\n\n    token = len(token_list)\n    type = len(set(token_list))\n    return (token, type)\nShow code\ncount_token_type(low_diversity)\n\n\n(19, 8)\nShow code\ncount_token_type(high_diversity)\n\n\n(19, 19)\nShow code\nlow_diversity2 = \"The dog ran. The dog jumped. The dog barked. The dog played. The dog ran quickly. The dog jumped so high. The dog barked very loudly. The dog played, sat, and rolled. The dog sneezed. The dog ate the food.\"\nShow code\ncount_token_type(low_diversity2)\n\n\n(40, 18)\nShow code\nlow_diversity3 = \"The parrot squawked loudly. The parrot chirped again. A toucan perched nearby. The parrot fluttered. Wings flapped softly. The parrot chirped again. Feathers shimmered under sunlight. The crow cawed. The parrot glided low. The air shimmered. The owl blinked slowly. The parrot perched again. The owl blinked slowly. The parrot shrieked. The parrot chirped nearby again. The parrot squawked again.\"\nShow code\ncount_token_type(low_diversity3)\n\n\n(60, 27)"
  },
  {
    "objectID": "2025/notebooks/session-6.html#using-lexical-diversity-package",
    "href": "2025/notebooks/session-6.html#using-lexical-diversity-package",
    "title": "Session 6 â€” Computing simple lexical diversity and sophistication index",
    "section": "Using Lexical Diversity package",
    "text": "Using Lexical Diversity package\nThere is a package called TAALED maintained by Dr.Â Kris Kyle.\n\n\nShow code\nfrom taaled import ld\nfrom pylats import lats"
  },
  {
    "objectID": "2025/sessions/day1/session3.html",
    "href": "2025/sessions/day1/session3.html",
    "title": "Session 3",
    "section": "",
    "text": "You will learn how to conduct basic corpus searches.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#one-liner",
    "href": "2025/sessions/day1/session3.html#one-liner",
    "title": "Session 3",
    "section": "",
    "text": "You will learn how to conduct basic corpus searches.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#learning-objectives",
    "href": "2025/sessions/day1/session3.html#learning-objectives",
    "title": "Session 3",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nConduct KWIC searches on English-Corpora.org\nSort KWIC search results to obtain qualitative observation about language use\nUse advanced search strings such as regular expression to fine-tune the search results",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#key-concepts",
    "href": "2025/sessions/day1/session3.html#key-concepts",
    "title": "Session 3",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nKey Words In Context (KWIC)\nLexical Counting unit:\n\nToken\nLemma\nType\n\nRegular Expressions",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#tools-used",
    "href": "2025/sessions/day1/session3.html#tools-used",
    "title": "Session 3",
    "section": "ğŸ› ï¸ Tools Used",
    "text": "ğŸ› ï¸ Tools Used\n\nEnglish-Corpora.org\nAntConc",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#materials",
    "href": "2025/sessions/day1/session3.html#materials",
    "title": "Session 3",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\n\nView slides in fullscreen",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#reflection",
    "href": "2025/sessions/day1/session3.html#reflection",
    "title": "Session 3",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#learning-objectives",
    "href": "2025/sessions/day1/session1.html#learning-objectives",
    "title": "Session 1",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nOverview the content of the current course\nExplain key success criteria in this course\nConduct the very first corpus search\nExplain different types of corpus linguistic analysis for different focus:\n\nfrequency analysis,\nconcordance analysis,\ncollocation analysis,\nPart-Of-Speech Tagging, etc.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#key-concepts",
    "href": "2025/sessions/day1/session1.html#key-concepts",
    "title": "Session 1",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nConceptual Overview of the corpus linguistic methods\n\nFrequency\nConcordance\nCollocation analysis\nPart-Of-Speech Tagging\nDependency Parsing",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#required-readings",
    "href": "2025/sessions/day1/session1.html#required-readings",
    "title": "Session 1",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\n(Skim) Davies (2015) Available through the shared drive.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#notes",
    "href": "2025/sessions/day1/session1.html#notes",
    "title": "Session 1",
    "section": "ğŸ“ Notes",
    "text": "ğŸ“ Notes\nNeeds analysis (After giving overview) is conducted at the end of this session.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#materials",
    "href": "2025/sessions/day1/session1.html#materials",
    "title": "Session 1",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#reflection",
    "href": "2025/sessions/day1/session1.html#reflection",
    "title": "Session 1",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html",
    "href": "2025/sessions/day5/index.html",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "",
    "text": "Day 5 explores cutting-edge applications of Large Language Models in corpus linguistics and provides dedicated time for final project development.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#overview",
    "href": "2025/sessions/day5/index.html#overview",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "",
    "text": "Day 5 explores cutting-edge applications of Large Language Models in corpus linguistics and provides dedicated time for final project development.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#key-concepts",
    "href": "2025/sessions/day5/index.html#key-concepts",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nLarge Language Models (LLMs) and Language Generation\nPrompt engineering\nFine-tuning\nLLM-assisted linguistic annotation\nResearch design and methodology\nProject presentation skills",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#preparation",
    "href": "2025/sessions/day5/index.html#preparation",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 5:\n\nRead:\n\nMizumoto, A., Shintani, N., Sasaki, M., & Teng, M. F. (2024). Testing the viability of ChatGPT as a companion in L2 writing accuracy assessment. Research Methods in Applied Linguistics, 3(2), 100116.\n\nSkim:\n\nKim, M., & Lu, X. (2024). Exploring the potential of using ChatGPT for rhetorical move-step analysis. Journal of English for Academic Purposes, 71, 101422.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#schedule",
    "href": "2025/sessions/day5/index.html#schedule",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 13: LLMs in Linguistic Analysis\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 14: Group Project Time\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 15: Project Presentations and Wrap-up\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#assignments",
    "href": "2025/sessions/day5/index.html#assignments",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Assignments",
    "text": "Assignments\n\nFinal Project: Final Project Guidelines\nGroup presentations today\nFinal submission deadline: [Check syllabus]",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#reflection",
    "href": "2025/sessions/day5/index.html#reflection",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session14.html",
    "href": "2025/sessions/day5/session14.html",
    "title": "Session 14",
    "section": "",
    "text": "Group project time. Please use the time wisely.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 14"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session14.html#one-liner",
    "href": "2025/sessions/day5/session14.html#one-liner",
    "title": "Session 14",
    "section": "",
    "text": "Group project time. Please use the time wisely.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 14"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session14.html#reflection",
    "href": "2025/sessions/day5/session14.html#reflection",
    "title": "Session 14",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 14"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html",
    "href": "2025/sessions/day2/index.html",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "",
    "text": "Day 2 focuses on analyzing vocabulary in corpus linguistics, introducing concepts of lexical richness, particularly diversity and sophistication.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#overview",
    "href": "2025/sessions/day2/index.html#overview",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "",
    "text": "Day 2 focuses on analyzing vocabulary in corpus linguistics, introducing concepts of lexical richness, particularly diversity and sophistication.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#key-concepts",
    "href": "2025/sessions/day2/index.html#key-concepts",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nLexical Richness (text internal vs external measures)\nLexical Diversity (Type-Token Ratio, MTLD)\nLexical Sophistication (frequency, concreteness, phonological neighbors)\nLexical profiling\nFrequency Lists and Zipf law",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#preparation",
    "href": "2025/sessions/day2/index.html#preparation",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 2:\n\nRead:\n\nDurrant Ch. 3\n\nSkim:\n\nDurrant Ch. 4 (Ignore R codes if you are not familiar)\nEguchi, M., & Kyle, K. (2020). Continuing to Explore the Multidimensional Nature of Lexical Sophistication. The Modern Language Journal, 104(2), 381â€“400.\n\nWatch:\n\nLaurence Anthonyâ€™s intro to AntConc\n\nGetting started (10 mins)\nCorpus manager Basics (18 mins) \nWord list tool basics (7 mins)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#schedule",
    "href": "2025/sessions/day2/index.html#schedule",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 4: Analyzing vocabulary (1) â€” Conceptual overview\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 5: Frequency Analysis and Lexical Profiling\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 6: Computing Lexical Measures\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#assignments",
    "href": "2025/sessions/day2/index.html#assignments",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Assignments",
    "text": "Assignments\n\nDue Tomorrow: Hands-on Assignment 2\nComplete lexical analysis exercises using AntConc and web applications",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#reflection",
    "href": "2025/sessions/day2/index.html#reflection",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#learning-objectives",
    "href": "2025/sessions/day2/session5.html#learning-objectives",
    "title": "Session 5",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, you will be able to:\n\n\nCompute frequency of a single-word lexical item in reference corpora\nDerive vocabulary frequency list using concordancing software (e.g., AntConc)\nApply tokenization on the Japanese language corpus for frequency analysis\nConduct Lexical Profiling using a web-application or desktop application (e.g., AntWordProfiler)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#key-concepts",
    "href": "2025/sessions/day2/session5.html#key-concepts",
    "title": "Session 5",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nLexical profiling\nFrequency Lists\nZipfâ€™s law\nLexical coverage",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#tools-used",
    "href": "2025/sessions/day2/session5.html#tools-used",
    "title": "Session 5",
    "section": "ğŸ› ï¸ Tools Used",
    "text": "ğŸ› ï¸ Tools Used\n\nAntConc\nAntWordProfiler\nNew Word Levels Checker\nLexTutor",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#materials",
    "href": "2025/sessions/day2/session5.html#materials",
    "title": "Session 5",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#reflection",
    "href": "2025/sessions/day2/session5.html#reflection",
    "title": "Session 5",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#frequency-list",
    "href": "2025/sessions/day2/session5.html#frequency-list",
    "title": "Session 5",
    "section": "Frequency list",
    "text": "Frequency list",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#lexical-profiling",
    "href": "2025/sessions/day2/session5.html#lexical-profiling",
    "title": "Session 5",
    "section": "Lexical Profiling",
    "text": "Lexical Profiling",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#keyness-analysis",
    "href": "2025/sessions/day2/session5.html#keyness-analysis",
    "title": "Session 5",
    "section": "Keyness Analysis",
    "text": "Keyness Analysis",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#learning-objectives",
    "href": "2025/sessions/day3/session8.html#learning-objectives",
    "title": "Session 8",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nSearch for window-based collocations and n-grams in AntConc\nCalculate commonly used strengths of association measures by hand using spreadsheet software\nDiscuss benefits and drawbacks of different strength of association measures",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#key-concepts",
    "href": "2025/sessions/day3/session8.html#key-concepts",
    "title": "Session 8",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nn-gram search\nWindow-based collocation search\nStrengths of Association measures â€” T-score, Mutual Information, LogDice",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#required-readings",
    "href": "2025/sessions/day3/session8.html#required-readings",
    "title": "Session 8",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\n(Skim) Durrant (2023) Ch. 8 (Ignore R codes if you are not familiar)",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#materials",
    "href": "2025/sessions/day3/session8.html#materials",
    "title": "Session 8",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#reflection",
    "href": "2025/sessions/day3/session8.html#reflection",
    "title": "Session 8",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now do the followings:\n\nGenerate lists of n-grams using AntConc.\nSearch for collocates with AntConc.\nCalculate major Strengths of Association (SOA) measures by hand.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html",
    "href": "2025/sessions/day3/index.html",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "",
    "text": "Day 3 explores multiword units, collocations, and statistical measures for analyzing word combinations in corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#overview",
    "href": "2025/sessions/day3/index.html#overview",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "",
    "text": "Day 3 explores multiword units, collocations, and statistical measures for analyzing word combinations in corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#key-concepts",
    "href": "2025/sessions/day3/index.html#key-concepts",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nTypes of multiword units (collocation, n-grams, lexical bundles)\nAssociation strengths (t-score, Mutual Information, LogDice)\nContext window vs dependency bigram approaches\nn-gram search and window-based collocation search\nLinear regression analysis for corpus data",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#preparation",
    "href": "2025/sessions/day3/index.html#preparation",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 3:\n\nRead:\n\nDurrant (2023) Ch. 7\nGablasova, D., Brezina, V., & McEnery, T. (2017). Collocations in Corpusâ€Based Language Learning Research. Language Learning, 67(S1), 155â€“179.\n\nSkim:\n\nDurrant (2023) Ch. 8 (Ignore R codes if you are not familiar)\nEguchi & Kyle (2020) - review if needed",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#schedule",
    "href": "2025/sessions/day3/index.html#schedule",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 7: Multiword Units â€” Conceptual Overview\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 8: Hands-on Collocation Analysis\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 9: Learner Corpus Mini-Research\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#assignments",
    "href": "2025/sessions/day3/index.html#assignments",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Assignments",
    "text": "Assignments\n\nDue Tomorrow: Hands-on Assignment 3\nPrepare mini-project research topic and questions for presentation",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#reflection",
    "href": "2025/sessions/day3/index.html#reflection",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#learning-objectives",
    "href": "2025/sessions/day4/session12.html#learning-objectives",
    "title": "Session 12",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nConduct linguistic complexity analysis using a template Python code provided by the instructor.\n(Optional) Apply the concept of linguistic complexity to the Japanese language.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#required-readings",
    "href": "2025/sessions/day4/session12.html#required-readings",
    "title": "Session 12",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nReread Kyle & Crossley (2018) again.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#recommended-readings",
    "href": "2025/sessions/day4/session12.html#recommended-readings",
    "title": "Session 12",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nKyle, K., & Crossley, S. (2017). Assessing syntactic sophistication in L2 writing: A usage-based approach. Language Testing, 34(4), 513â€“535. https://doi.org/10.1177/0265532217712554\nKyle, K., Choe, A. T., Eguchi, M., LaFlair, G., & Ziegler, N. (2021). A Comparison of Spoken and Written Language Use in Traditional and Technologyâ€Mediated Learning Environments. ETS Research Report Series, 2021(1), 1â€“29. https://doi.org/10.1002/ets2.12329",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#tools-used",
    "href": "2025/sessions/day4/session12.html#tools-used",
    "title": "Session 12",
    "section": "ğŸ› ï¸ Tools Used",
    "text": "ğŸ› ï¸ Tools Used\n\nTagAnt\nSimple Text Analyzer: A web app created for you.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#materials",
    "href": "2025/sessions/day4/session12.html#materials",
    "title": "Session 12",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#reflection",
    "href": "2025/sessions/day4/session12.html#reflection",
    "title": "Session 12",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#learning-objectives",
    "href": "2025/sessions/day4/session10.html#learning-objectives",
    "title": "Session 10",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nProvide historical overview of the syntactic complexity research\nDescribe different approaches to grammatical features:\n\nGrammatical complexity strand\nFine-grained grammatical complexity strand\nDescriptive (register-based analysis) strand\nVerb Argument Construction (VAC) strand\n\nUnderstand current trends of syntactic complexity research",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#key-concepts",
    "href": "2025/sessions/day4/session10.html#key-concepts",
    "title": "Session 10",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nGrammatical complexity\nPredictive measures versus Descriptive measures\n\n\nğŸ“š Required Readings\n\nDurrant Ch. 5.\nKyle, K., & Crossley, S. A. (2018). Measuring Syntactic Complexity in L2 Writing Using Fineâ€Grained Clausal and Phrasal Indices. The Modern Language Journal, 102(2), 333â€“349. https://doi.org/10.1111/modl.12468\n\n\n\nRecommended Readings\n\nNorris, J. M., & Ortega, L. (2009). Towards an Organic Approach to Investigating CAF in Instructed SLA: The Case of Complexity. Applied Linguistics, 30(4), 555â€“578. https://doi.org/10.1093/applin/amp044\nBiber, D., Gray, B., Staples, S., & Egbert, J. (2020). Investigating grammatical complexity in L2 English writing research: Linguistic description versus predictive measurement. Journal of English for Academic Purposes, 46, 100869. https://doi.org/10.1016/j.jeap.2020.100869",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#materials",
    "href": "2025/sessions/day4/session10.html#materials",
    "title": "Session 10",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#reflection",
    "href": "2025/sessions/day4/session10.html#reflection",
    "title": "Session 10",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now:\n\nDescribe classic linguistic complexity measures\nDescribe how",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/syllabus/schedule.html",
    "href": "2025/syllabus/schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "This course covers foundational concepts in corpus linguistics, corpus analysis methods, and their research applications across the following four areas: vocabulary, multiword units, and grammar.\n\n\n\nDay\nSession No.\nSession title\n\n\n\n\nDay 1 (Aug.Â 2nd, Sat)\n\nIntroduction to Linguistic Data Analysis\n\n\n\nSession 1\nGetting Started with Corpus Linguistics\n\n\n\nSession 2\nFoundations of Corpus Linguistics\n\n\n\nSession 3\nBasic Corpus Search\n\n\nDay 2 (Aug.Â 4th, Mon)\n\nAnalyzing Vocabulary\n\n\n\nSession 4\nConceptual overview\n\n\n\nSession 5\nFrequency lists & Lexical profiling\n\n\n\nSession 6\nLexical diversity & Sophistication\n\n\nDay 3 (Aug.Â 5th, Tue)\n\nAnalyzing Multiword Units\n\n\n\nSession 7\nConceptual overview\n\n\n\nSession 8\nCollocations & N-grams\n\n\n\nSession 9\nMini-research & Final project overview\n\n\nDay 4 (Aug.Â 6th, Wed)\n\nAnalyzing Grammar\n\n\n\nSession 10\nConceptual overview\n\n\n\nSession 11\nPOS-tagging and Dependency Parsing\n\n\n\nSession 12\nSyntactic Complexity\n\n\nDay 5 (Aug.Â 7th, Thu)\n\nAdvanced Topics & Wrap-up\n\n\n\nSession 13\nUsing large language models for language annotation\n\n\n\nSession 14\nFinal project preparation time\n\n\n\nSession 15\nFinal project presentation",
    "crumbs": [
      "Syllabus",
      "Course Schedule"
    ]
  },
  {
    "objectID": "2025/syllabus/schedule.html#overview",
    "href": "2025/syllabus/schedule.html#overview",
    "title": "Course Schedule",
    "section": "",
    "text": "This course covers foundational concepts in corpus linguistics, corpus analysis methods, and their research applications across the following four areas: vocabulary, multiword units, and grammar.\n\n\n\nDay\nSession No.\nSession title\n\n\n\n\nDay 1 (Aug.Â 2nd, Sat)\n\nIntroduction to Linguistic Data Analysis\n\n\n\nSession 1\nGetting Started with Corpus Linguistics\n\n\n\nSession 2\nFoundations of Corpus Linguistics\n\n\n\nSession 3\nBasic Corpus Search\n\n\nDay 2 (Aug.Â 4th, Mon)\n\nAnalyzing Vocabulary\n\n\n\nSession 4\nConceptual overview\n\n\n\nSession 5\nFrequency lists & Lexical profiling\n\n\n\nSession 6\nLexical diversity & Sophistication\n\n\nDay 3 (Aug.Â 5th, Tue)\n\nAnalyzing Multiword Units\n\n\n\nSession 7\nConceptual overview\n\n\n\nSession 8\nCollocations & N-grams\n\n\n\nSession 9\nMini-research & Final project overview\n\n\nDay 4 (Aug.Â 6th, Wed)\n\nAnalyzing Grammar\n\n\n\nSession 10\nConceptual overview\n\n\n\nSession 11\nPOS-tagging and Dependency Parsing\n\n\n\nSession 12\nSyntactic Complexity\n\n\nDay 5 (Aug.Â 7th, Thu)\n\nAdvanced Topics & Wrap-up\n\n\n\nSession 13\nUsing large language models for language annotation\n\n\n\nSession 14\nFinal project preparation time\n\n\n\nSession 15\nFinal project presentation",
    "crumbs": [
      "Syllabus",
      "Course Schedule"
    ]
  },
  {
    "objectID": "2025/syllabus/schedule.html#important-notes",
    "href": "2025/syllabus/schedule.html#important-notes",
    "title": "Course Schedule",
    "section": "Important Notes",
    "text": "Important Notes\n\nAll times are Japan Standard Time (JST)\nBring your laptop to all sessions\nComplete readings before each day",
    "crumbs": [
      "Syllabus",
      "Course Schedule"
    ]
  },
  {
    "objectID": "2025/slides/session-7.html#learning-objectives",
    "href": "2025/slides/session-7.html#learning-objectives",
    "title": "Session 7: Multiword Units",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nExplain different types of multiword units: collocation, n-grams, lexical bundles\nDemonstrate how major association strengths measures (t-score, Mutual Information, and LogDice) are calculated using examples"
  },
  {
    "objectID": "2025/slides/session-5.html#learning-objectives",
    "href": "2025/slides/session-5.html#learning-objectives",
    "title": "Session 5: Hands-on activity #2",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, you will be able to:\n\n\n\nCompute frequency of a single-word lexical item in reference corpora\nDerive vocabulary frequency list using concordancing software (e.g., AntConc)\nApply tokenization on the Japanese language corpus for frequency analysis\nConduct Lexical Profiling using a web-application or desktop application (e.g., AntWordProfiler)"
  },
  {
    "objectID": "2025/slides/session-5.html#introduction",
    "href": "2025/slides/session-5.html#introduction",
    "title": "Session 5: Hands-on activity #2",
    "section": "Introduction",
    "text": "Introduction\n\nAntConc is free concordancing tool.\nDeveloped by Laurence ANTHONY."
  },
  {
    "objectID": "2025/slides/session-5.html#hands-on-activity",
    "href": "2025/slides/session-5.html#hands-on-activity",
    "title": "Session 5: Hands-on activity #2",
    "section": "Hands-on Activity",
    "text": "Hands-on Activity"
  },
  {
    "objectID": "2025/slides/session-5.html#task-1-loading-a-corpus-to-antconc",
    "href": "2025/slides/session-5.html#task-1-loading-a-corpus-to-antconc",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task 1: Loading a corpus to AntConc",
    "text": "Task 1: Loading a corpus to AntConc"
  },
  {
    "objectID": "2025/slides/session-5.html#open-antconc",
    "href": "2025/slides/session-5.html#open-antconc",
    "title": "Session 5: Hands-on activity #2",
    "section": "Open AntConc",
    "text": "Open AntConc\n\nAntConc"
  },
  {
    "objectID": "2025/slides/session-5.html#antconc-window",
    "href": "2025/slides/session-5.html#antconc-window",
    "title": "Session 5: Hands-on activity #2",
    "section": "AntConc window",
    "text": "AntConc window\n\nAntConc2"
  },
  {
    "objectID": "2025/slides/session-5.html#load-a-corpus",
    "href": "2025/slides/session-5.html#load-a-corpus",
    "title": "Session 5: Hands-on activity #2",
    "section": "Load a corpus",
    "text": "Load a corpus\nNow, letâ€™s load a corpus.\n\nLoad-corpus"
  },
  {
    "objectID": "2025/slides/session-5.html#task-1-creating-a-frequency-list",
    "href": "2025/slides/session-5.html#task-1-creating-a-frequency-list",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task 1: Creating a frequency list",
    "text": "Task 1: Creating a frequency list"
  },
  {
    "objectID": "2025/slides/session-5.html#word",
    "href": "2025/slides/session-5.html#word",
    "title": "Session 5: Hands-on activity #2",
    "section": "Word",
    "text": "Word\nLetâ€™s now create a frequency list\n\nSelect Word analysis option\nSet Min. Freq and Min. Range\n\n\nMin. Freq = the number of times the word should occur in the corpus\nMin. Range = the number of files in which the word should occur\n\n\nHit Start"
  },
  {
    "objectID": "2025/slides/session-5.html#lets-try",
    "href": "2025/slides/session-5.html#lets-try",
    "title": "Session 5: Hands-on activity #2",
    "section": "Letâ€™s try",
    "text": "Letâ€™s try\n\nSet min. frequency = 3; min. range = 3"
  },
  {
    "objectID": "2025/slides/session-5.html#saving-the-frequency-list",
    "href": "2025/slides/session-5.html#saving-the-frequency-list",
    "title": "Session 5: Hands-on activity #2",
    "section": "Saving the frequency list",
    "text": "Saving the frequency list\n\nFrom File hit save the current results\n\n\nsave-list"
  },
  {
    "objectID": "2025/slides/session-5.html#frequency-list",
    "href": "2025/slides/session-5.html#frequency-list",
    "title": "Session 5: Hands-on activity #2",
    "section": "Frequency list",
    "text": "Frequency list\n\nWe will use the BROWN frequency list in the next session.\n\n\nsave-list"
  },
  {
    "objectID": "2025/slides/session-5.html#task-2-plot-frequencies",
    "href": "2025/slides/session-5.html#task-2-plot-frequencies",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task 2: Plot frequencies",
    "text": "Task 2: Plot frequencies\n\nLetâ€™s now understand the distributions of words in language.\nVisit our simple-text-analyzer tool.\nHit Frequency analysis and upload the frequency list.\nWhat did you notice?"
  },
  {
    "objectID": "2025/slides/session-5.html#frequency-plot",
    "href": "2025/slides/session-5.html#frequency-plot",
    "title": "Session 5: Hands-on activity #2",
    "section": "Frequency Plot",
    "text": "Frequency Plot\n\nVery few words occupy most of the corpus.\n\n\nBROWN frequency"
  },
  {
    "objectID": "2025/slides/session-5.html#implication-of-frequency-on-learning-teaching-and-assessment",
    "href": "2025/slides/session-5.html#implication-of-frequency-on-learning-teaching-and-assessment",
    "title": "Session 5: Hands-on activity #2",
    "section": "Implication of frequency on learning, teaching, and assessment",
    "text": "Implication of frequency on learning, teaching, and assessment"
  },
  {
    "objectID": "2025/slides/session-5.html#task-3-lexical-profiling",
    "href": "2025/slides/session-5.html#task-3-lexical-profiling",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task 3: Lexical profiling",
    "text": "Task 3: Lexical profiling\nNow that we understand an important property of language (Zipfâ€™s law), letâ€™s conduct lexical profiling."
  },
  {
    "objectID": "2025/slides/session-5.html#task-5-tokenizing-non-english-languages-for-frequency-analysis",
    "href": "2025/slides/session-5.html#task-5-tokenizing-non-english-languages-for-frequency-analysis",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task 5: Tokenizing non-English languages for frequency analysis",
    "text": "Task 5: Tokenizing non-English languages for frequency analysis\n\nUp to this point, we only dealt with English.\nEnglish is very convenient in corpus analysis because of the white spaces.\nAsian languages have completely different writing system from Indo-European language, and it makes it difficult to tokenize texts in to words.\nI am planning to eat Oysters after this intensive course.\nã“ã®çŸ­æœŸé›†ä¸­è¬›åº§ãŒçµ‚ã‚ã£ãŸã‚‰ã€ã‚«ã‚­ã‚’é£Ÿã¹ãŸã„ã¨æ€ã£ã¦ã„ã¾ã™ã€‚"
  },
  {
    "objectID": "2025/slides/session-5.html#tagant",
    "href": "2025/slides/session-5.html#tagant",
    "title": "Session 5: Hands-on activity #2",
    "section": "TagAnt",
    "text": "TagAnt\n\nTokenization (segmenting running text into words) needs more advanced statistical algorithms.\nTagAnt is a free tool (again developped by Laurence ANTHONY).\nIt uses modern natural language processing tool (called spaCy) to tokenize input texts."
  },
  {
    "objectID": "2025/slides/session-5.html#tokenizing-japanese",
    "href": "2025/slides/session-5.html#tokenizing-japanese",
    "title": "Session 5: Hands-on activity #2",
    "section": "Tokenizing Japanese",
    "text": "Tokenizing Japanese\n\nDownload and open TagAnt.\nCopy and paste a sample text into Input Text.\nSelect language.\nSelect Output format."
  },
  {
    "objectID": "2025/slides/session-5.html#result-of-tagant-segmentation",
    "href": "2025/slides/session-5.html#result-of-tagant-segmentation",
    "title": "Session 5: Hands-on activity #2",
    "section": "Result of TagAnt segmentation",
    "text": "Result of TagAnt segmentation\n\n\n\n\n\nHorizontal display\n\n\n\n\n\n\nVertical display"
  },
  {
    "objectID": "2025/slides/session-5.html#activity-instruction",
    "href": "2025/slides/session-5.html#activity-instruction",
    "title": "Session 5: Hands-on activity #2",
    "section": "Activity instruction",
    "text": "Activity instruction\nTask\nCompile a Japanese frequency list based on corpus.\nResource\n\nDownload a Japanese texts from Google Drive.\nUse AntConc and TagAnt.\n\nSubmission\n\nSubmit a frequency list\nDescription"
  },
  {
    "objectID": "2025/slides/session-5.html#optional-task-4-vocabulary-profiling-through-antwordprofiler",
    "href": "2025/slides/session-5.html#optional-task-4-vocabulary-profiling-through-antwordprofiler",
    "title": "Session 5: Hands-on activity #2",
    "section": "(Optional) Task 4: Vocabulary Profiling through AntWordProfiler",
    "text": "(Optional) Task 4: Vocabulary Profiling through AntWordProfiler"
  },
  {
    "objectID": "2025/slides/session-1.html#instructor",
    "href": "2025/slides/session-1.html#instructor",
    "title": "Session 1: Introduction",
    "section": "Instructor",
    "text": "Instructor"
  },
  {
    "objectID": "2025/slides/session-1.html#session-1-agenda",
    "href": "2025/slides/session-1.html#session-1-agenda",
    "title": "Session 1: Introduction",
    "section": "Session 1 Agenda",
    "text": "Session 1 Agenda\n\nIntroduction to Corpus Linguistics\n\nWhat is a corpus?\nWhy use corpora in linguistics?\nTypes of linguistic corpora\n\nGetting Started\n\nCourse tools and resources\nFirst hands-on activity"
  },
  {
    "objectID": "2025/slides/session-1.html#session-1-agenda-contd",
    "href": "2025/slides/session-1.html#session-1-agenda-contd",
    "title": "Session 1: Introduction",
    "section": "Session 1 Agenda (Contâ€™d)",
    "text": "Session 1 Agenda (Contâ€™d)\n\nCourse Introduction\n\nObjectives and learning outcomes\nCourse structure and expectations"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-linguistics",
    "href": "2025/slides/session-1.html#corpus-linguistics",
    "title": "Session 1: Introduction",
    "section": "Corpus linguistics",
    "text": "Corpus linguistics\nCorpus linguistics = the investigation of linguistic research question that have been framed in terms of the conditional distribution of linguisitc phenomena in a linguistic corpus.\n(Stefanowitsch, 2020, p.Â 56)"
  },
  {
    "objectID": "2025/slides/session-1.html#what-does-it-mean",
    "href": "2025/slides/session-1.html#what-does-it-mean",
    "title": "Session 1: Introduction",
    "section": "What does it mean?",
    "text": "What does it mean?"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-linguistics-1",
    "href": "2025/slides/session-1.html#corpus-linguistics-1",
    "title": "Session 1: Introduction",
    "section": "Corpus linguistics =",
    "text": "Corpus linguistics =\n\nthe investigation of linguistic research question\nthat have been framed in terms of the conditional distribution of linguisitc phenomena\nin a linguistic corpus.\n\n(Stefanowitsch, 2020, p.Â 56)"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus",
    "href": "2025/slides/session-1.html#corpus",
    "title": "Session 1: Introduction",
    "section": "Corpus",
    "text": "Corpus"
  },
  {
    "objectID": "2025/slides/session-1.html#linguistic-corpus",
    "href": "2025/slides/session-1.html#linguistic-corpus",
    "title": "Session 1: Introduction",
    "section": "Linguistic Corpus",
    "text": "Linguistic Corpus\n(Linguistic) Corpus =\n\nâ€œa collection of samples of language useâ€ that is:\n\nauthentic\nrepresentative\nlarge\n\n\n(To be explored more in session 2)"
  },
  {
    "objectID": "2025/slides/session-1.html#what-do-corpora-contain",
    "href": "2025/slides/session-1.html#what-do-corpora-contain",
    "title": "Session 1: Introduction",
    "section": "What do corpora contain?",
    "text": "What do corpora contain?\n\n\nlanguage samples produced in the wild for specific communicative purposes\n\nWritten language:\n\nMagazines\nNews Paper\nBlog\n\nSpoken language:\n\ntranscriptions of spoken exchanges\n\nTV or radio shows\nConversations"
  },
  {
    "objectID": "2025/slides/session-1.html#examples",
    "href": "2025/slides/session-1.html#examples",
    "title": "Session 1: Introduction",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "2025/slides/session-1.html#section",
    "href": "2025/slides/session-1.html#section",
    "title": "Session 1: Introduction",
    "section": "",
    "text": "COCA example"
  },
  {
    "objectID": "2025/slides/session-1.html#section-1",
    "href": "2025/slides/session-1.html#section-1",
    "title": "Session 1: Introduction",
    "section": "",
    "text": "COCA example"
  },
  {
    "objectID": "2025/slides/session-1.html#examples-1",
    "href": "2025/slides/session-1.html#examples-1",
    "title": "Session 1: Introduction",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-of-contemporary-american-coca",
    "href": "2025/slides/session-1.html#corpus-of-contemporary-american-coca",
    "title": "Session 1: Introduction",
    "section": "Corpus Of Contemporary American (COCA)",
    "text": "Corpus Of Contemporary American (COCA)\n\n\n\n\n\n\n\n\n\n\nGenre\n# texts\n# words\nExplanation\n\n\n\n\nSpoken\n44,803\n127,396,932\nTranscripts of unscripted conversation from more than 150 different TV and radio programs (examples: All Things Considered (NPR), Newshour (PBS), Good Morning America (ABC), Oprah)\n\n\nFiction\n25,992\n119,505,305\nShort stories and plays from literary magazines, childrenâ€™s magazines, popular magazines, first chapters of first edition books 1990-present, and fan fiction.\n\n\nMagazines\n86,292\n127,352,030\nNearly 100 different magazines, with a good mix between specific domains like news, health, home and gardening, women, financial, religion, sports, etc.\n\n\nNewspapers\n90,243\n122,958,016\nNewspapers from across the US, including: USA Today, New York Times, Atlanta Journal Constitution, San Francisco Chronicle, etc. Good mix between different sections of the newspaper, such as local news, opinion, sports, financial, etc.\n\n\nAcademic\n26,137\n120,988,361\nMore than 200 different peer-reviewed journals. These cover the full range of academic disciplines, with a good balance among education, social sciences, history, humanities, law, medicine, philosophy/religion, science/technology, and business"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-of-contemporary-american-coca-1",
    "href": "2025/slides/session-1.html#corpus-of-contemporary-american-coca-1",
    "title": "Session 1: Introduction",
    "section": "Corpus of Contemporary American (COCA)",
    "text": "Corpus of Contemporary American (COCA)\n\n\n\n\n\n\n\n\n\n\nGenre\n# texts\n# words\nExplanation\n\n\n\n\nWeb (Genl)\n88,989\n129,899,427\nClassified into the web genres of academic, argument, fiction, info, instruction, legal, news, personal, promotion, review web pages (by Serge Sharoff). Taken from the US portion of the GloWbE corpus.\n\n\nWeb (Blog)\n98,748\n125,496,216\nTexts that were classified by Google as being blogs. Further classified into the web genres of academic, argument, fiction, info, instruction, legal, news, personal, promotion, review web pages. Taken from the US portion of the GloWbE corpus.\n\n\nTV/Movies\n23,975\n129,293,467\nSubtitles from OpenSubtitles.org, and later the TV and Movies corpora. Studies have shown that the language from these shows and movies is even more colloquial / core than the data in actual â€œspoken corporaâ€.\n\n\nTotal\n485,179\n1,002,889,754\n\n\n\n\n\nsee more details"
  },
  {
    "objectID": "2025/slides/session-1.html#examples-2",
    "href": "2025/slides/session-1.html#examples-2",
    "title": "Session 1: Introduction",
    "section": "Examples",
    "text": "Examples\nInsert another example"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-linguistics-2",
    "href": "2025/slides/session-1.html#corpus-linguistics-2",
    "title": "Session 1: Introduction",
    "section": "Corpus linguistics =",
    "text": "Corpus linguistics =\n\nthe investigation of linguistic research question\nthat have been framed in terms of the conditional distribution of linguisitc phenomena\n\n\n\nin a linguistic corpus.\n\n\n(Stefanowitsch, 2020, p.Â 56)"
  },
  {
    "objectID": "2025/slides/session-1.html#the-conditional-distribution-of-linguistic-phenomena",
    "href": "2025/slides/session-1.html#the-conditional-distribution-of-linguistic-phenomena",
    "title": "Session 1: Introduction",
    "section": "The conditional distribution of linguistic phenomena",
    "text": "The conditional distribution of linguistic phenomena"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-linguistics-looks-into-conditional-distribution",
    "href": "2025/slides/session-1.html#corpus-linguistics-looks-into-conditional-distribution",
    "title": "Session 1: Introduction",
    "section": "Corpus linguistics looks into (conditional) distribution",
    "text": "Corpus linguistics looks into (conditional) distribution\n\n(roughly speaking) frequency (= occurrence) of a phenomena\nâ†’ How many times X occurs?\nconditional\nâ†’ How does X occur given Y?"
  },
  {
    "objectID": "2025/slides/session-1.html#examples-3",
    "href": "2025/slides/session-1.html#examples-3",
    "title": "Session 1: Introduction",
    "section": "Examples",
    "text": "Examples\n\nFrequency of Dog"
  },
  {
    "objectID": "2025/slides/session-1.html#more-about-conditional-distribution",
    "href": "2025/slides/session-1.html#more-about-conditional-distribution",
    "title": "Session 1: Introduction",
    "section": "More about conditional distribution",
    "text": "More about conditional distribution\nSegment corpus and calculate occurrences by:\n\ngenres (written vs spoken)\nyear (1970s vs 2000s)\ngeographical region (British vs American)\netc."
  },
  {
    "objectID": "2025/slides/session-1.html#conditional-distribution-of-dog-across-genre-and-year",
    "href": "2025/slides/session-1.html#conditional-distribution-of-dog-across-genre-and-year",
    "title": "Session 1: Introduction",
    "section": "Conditional distribution of Dog across genre and year",
    "text": "Conditional distribution of Dog across genre and year\n\nDog by section"
  },
  {
    "objectID": "2025/slides/session-1.html#linguistic-phenomena",
    "href": "2025/slides/session-1.html#linguistic-phenomena",
    "title": "Session 1: Introduction",
    "section": "Linguistic phenomena",
    "text": "Linguistic phenomena\n\nWe have mostly talked about word.\nHowever, corpus linguists are interested in more than words.\n\nExamples:\n\nMultiword Units\nGrammar\nDiscourse"
  },
  {
    "objectID": "2025/slides/session-1.html#multiword-units-more-on-day-3",
    "href": "2025/slides/session-1.html#multiword-units-more-on-day-3",
    "title": "Session 1: Introduction",
    "section": "Multiword Units (More on Day 3)",
    "text": "Multiword Units (More on Day 3)\nWith corpus methods, we can investigate how two or more words co-occur together.\n\nWhat does the word â€œdogâ€ occur together in English?\nAny guesses?"
  },
  {
    "objectID": "2025/slides/session-1.html#collocates",
    "href": "2025/slides/session-1.html#collocates",
    "title": "Session 1: Introduction",
    "section": "Collocates",
    "text": "Collocates\nIn COCA, the following word co-occur with â€œdogâ€: (Collocates = words that frequently co-occur with the node word.)"
  },
  {
    "objectID": "2025/slides/session-1.html#fill-in-the-blank",
    "href": "2025/slides/session-1.html#fill-in-the-blank",
    "title": "Session 1: Introduction",
    "section": "Fill in the blank",
    "text": "Fill in the blank\n\nâ€œOn the other _____â€\nHow do you know?"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-gives-you-some-ideas",
    "href": "2025/slides/session-1.html#corpus-gives-you-some-ideas",
    "title": "Session 1: Introduction",
    "section": "Corpus gives you some ideas",
    "text": "Corpus gives you some ideas\n\nSimple answer: They are used together so often.\nCorpus answer: Given the sequence â€œon the otherâ€, the probability of seeing the word â€œhandâ€ next is very high. â†’ Their Strengths Of Association (SOA) is high.\n\n\n\nWe will cover how to calculate simple SOA measures on Day 3."
  },
  {
    "objectID": "2025/slides/session-1.html#grammar-more-on-day-4",
    "href": "2025/slides/session-1.html#grammar-more-on-day-4",
    "title": "Session 1: Introduction",
    "section": "Grammar (More on Day 4)",
    "text": "Grammar (More on Day 4)\nWe can do similar with grammar (or morpho-syntax).\n\nHow often do we expect â€œby Xâ€ construction in passive construction."
  },
  {
    "objectID": "2025/slides/session-1.html#frequency-of-by-x-in-passive-construction",
    "href": "2025/slides/session-1.html#frequency-of-by-x-in-passive-construction",
    "title": "Session 1: Introduction",
    "section": "Frequency of â€œby Xâ€ in passive construction",
    "text": "Frequency of â€œby Xâ€ in passive construction"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-linguistics-3",
    "href": "2025/slides/session-1.html#corpus-linguistics-3",
    "title": "Session 1: Introduction",
    "section": "Corpus linguistics =",
    "text": "Corpus linguistics =\n\nthe investigation of linguistic research question\nthat have been framed in terms of the conditional distribution of linguisitc phenomena\nin a linguistic corpus."
  },
  {
    "objectID": "2025/slides/session-1.html#linguistic-research-questions",
    "href": "2025/slides/session-1.html#linguistic-research-questions",
    "title": "Session 1: Introduction",
    "section": "Linguistic Research Questions",
    "text": "Linguistic Research Questions\nExamples:\nBasic corpus research\n\nWhat are relationships between processing speed and language frequency\n\nLearner Corpus Research:\n\nWhat kind of vocabulary do second-language learners with varying proficiency levels produce?"
  },
  {
    "objectID": "2025/slides/session-1.html#course-description",
    "href": "2025/slides/session-1.html#course-description",
    "title": "Session 1: Introduction",
    "section": "Course description",
    "text": "Course description\n\nThis 5-day introduction:\n\ncovers key concepts in corpus linguistics and learner corpus research\nteaches you how to conduct simple corpus searches using Concordance software\ngives you an overview of methods to investigate conditional distributions (e.g., frequency, co-occurrences) of vocabulary, multiword units, and grammatical items.\nintroduces foundational methods to identify linguisitic phenomena using corpus and how to know about their distribution\ndiscusses important applications of corpus methods in applied linguistic (second language) research"
  },
  {
    "objectID": "2025/slides/session-1.html#about-the-course-website",
    "href": "2025/slides/session-1.html#about-the-course-website",
    "title": "Session 1: Introduction",
    "section": "About the course website",
    "text": "About the course website\n\nThe course materials are available through the following two:\n\nCourse website\n\nWe use the website to communicate course schedules, plans, and slides.\n\nGoogle Classroom\n\nWe use Google Classroom for assignment submission, discussion forum, and distributing other materials (e.g., Readings)."
  },
  {
    "objectID": "2025/slides/session-1.html#session-overview",
    "href": "2025/slides/session-1.html#session-overview",
    "title": "Session 1: Introduction",
    "section": "Session overview",
    "text": "Session overview\n\n\n\n\nDay\nTheme\nSessions\n\n\n\n\nDay 1\nIntroduction & Corpus Basics\n\n\n\nDay 2\nAnalysis of Vocabulary & Multiword Units (1)\n\n\n\nDay 3\nAnalysis of Vocabulary & Multiword Units (2)\n\n\n\nDay 4\nAnalysis of Grammar\n\n\n\nDay 5\nAdvanced Topics & Projects"
  },
  {
    "objectID": "2025/slides/session-1.html#daily-schedule",
    "href": "2025/slides/session-1.html#daily-schedule",
    "title": "Session 1: Introduction",
    "section": "Daily schedule",
    "text": "Daily schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 1\n\n\n12:00-13:00\nLunch break\n\n\n13:00-14:30\nSession 2\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 3"
  },
  {
    "objectID": "2025/slides/session-1.html#if-we-have-time",
    "href": "2025/slides/session-1.html#if-we-have-time",
    "title": "Session 1: Introduction",
    "section": "If we have timeâ€¦",
    "text": "If we have timeâ€¦\nMove to the next part."
  },
  {
    "objectID": "2025/slides/session-2.html#learning-objectives",
    "href": "2025/slides/session-2.html#learning-objectives",
    "title": "Session 2: Corpus as a scientific method",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nDefine corpus linguistics as an empirical methodology\nExplain key limitations of introspection in linguistic research\nDescribe the role of frequency data and patterns in corpus analysis\nIdentify and explain the basic steps in corpus-based research\nReflect on their own stance toward data, intuition, and linguistic evidence"
  },
  {
    "objectID": "2025/slides/session-2.html#common-criticism",
    "href": "2025/slides/session-2.html#common-criticism",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Common criticism",
    "text": "Common criticism\n\ncorpora are usage data and thus of no use in studying linguistic knowledge;\n\ncorpora and the data derived from them are necessarily incomplete;\ncorpora contain only linguistic forms (represented as graphemic strings), but no information about the semantics, pragmatics, etc. of these forms; and\n\ncorpora do not contain negative evidence, i.e., they can only tell us what is possible in a given language, but not what is not possible."
  },
  {
    "objectID": "2025/slides/session-12.html#learning-objectives",
    "href": "2025/slides/session-12.html#learning-objectives",
    "title": "Session 12: Hands-on Activity",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nConduct linguistic complexity analysis using a template Python code provided by the instructor.\n(Optional) Apply the concept of linguistic complexity to the Japanese language."
  },
  {
    "objectID": "2025/slides/session-11.html#learning-objectives",
    "href": "2025/slides/session-11.html#learning-objectives",
    "title": "Session 11: Hands-on Activity",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nUnderstand NLP tasks such as POS tagging and dependency parsing\nUnderstand how automated parsing works\nConduct POS tagging using spaCy library in Python (through Google Colab)\nConduct Dependency parsing using spaCy library in Python (through Google Colab)\nConduct multi-lingual Part-Of-Speech (POS) tagging using TagAnt"
  },
  {
    "objectID": "2025/slides/session-8.html#learning-objectives",
    "href": "2025/slides/session-8.html#learning-objectives",
    "title": "Session 8: Hands-on activity #4",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nSearch for window-based collocations and n-grams in AntConc\nCalculate commonly used strengths of association measures by hand using spreadsheet software\nDiscuss benefits and drawbacks of different strength of association measures"
  },
  {
    "objectID": "2025/assignments/hands-on-4/draft.html",
    "href": "2025/assignments/hands-on-4/draft.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/hands-on-3/draft.html",
    "href": "2025/assignments/hands-on-3/draft.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/hands-on-2/draft.html",
    "href": "2025/assignments/hands-on-2/draft.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html",
    "href": "2025/assignments/hands-on-1/index.html",
    "title": "Hands-on Assignment 1",
    "section": "",
    "text": "Under Construction\n\n\n\nThis course website is currently under construction and will be ready for the class starting August 2nd, 2025. Content is being actively developed and updated.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html#task-1-constructing-corpus-search-research-question-and-hypothesis",
    "href": "2025/assignments/hands-on-1/index.html#task-1-constructing-corpus-search-research-question-and-hypothesis",
    "title": "Hands-on Assignment 1",
    "section": "Task 1: Constructing corpus-search research question and hypothesis",
    "text": "Task 1: Constructing corpus-search research question and hypothesis\nIn this task, you are asked to articulate your research question and hypothesis for your first corpus assignment. Use the following information as guideline.\n\nResearch question:\n\nResearch questions should be answerable. Follow the class material to construct a intruiging research question that you can answer with basic corpus search skills you learned.\nWrite one or two research questions you would like to answer through this assignment.\n\n\n\nHypothesis:\n\nOnce you decided on research question, state your hypothesis.\nUse the course material to help you articulate research hypothesis.\nWrite a short paragraph stating your hypothesis and why you think your research hypotheses may be true.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html#task-2-methods",
    "href": "2025/assignments/hands-on-1/index.html#task-2-methods",
    "title": "Hands-on Assignment 1",
    "section": "Task 2: Methods",
    "text": "Task 2: Methods\nIn this task, you are asked to describe the methods choice of your corpus search. This should include justifications for the corpora used, type of corpus search used.\n\nCorpora\n\nWhich corpora or sub-section of a corpus would you conduct a search on? Why? Justify your choice in a paragraph.\n\n\n\nSearch Methods and Plan\n\nWhat corpus search methods would you choose and why? In this assignment, search methods mainly include methods available through English-Corpora.org, including LIST, Chart, Collocates, Compare and KWIC.\nIn what way are you planning to conduct the search and what kind of information are you expecting from it?",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html#task-3-result",
    "href": "2025/assignments/hands-on-1/index.html#task-3-result",
    "title": "Hands-on Assignment 1",
    "section": "Task 3: Result",
    "text": "Task 3: Result\nIn this part of the assignment, you are asked to describe the search results and provide interpretations on the findings.\n\nCorpus search results: Provide some numbers or discourse samples based on your corpus search. This can be frequency list, table with frequency counts, or a copy of KWIC results.\nInterpretation: Write a paragraph, providing some interpretations of your findings.\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\nincludes one or two research questions\narticulates hypotheses\ninclude a list of corpora used\njustifies the selection of corpus\noutlines the search plan (key phrase, regular expression, sorting, filtering, etc.)\nincludes results of your search\nprovides interpretation of the findings.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html",
    "href": "2025/assignments/hands-on-1/draft.html",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "",
    "text": "How does the frequency of word X change over time, and what do the concordance lines reveal about changes in usage?\nHow does the frequency of word X differ across text types Z, and what patterns appear in the KWIC display?\nWhat is the frequency distribution of grammatical pattern X across different text types, and what contexts does KWIC reveal?\nHow has the usage of word X evolved in the past Y years based on frequency trends and concordance evidence?\nWhat are the typical contexts in which expression X appears based on concordance analysis?\nHow do different varieties of language Y (e.g., British vs.Â American English) differ in frequency and contexts of feature X?\nWhat patterns emerge when comparing the frequency and KWIC results for synonyms X and Y?\nHow does register (formal/informal) affect both the frequency and typical contexts of linguistic feature X?\nWhich words frequently appear near word X based on manual analysis of concordance lines?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-change-over-time-and-what-do-the-concordance-lines-reveal-about-changes-in-usage",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-change-over-time-and-what-do-the-concordance-lines-reveal-about-changes-in-usage",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How does the frequency of word X change over time, and what do the concordance lines reveal about changes in usage?",
    "text": "Template: How does the frequency of word X change over time, and what do the concordance lines reveal about changes in usage?\n\nExample Questions:\n\nHow does the frequency of â€œemailâ€ change from 1990-2020, and what do concordance lines reveal about its usage evolution?\nHow has the frequency of â€œglobal warmingâ€ vs â€œclimate changeâ€ shifted over decades, and what contexts show this shift?\nHow does the frequency of â€œshallâ€ change over time in American English, and what do KWIC results show about its declining contexts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-differ-across-text-types-z-and-what-patterns-appear-in-the-kwic-display",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-differ-across-text-types-z-and-what-patterns-appear-in-the-kwic-display",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How does the frequency of word X differ across text types Z, and what patterns appear in the KWIC display?",
    "text": "Template: How does the frequency of word X differ across text types Z, and what patterns appear in the KWIC display?\n\nExample Questions:\n\nHow does the frequency of â€œthereforeâ€ differ between academic and newspaper texts, and what sentence positions does it occupy in each?\nHow does the frequency of contractions (e.g., â€œdonâ€™tâ€) vary between spoken and written corpora, and what patterns emerge in KWIC?\nHow does the frequency of â€œgetâ€ differ across fiction vs.Â academic writing, and what meanings predominate in each genre?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-what-is-the-frequency-distribution-of-grammatical-pattern-x-across-different-text-types-and-what-contexts-does-kwic-reveal",
    "href": "2025/assignments/hands-on-1/draft.html#template-what-is-the-frequency-distribution-of-grammatical-pattern-x-across-different-text-types-and-what-contexts-does-kwic-reveal",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: What is the frequency distribution of grammatical pattern X across different text types, and what contexts does KWIC reveal?",
    "text": "Template: What is the frequency distribution of grammatical pattern X across different text types, and what contexts does KWIC reveal?\n\nExample Questions:\n\nWhat is the frequency of â€œthere is/areâ€ constructions in spoken vs.Â written English, and what follows this pattern in each?\nHow frequent is the â€œnot onlyâ€¦but alsoâ€ construction across different registers, and what types of elements does it connect?\nWhat is the distribution of passive voice (â€œwas/were + past participleâ€) in news vs.Â academic texts, and what verbs commonly appear?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-has-the-usage-of-word-x-evolved-in-the-past-y-years-based-on-frequency-trends-and-concordance-evidence",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-has-the-usage-of-word-x-evolved-in-the-past-y-years-based-on-frequency-trends-and-concordance-evidence",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How has the usage of word X evolved in the past Y years based on frequency trends and concordance evidence?",
    "text": "Template: How has the usage of word X evolved in the past Y years based on frequency trends and concordance evidence?\n\nExample Questions:\n\nHow has the usage of â€œgayâ€ evolved from 1950-2020 based on frequency and changing contexts in concordance lines?\nHow has â€œliterallyâ€ changed in frequency and usage patterns over the past 30 years?\nHow has the word â€œviralâ€ evolved in meaning from 1990-2020 based on concordance evidence?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-what-are-the-typical-contexts-in-which-expression-x-appears-based-on-concordance-analysis",
    "href": "2025/assignments/hands-on-1/draft.html#template-what-are-the-typical-contexts-in-which-expression-x-appears-based-on-concordance-analysis",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: What are the typical contexts in which expression X appears based on concordance analysis?",
    "text": "Template: What are the typical contexts in which expression X appears based on concordance analysis?\n\nExample Questions:\n\nIn what contexts does the phrase â€œat the end of the dayâ€ typically appear, and is it more common in spoken or written English?\nWhat are the typical contexts for â€œon the other handâ€ and what usually precedes it?\nIn what contexts does â€œfrankly speakingâ€ appear, and what types of statements follow it?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-do-different-varieties-of-language-y-differ-in-frequency-and-contexts-of-feature-x",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-do-different-varieties-of-language-y-differ-in-frequency-and-contexts-of-feature-x",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How do different varieties of language Y differ in frequency and contexts of feature X?",
    "text": "Template: How do different varieties of language Y differ in frequency and contexts of feature X?\n\nExample Questions:\n\nHow do British and American English differ in the frequency and contexts of â€œquiteâ€?\nWhat is the frequency difference of â€œshallâ€ between British and American English, and in what contexts does each variety use it?\nHow do British and American English differ in using â€œat the weekendâ€ vs.Â â€œon the weekendâ€?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-what-patterns-emerge-when-comparing-the-frequency-and-kwic-results-for-synonyms-x-and-y",
    "href": "2025/assignments/hands-on-1/draft.html#template-what-patterns-emerge-when-comparing-the-frequency-and-kwic-results-for-synonyms-x-and-y",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: What patterns emerge when comparing the frequency and KWIC results for synonyms X and Y?",
    "text": "Template: What patterns emerge when comparing the frequency and KWIC results for synonyms X and Y?\n\nExample Questions:\n\nWhat patterns emerge when comparing â€œbigâ€ vs.Â â€œlargeâ€ in terms of frequency and the nouns they modify?\nHow do â€œbeginâ€ and â€œstartâ€ differ in frequency and grammatical patterns (begin to/begin -ing)?\nWhat differences appear between â€œbuyâ€ and â€œpurchaseâ€ in frequency across registers and typical objects?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-does-register-affect-both-the-frequency-and-typical-contexts-of-linguistic-feature-x",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-does-register-affect-both-the-frequency-and-typical-contexts-of-linguistic-feature-x",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How does register affect both the frequency and typical contexts of linguistic feature X?",
    "text": "Template: How does register affect both the frequency and typical contexts of linguistic feature X?\n\nExample Questions:\n\nHow does register (academic vs.Â conversational) affect the frequency and contexts of â€œhoweverâ€?\nHow do formal and informal registers differ in the frequency and usage of phrasal verbs like â€œfind outâ€ vs.Â â€œdiscoverâ€?\nHow does register influence the frequency and positioning of â€œmoreoverâ€ and â€œbesidesâ€?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-which-words-frequently-appear-near-word-x-based-on-manual-analysis-of-concordance-lines",
    "href": "2025/assignments/hands-on-1/draft.html#template-which-words-frequently-appear-near-word-x-based-on-manual-analysis-of-concordance-lines",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: Which words frequently appear near word X based on manual analysis of concordance lines?",
    "text": "Template: Which words frequently appear near word X based on manual analysis of concordance lines?\n\nExample Questions:\n\nWhich words frequently appear immediately before and after â€œdecisionâ€ in business English?\nWhat words commonly appear within 3 words of â€œabsolutelyâ€ in spoken English?\nWhich adjectives most frequently appear before â€œconsequencesâ€ in news texts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#social-media-and-internet-language",
    "href": "2025/assignments/hands-on-1/draft.html#social-media-and-internet-language",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Social Media and Internet Language",
    "text": "Social Media and Internet Language\n\nExample Questions:\n\nHow has the frequency of â€œlolâ€ changed from 2000-2020, and in what contexts does it appear beyond informal communication?\nWhat is the frequency difference of â€œselfieâ€ before and after 2010, and what verbs commonly appear with it?\nHow do â€œemojiâ€ and â€œemoticonâ€ differ in frequency over time, and what contexts show their usage patterns?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#pop-culture-and-entertainment",
    "href": "2025/assignments/hands-on-1/draft.html#pop-culture-and-entertainment",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Pop Culture and Entertainment",
    "text": "Pop Culture and Entertainment\n\nExample Questions:\n\nHow does the frequency of â€œbinge-watchâ€ compare to â€œmarathonâ€ (in TV context) and when did each term become popular?\nWhat patterns emerge when comparing â€œK-popâ€ mentions across different text types and time periods?\nHow has â€œanimeâ€ increased in frequency in English corpora, and what words commonly co-occur with it?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#technology-and-gaming",
    "href": "2025/assignments/hands-on-1/draft.html#technology-and-gaming",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Technology and Gaming",
    "text": "Technology and Gaming\n\nExample Questions:\n\nHow do â€œappâ€ and â€œapplicationâ€ differ in frequency across registers, and which one dominates in informal contexts?\nWhat is the frequency evolution of â€œGoogleâ€ as a verb from 2000-2020, and what objects follow it?\nHow has â€œstream/streamingâ€ changed in meaning from 2000-2020 based on concordance contexts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#language-learning-and-education",
    "href": "2025/assignments/hands-on-1/draft.html#language-learning-and-education",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Language Learning and Education",
    "text": "Language Learning and Education\n\nExample Questions:\n\nHow do native speakers use â€œactuallyâ€ vs how it appears in academic writing by non-native speakers?\nWhat is the frequency of â€œI thinkâ€ vs â€œIn my opinionâ€ across spoken and written English?\nHow do apologetic expressions like â€œsorryâ€ and â€œexcuse meâ€ differ in frequency and contexts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#food-and-lifestyle",
    "href": "2025/assignments/hands-on-1/draft.html#food-and-lifestyle",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Food and Lifestyle",
    "text": "Food and Lifestyle\n\nExample Questions:\n\nHow has â€œbubble teaâ€ entered English usage, and what verbs are associated with it?\nWhat is the frequency difference between â€œsushiâ€ in the 1990s vs 2010s, and how have its contexts changed?\nHow do â€œramenâ€ and â€œnoodlesâ€ compare in frequency and what adjectives modify each?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#common-efl-learner-interests",
    "href": "2025/assignments/hands-on-1/draft.html#common-efl-learner-interests",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Common EFL Learner Interests",
    "text": "Common EFL Learner Interests\n\nExample Questions:\n\nHow does â€œdeadlineâ€ appear in different registers, and what verbs commonly precede it (meet/miss/extend)?\nWhat patterns show the difference between â€œtake a testâ€ vs â€œtake an examâ€ in various English varieties?\nHow do â€œpart-time jobâ€ and â€œinternshipâ€ differ in frequency and contexts across text types?"
  },
  {
    "objectID": "2025/assignments/final-project/index.html",
    "href": "2025/assignments/final-project/index.html",
    "title": "Final Project",
    "section": "",
    "text": "For the final assignment of the current 5-day intensive course, I have two possible plans we can discuss and decide together.\nLetâ€™s decide by the end of Day 2.",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/final-project/index.html#project-guidelines",
    "href": "2025/assignments/final-project/index.html#project-guidelines",
    "title": "Final Project",
    "section": "",
    "text": "For the final assignment of the current 5-day intensive course, I have two possible plans we can discuss and decide together.\nLetâ€™s decide by the end of Day 2.",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/final-project/index.html#option-a---conducting-a-separate-mini-project",
    "href": "2025/assignments/final-project/index.html#option-a---conducting-a-separate-mini-project",
    "title": "Final Project",
    "section": "Option A - Conducting a separate mini-project",
    "text": "Option A - Conducting a separate mini-project\n\nOption A is more extensive in that you will be asked to conduct a new mini-project using the toolkit you have learned throughout the course.\nGiven the limited time, however, this plan requires a lot of commitment to the present course and may not be feasible, (but we can try if youâ€™d like!).\nWe will discuss possible alternatives (like Option B below)",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/final-project/index.html#option-b---revisiting-one-of-the-completed-hands-on-assignments",
    "href": "2025/assignments/final-project/index.html#option-b---revisiting-one-of-the-completed-hands-on-assignments",
    "title": "Final Project",
    "section": "Option B - Revisiting one of the completed hands-on assignments",
    "text": "Option B - Revisiting one of the completed hands-on assignments\n\nAs this 5-day intensive course teaches you a lot of new techniques and approaches to analyze lingusitic data, it is important for us to revisit the already completed assignments and consolidate our skills.\nIn Option B, you will be asked to make a presentation on one of your previously completed hands-on assignments, clearly articulating the thinking process as well as potential extension of your approach.\nMore details will be provided on the first day of the course.",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/answer-key.html",
    "href": "2025/assignments/hands-on-1/answer-key.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/index.html",
    "href": "2025/assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "This section contains all course assignments and the final project.",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#overview",
    "href": "2025/assignments/index.html#overview",
    "title": "Assignments",
    "section": "",
    "text": "This section contains all course assignments and the final project.",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#assignments",
    "href": "2025/assignments/index.html#assignments",
    "title": "Assignments",
    "section": "Assignments",
    "text": "Assignments\n\nHands-on Assignment 1\nHands-on Assignment 2\nHands-on Assignment 3\nHands-on Assignment 4\nFinal Project",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#hands-on-activities",
    "href": "2025/assignments/index.html#hands-on-activities",
    "title": "Assignments",
    "section": "Hands-on activities",
    "text": "Hands-on activities\nEach of the four hands-on assignments are evaluated by two criteria: Submission and Quality.\n\n\n\n\n\n\n\n\nGrading Criteria\nDetails\nScore\n\n\n\n\nSubmission on time\nThe first submission is recieved in time.\n3 points\n\n\nQuality\nThe submission meets criteria. You can resubmit your work by resubmission deadlines.\n4 points Ã— 3 questions",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#hands-on-activity-grading-rubrics",
    "href": "2025/assignments/index.html#hands-on-activity-grading-rubrics",
    "title": "Assignments",
    "section": "Hands-on activity grading rubrics",
    "text": "Hands-on activity grading rubrics\n\n\n\n\n\n\n\nScore\nDescriptor\n\n\n\n\n4\n- All the required/essential components are addressed;  - The write-up is complete and easy to follow.\n\n\n3\n- One or two required/essential components are missing;  - The write-up lacks elaboration.\n\n\n2\n- Half of the required/essential components are missing;  - The write-up lacks critical information.\n\n\n1\n- Most of the required/essential components are missing;  - The write-up is outline based",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html",
    "href": "2025/assignments/hands-on-2/index.html",
    "title": "Hands-on Assignment 2",
    "section": "",
    "text": "Under Construction\n\n\n\nThis course website is currently under construction and will be ready for the class starting August 2nd, 2025. Content is being actively developed and updated.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html#task-1-a-japanese-word-frequency-list",
    "href": "2025/assignments/hands-on-2/index.html#task-1-a-japanese-word-frequency-list",
    "title": "Hands-on Assignment 2",
    "section": "Task 1: A Japanese word frequency List",
    "text": "Task 1: A Japanese word frequency List\n\nConstruct a Japanese word frequency list.\nState a purpose for which you are creating a Japanese word frequency list\n\n\n\n\n\n\n\nSubmission:\n\n\n\n\nA Japanese word frequency list (.txt or .tsv format)\nDescriptive paragraphs explaining the frequency distributions of Japanese language.\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\n[ ]",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html#task-2-hand-calculated-lexical-diversity-scores-and-taaled-results",
    "href": "2025/assignments/hands-on-2/index.html#task-2-hand-calculated-lexical-diversity-scores-and-taaled-results",
    "title": "Hands-on Assignment 2",
    "section": "Task 2: Hand-calculated lexical diversity scores and TAALED results",
    "text": "Task 2: Hand-calculated lexical diversity scores and TAALED results\nSubmission:\n\nSpreadsheet file containing hand-calculated lexical diversity scores.\nPlots that contains results of the lexical diversity analysis.\nDescriptive paragraphs explaining the trends of lexical diversity (200-300 words).\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\n[ ]",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html#task-3-qualitative-analysis-of-lexical-sophistication",
    "href": "2025/assignments/hands-on-2/index.html#task-3-qualitative-analysis-of-lexical-sophistication",
    "title": "Hands-on Assignment 2",
    "section": "Task 3: Qualitative analysis of lexical sophistication",
    "text": "Task 3: Qualitative analysis of lexical sophistication\nSubmission:\n\nDescriptive paragraph(s) explaining the index of your choice.\nPlots that contains results of the lexical sophistication analysis.\nDescriptive paragraphs explaining the trends of lexical sophistication (200-300 words).\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\n[ ]",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-3/index.html",
    "href": "2025/assignments/hands-on-3/index.html",
    "title": "Hands-on Assignment 3",
    "section": "",
    "text": "Under Construction\n\n\n\nThis course website is currently under construction and will be ready for the class starting August 2nd, 2025. Content is being actively developed and updated.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 3"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-3/index.html#task-1-comparing-two-lists-of-formulaic-language-4-points",
    "href": "2025/assignments/hands-on-3/index.html#task-1-comparing-two-lists-of-formulaic-language-4-points",
    "title": "Hands-on Assignment 3",
    "section": "Task 1: Comparing two lists of formulaic language (4 points)",
    "text": "Task 1: Comparing two lists of formulaic language (4 points)\nIn the first task, I would like you to select two corpus (or two clearly definable sections of a corpus) and compare and formulaic language that occur in the two (sub-sections of the) corpora.\nResearch questions:\nMethods:\nResults:\n\n\n\n\n\n\nSubmission\n\n\n\n\nA text file (.txt or .tsv) for generated n-grams or collocations\nA word file (.docx) for prose descriptions.\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\n[ ]",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 3"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-3/index.html#task-2-3-mini-research-project-8-points-altogether",
    "href": "2025/assignments/hands-on-3/index.html#task-2-3-mini-research-project-8-points-altogether",
    "title": "Hands-on Assignment 3",
    "section": "Task 2 & 3: Mini-research project (8 points altogether)",
    "text": "Task 2 & 3: Mini-research project (8 points altogether)\nThe task 2 and 3 are related to the mini-research project.\nIn this part of the assignment, you will conduct a mini-research project to describe uses of single- and multi-word units in a corpus you choose.\nSpecifically, you will:\n\nselect lexical richness or phraseological sophistication indices to answer a set of research questions\nanalyze the chosen corpus with the selected indices\npresent the results and interpretation in a written prose\n\n\nCorpus choice\nIn this assignment, please choose one of the following corpora:\n\nGrowth in Grammar (GiG) corpus (Durrant)\nBritish Academic Written English\nSome Japanese corpus here.\n\nResearch questions:\nMethods:\nResults:\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\n[ ]",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 3"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html",
    "href": "2025/assignments/hands-on-4/index.html",
    "title": "Hands-on Assignment 4",
    "section": "",
    "text": "Under Construction\n\n\n\nThis course website is currently under construction and will be ready for the class starting August 2nd, 2025. Content is being actively developed and updated.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 4"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html#task-1-research-questions-hypotheses-and-methods",
    "href": "2025/assignments/hands-on-4/index.html#task-1-research-questions-hypotheses-and-methods",
    "title": "Hands-on Assignment 4",
    "section": "Task 1: Research questions, Hypotheses and Methods",
    "text": "Task 1: Research questions, Hypotheses and Methods\nIn this task you will describe research questions, hypothesis, and methods.\n\nResearch questions\n\nResearch questions should include:\n\ntype of features you are looking at (e.g., adverbial clauses)\nfactors that defines your sub-corpora (e.g., grade)\n\n\n\n\nHypothesis\n\nYour research hypothesis should:\n\ndescribe your predictions in terms of:\n\nquantitative trends of the feature in relation to the factor you are interested in.\n\n\n\n\n\nDefinitions of grammatical features to extract\n\nYou must describe the specific grammatical features that you plan to extract.\nFor example, for clausal features you need to specify if:\n\nyou are interested in subordinate clauses or embedded clauses\nyou are interested in particular type of clauses",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 4"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html#task-2-fine-grained-descriptive-grammatical-features",
    "href": "2025/assignments/hands-on-4/index.html#task-2-fine-grained-descriptive-grammatical-features",
    "title": "Hands-on Assignment 4",
    "section": "Task 2: Fine-grained Descriptive grammatical features",
    "text": "Task 2: Fine-grained Descriptive grammatical features\n\nOnce you articulated the information above, you will now conduct a search over the corpus.\nYou should use either simple text analyzer or your own Colab Notebook.\n\nI will specify which option should be used by the time we start working on this assignment (that is, this depends on your progress as a group.)\n\nDescription of rules to identify desirable linguistic feature.\n\nFor example, you will need to specify amod for dependency label to extract adjective + noun phrase.",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 4"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html#task-3-results-and-interpretation",
    "href": "2025/assignments/hands-on-4/index.html#task-3-results-and-interpretation",
    "title": "Hands-on Assignment 4",
    "section": "Task 3: Results and interpretation",
    "text": "Task 3: Results and interpretation\n\nProvide the results of your corpus analysis in a way you think is most effective to address your research questions. Make effective use of tables, plots, or other data presentation technique as you think.\nProvide descriptive paragraphs to walk the reader through the results and how to interprete that results.\n\n\n\n\n\n\n\nSubmission\n\n\n\n\nA word file (.docx file) that addresses requirements in a written format (one or two pages depending on your analysis results.).\n\nScreenshots of your search settings on the simple text analyzer tool.\n\nIF you use colab, Google Colab notebook (.ipynb file) with extraction code and results.\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission â€¦\n\noutlines research questions and hypotheses\nprovide description of your approach (algorithms and rules) to identify the desired linguistic structure.\nprovides analysis results and their interpretations in relation to the research questions",
    "crumbs": [
      "Assignments",
      "Hands-on Assignment 4"
    ]
  },
  {
    "objectID": "2025/slides/session-9.html#learning-objectives",
    "href": "2025/slides/session-9.html#learning-objectives",
    "title": "Session 9: Research Application",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nJustify choices of lexical richness measures to investigate a research questions\nConduct a simple statistical analysis of selected corpus on small sets of lexical measures using JASP software"
  },
  {
    "objectID": "2025/slides/session-10.html#learning-objectives",
    "href": "2025/slides/session-10.html#learning-objectives",
    "title": "Session 10: Analyzing Grammar",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nProvide historical overview of the syntactic complexity research\nDescribe different approaches to grammatical features:\n\nGrammatical complexity strand\nFine-grained grammatical complexity strand\nDescriptive (register-based analysis) strand\nVerb Argument Construction (VAC) strand\n\nUnderstand current trends of syntactic complexity research"
  },
  {
    "objectID": "2025/slides/session-13.html#learning-objectives",
    "href": "2025/slides/session-13.html#learning-objectives",
    "title": "Session 12: Hands-on Activity",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nDescribe how LLMs are trained generally and what LLMs do to produce language.\nExplain the benefits and drawbacks of using LLMs for linguistic annotation.\nDemonstrate/discuss potential impacts of prompts on the LLMs performance on linguistic annotation.\nDesign an experiment to investigate LLMs output accuracy on a given annotation task."
  },
  {
    "objectID": "2025/slides/session-3.html#learning-objectives",
    "href": "2025/slides/session-3.html#learning-objectives",
    "title": "Session 3: Hands-on #1",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nConduct KWIC searches on English-Corpora.org\nSort KWIC search results to obtain qualitative observation about language use\nUse advanced search strings such as regular expression to fine-tune the search results"
  },
  {
    "objectID": "2025/slides/session-3.html#task-a---simple-word-search",
    "href": "2025/slides/session-3.html#task-a---simple-word-search",
    "title": "Session 3: Hands-on #1",
    "section": "Task A - Simple word search",
    "text": "Task A - Simple word search"
  },
  {
    "objectID": "2025/slides/session-3.html#first-corpus-search",
    "href": "2025/slides/session-3.html#first-corpus-search",
    "title": "Session 3: Hands-on #1",
    "section": "First corpus search",
    "text": "First corpus search\n\nLetâ€™s start our journey.\nFirst of all, letâ€™s search the following word in COCA.\n\nrun"
  },
  {
    "objectID": "2025/slides/session-3.html#result-of-dog",
    "href": "2025/slides/session-3.html#result-of-dog",
    "title": "Session 3: Hands-on #1",
    "section": "Result of dog",
    "text": "Result of dog\n\nYou might get slightly different results."
  },
  {
    "objectID": "2025/slides/session-3.html#result-of-run",
    "href": "2025/slides/session-3.html#result-of-run",
    "title": "Session 3: Hands-on #1",
    "section": "Result of run",
    "text": "Result of run\n\nSearch - run\nYou might get slightly different results."
  },
  {
    "objectID": "2025/slides/session-3.html#lemma-search",
    "href": "2025/slides/session-3.html#lemma-search",
    "title": "Session 3: Hands-on #1",
    "section": "LEMMA search",
    "text": "LEMMA search\n\nLemma is a group of word form for the head-word with grammatical inflection.\nYou can search LEMMA in English-Corpora.org through Capital letters.\nThe search methods will depend on the corpus tool you use."
  },
  {
    "objectID": "2025/slides/session-3.html#lemma-search-1",
    "href": "2025/slides/session-3.html#lemma-search-1",
    "title": "Session 3: Hands-on #1",
    "section": "LEMMA search",
    "text": "LEMMA search\n\nSearch - consider"
  },
  {
    "objectID": "2025/slides/session-3.html#concordances-or-kwic",
    "href": "2025/slides/session-3.html#concordances-or-kwic",
    "title": "Session 3: Hands-on #1",
    "section": "Concordances (or KWIC)",
    "text": "Concordances (or KWIC)\n\nNow you know how many times XX occurs in COCA, you want to see the context in which XX occur.\nThis goal can be accomplished with KWIC search."
  },
  {
    "objectID": "2025/slides/session-3.html#kwic-view",
    "href": "2025/slides/session-3.html#kwic-view",
    "title": "Session 3: Hands-on #1",
    "section": "KWIC view",
    "text": "KWIC view\n\nGo back to SEARCH window.\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#kwic-view-1",
    "href": "2025/slides/session-3.html#kwic-view-1",
    "title": "Session 3: Hands-on #1",
    "section": "KWIC view",
    "text": "KWIC view\n\nClick on the + button in the search menu and enter your search word.\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#kwic-view-2",
    "href": "2025/slides/session-3.html#kwic-view-2",
    "title": "Session 3: Hands-on #1",
    "section": "KWIC view",
    "text": "KWIC view\n\nThe result is displayed. Default sort : R1 &gt; R2 &gt; R3\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#sorting-kwic-window",
    "href": "2025/slides/session-3.html#sorting-kwic-window",
    "title": "Session 3: Hands-on #1",
    "section": "Sorting KWIC window",
    "text": "Sorting KWIC window\n\nNow letâ€™s sort the results according to the position in context.\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#sorting-words",
    "href": "2025/slides/session-3.html#sorting-words",
    "title": "Session 3: Hands-on #1",
    "section": "Sorting words",
    "text": "Sorting words\nYou can sort the words.\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#lets-try-kwic",
    "href": "2025/slides/session-3.html#lets-try-kwic",
    "title": "Session 3: Hands-on #1",
    "section": "Letâ€™s Try: KWIC",
    "text": "Letâ€™s Try: KWIC\n\nChoose a word that you want to see the context for.\nSearch the word with KWIC.\nSort the word in the following way.\n\nDefault: R1 &gt; R2 &gt; R3\nCustom 1: L1 &gt; L2 &gt; L3\nCustom 2: L1 &gt; R1 &gt; R2"
  },
  {
    "objectID": "2025/slides/session-3.html#how-to-get-custom-1-and-2",
    "href": "2025/slides/session-3.html#how-to-get-custom-1-and-2",
    "title": "Session 3: Hands-on #1",
    "section": "How to get Custom 1 and 2",
    "text": "How to get Custom 1 and 2\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#chart-function",
    "href": "2025/slides/session-3.html#chart-function",
    "title": "Session 3: Hands-on #1",
    "section": "Chart function",
    "text": "Chart function\n\nNow that we learned how to conduct concordance search, letâ€™s experiment with some CHART function.\nThis allows you to return frequency by sections of of COCA (= conditional frequency)."
  },
  {
    "objectID": "2025/slides/session-3.html#search-radio-using-chart",
    "href": "2025/slides/session-3.html#search-radio-using-chart",
    "title": "Session 3: Hands-on #1",
    "section": "Search radio using CHART",
    "text": "Search radio using CHART\n\nGo to Search Tab, select, CHART and enter radio\n\n\nChart search"
  },
  {
    "objectID": "2025/slides/session-3.html#search-radio-using-chart-1",
    "href": "2025/slides/session-3.html#search-radio-using-chart-1",
    "title": "Session 3: Hands-on #1",
    "section": "Search radio using CHART",
    "text": "Search radio using CHART\n\nYou will get the following:"
  },
  {
    "objectID": "2025/slides/session-3.html#regular-expressions",
    "href": "2025/slides/session-3.html#regular-expressions",
    "title": "Session 3: Hands-on #1",
    "section": "Regular expressions",
    "text": "Regular expressions\nRegular expression (æ­£è¦è¡¨ç¾) allows you to search corpus through â€œpattern matchingâ€.\n\nHave you ever used (*) asterisk in your web search?\nThis is one of the regular expression (= wild card)"
  },
  {
    "objectID": "2025/slides/session-3.html#using-wild-card-in-search",
    "href": "2025/slides/session-3.html#using-wild-card-in-search",
    "title": "Session 3: Hands-on #1",
    "section": "Using wild card in search",
    "text": "Using wild card in search\n\nLetâ€™s go back to List search.\nEnter â€œa * of theâ€\nWhat result do you expect with this search?\nDonâ€™t turn to the next page YET!!!"
  },
  {
    "objectID": "2025/slides/session-3.html#result-with-a-of-the",
    "href": "2025/slides/session-3.html#result-with-a-of-the",
    "title": "Session 3: Hands-on #1",
    "section": "Result with a * of the",
    "text": "Result with a * of the"
  },
  {
    "objectID": "2025/slides/session-3.html#more-search-methods",
    "href": "2025/slides/session-3.html#more-search-methods",
    "title": "Session 3: Hands-on #1",
    "section": "More search methods",
    "text": "More search methods\n\nYou can check how to use other search methods in English-Corpora.org here"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search",
    "href": "2025/slides/session-3.html#collocation-search",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nThis is main topic for Day 3.\nBriefly, collocates search allows us to search for co-occurring words within specified window."
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-1",
    "href": "2025/slides/session-3.html#collocation-search-1",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nSelect Collocates"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-2",
    "href": "2025/slides/session-3.html#collocation-search-2",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nEnter search word"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-3",
    "href": "2025/slides/session-3.html#collocation-search-3",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nOptional enter the word to look for"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-4",
    "href": "2025/slides/session-3.html#collocation-search-4",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nSpecify window\n\n\nHow distant do you search for the collocates"
  },
  {
    "objectID": "2025/slides/session-4.html#learning-objectives",
    "href": "2025/slides/session-4.html#learning-objectives",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nExplain the purposes of linguistic measures\nList commonly used lexical measures in second language acquisition research\nExplain sub-constructs of lexical richness measures\n\nLexical Diversity\nLexical Sophistication"
  },
  {
    "objectID": "2025/slides/coca_table.html",
    "href": "2025/slides/coca_table.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenre\n# texts\n# words\nExplanation\n\n\n\n\nSpoken\n44,803\n127,396,932\nTranscripts of unscripted conversation from more than 150 different TV and radio programs (examples: All Things Considered (NPR), Newshour (PBS), Good Morning America (ABC), Oprah)\n\n\nFiction\n25,992\n119,505,305\nShort stories and plays from literary magazines, childrenâ€™s magazines, popular magazines, first chapters of first edition books 1990-present, and fan fiction.\n\n\nMagazines\n86,292\n127,352,030\nNearly 100 different magazines, with a good mix between specific domains like news, health, home and gardening, women, financial, religion, sports, etc.\n\n\nNewspapers\n90,243\n122,958,016\nNewspapers from across the US, including: USA Today, New York Times, Atlanta Journal Constitution, San Francisco Chronicle, etc. Good mix between different sections of the newspaper, such as local news, opinion, sports, financial, etc.\n\n\nAcademic\n26,137\n120,988,361\nMore than 200 different peer-reviewed journals. These cover the full range of academic disciplines, with a good balance among education, social sciences, history, humanities, law, medicine, philosophy/religion, science/technology, and business\n\n\nWeb (Genl)\n88,989\n129,899,427\nClassified into the web genres of academic, argument, fiction, info, instruction, legal, news, personal, promotion, review web pages (by Serge Sharoff). Taken from the US portion of the GloWbE corpus.\n\n\nWeb (Blog)\n98,748\n125,496,216\nTexts that were classified by Google as being blogs. Further classified into the web genres of academic, argument, fiction, info, instruction, legal, news, personal, promotion, review web pages. Taken from the US portion of the GloWbE corpus.\n\n\nTV/Movies\n23,975\n129,293,467\nSubtitles from OpenSubtitles.org, and later the TV and Movies corpora. Studies have shown that the language from these shows and movies is even more colloquial / core than the data in actual â€œspoken corporaâ€.\n\n\nTotal\n485,179\n1,002,889,754"
  },
  {
    "objectID": "2025/slides/session-6.html#learning-objectives",
    "href": "2025/slides/session-6.html#learning-objectives",
    "title": "Session 6: Hands-on activity #3",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, you will be able to:\n\n\n\nCompute simple lexical diversity measures using spreadsheet software\nCompute advanced lexical diversity measures using TAALED\nExplain how modern lexical diversity measures are calculated\nCalculate simple lexical sophistication measures using dedicated web application\nDescribe how lexical sophistication measures behave on a single input text.\nDiscuss benefits and drawbacks of lexical richness measures."
  },
  {
    "objectID": "2025/slides/session-6.html#terminology",
    "href": "2025/slides/session-6.html#terminology",
    "title": "Session 6: Hands-on activity #3",
    "section": "Terminology",
    "text": "Terminology\nIn this presentation, I will use the following terms:\n\nInput text: The text you want to analyze (e.g., learner produce text)."
  },
  {
    "objectID": "2025/slides/session-6.html#lexical-diversity",
    "href": "2025/slides/session-6.html#lexical-diversity",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lexical Diversity",
    "text": "Lexical Diversity\n\nLexical Diversity is computed internally to text.\ne.g., Type-Token Ratio:\n\nCount the number of unique word (Type) in the input text\nCount the number of total word (Token) in the input text\nDevide Type by token."
  },
  {
    "objectID": "2025/slides/session-6.html#lexical-sophistication",
    "href": "2025/slides/session-6.html#lexical-sophistication",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lexical Sophistication",
    "text": "Lexical Sophistication\n\nLexical Sophistication requires external resources to compute.\nTo derive a frequency index:\n\nCompile a frequency list\nFor each word in the input text, retrieve frequency score from the list\n\ne.g., tree â€“&gt; 1,0000\n\nAverage the frequency scores (out of item awarded the score)"
  },
  {
    "objectID": "2025/slides/session-6.html#computing-simple-lexical-diversity-measures-by-hand",
    "href": "2025/slides/session-6.html#computing-simple-lexical-diversity-measures-by-hand",
    "title": "Session 6: Hands-on activity #3",
    "section": "Computing simple Lexical Diversity measures by hand",
    "text": "Computing simple Lexical Diversity measures by hand"
  },
  {
    "objectID": "2025/slides/session-6.html#simple-text-example",
    "href": "2025/slides/session-6.html#simple-text-example",
    "title": "Session 6: Hands-on activity #3",
    "section": "Simple Text Example",
    "text": "Simple Text Example\nCound the type and token of the following texts.\n\n\n\n\n\n\n\nID\nText\n\n\n\n\nText 1\nâ€œThe dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.â€\n\n\nText 2\nâ€œA curious fox trotted briskly through the meadow, leaping over mossy logs, sniffing wildflowers, and vanishing into golden twilight.â€\n\n\n\nNote : Texts were generated by GPT for illustration purposes."
  },
  {
    "objectID": "2025/slides/session-6.html#simple-text-example-1",
    "href": "2025/slides/session-6.html#simple-text-example-1",
    "title": "Session 6: Hands-on activity #3",
    "section": "Simple Text Example",
    "text": "Simple Text Example\n\n\n\n\n\n\n\n\n\nID\nText\nToken\nType\n\n\n\n\nText 1\nâ€œThe dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.â€\n19\n8\n\n\nText 2\nâ€œA curious fox trotted briskly through the meadow, leaping over mossy logs, sniffing wildflowers, and vanishing into golden twilight.â€\n19\n19\n\n\n\nNote : Texts were generated by GPT for illustration purposes."
  },
  {
    "objectID": "2025/slides/session-6.html#impact-of-text-lengths",
    "href": "2025/slides/session-6.html#impact-of-text-lengths",
    "title": "Session 6: Hands-on activity #3",
    "section": "Impact of Text lengths",
    "text": "Impact of Text lengths\n\n\n\n\n\n\n\n\n\n\nID\nText\nToken\nType\n\n\n\n\nText 1a\nâ€œThe dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.â€\n19\n8\n\n\nText 1b\nâ€œThe dog ran. The dog jumped. The dog barked. The dog played. The dog ran quickly. The dog jumped so high. The dog barked very loudly. The dog played, sat, and rolled. The dog sneezed. The dog ate the food.â€\n40\n18\n\n\nText 1c\nâ€œThe parrot squawked loudly. The parrot chirped again. A toucan perched nearby. The parrot fluttered. Wings flapped softly. The parrot chirped again. Feathers shimmered under sunlight. The crow cawed. The parrot glided low. The air shimmered. The owl blinked slowly. The parrot perched again. The owl blinked slowly. The parrot shrieked. The parrot chirped nearby again. The parrot squawked again.â€\n60\n27\n\n\n\nNote : Texts were generated by GPT for illustration purposes."
  },
  {
    "objectID": "2025/slides/session-6.html#lets-calculate-some-classical-lexical-diversity-indices",
    "href": "2025/slides/session-6.html#lets-calculate-some-classical-lexical-diversity-indices",
    "title": "Session 6: Hands-on activity #3",
    "section": "Letâ€™s calculate some classical Lexical Diversity indices",
    "text": "Letâ€™s calculate some classical Lexical Diversity indices\n\nOpen Google Spreadsheet\nCalculate the lexical diversity indices on the next page."
  },
  {
    "objectID": "2025/slides/session-6.html#some-classic-lexical-diversity",
    "href": "2025/slides/session-6.html#some-classic-lexical-diversity",
    "title": "Session 6: Hands-on activity #3",
    "section": "Some classic lexical diversity",
    "text": "Some classic lexical diversity\nWe calculate this for illustration but NEVER use these in your study.\n\n\\(TTR = {nType \\over nToken}\\)\n\\(RootTTR = {nType \\over \\sqrt{nToken}}\\)\n\\(LogTTR = {\\log(nType) \\over \\log(nToken)}\\)\n\\(Maas = {\\log(nTokens) - \\log(nTypes) \\over \\log(nToken)^2}\\)"
  },
  {
    "objectID": "2025/slides/session-6.html#what-should-we-actually-use-then",
    "href": "2025/slides/session-6.html#what-should-we-actually-use-then",
    "title": "Session 6: Hands-on activity #3",
    "section": "What should we actually use then?",
    "text": "What should we actually use then?\n\nThe Measure of Textual Lexical Diversity (MTLD)\nMoving-Average Type Token Ratio (MATTR)\n\nâ†’ These are shown as more robust indices of LD."
  },
  {
    "objectID": "2025/slides/session-6.html#using-taaled-desktop-version",
    "href": "2025/slides/session-6.html#using-taaled-desktop-version",
    "title": "Session 6: Hands-on activity #3",
    "section": "Using TAALED desktop version",
    "text": "Using TAALED desktop version\n\nWe can use Tool for the Automatic Analysis of Lexical Diversity (TAALED)\nDownload it to your computer and we will use it to compute modern LD measures\n\n\nTAALED"
  },
  {
    "objectID": "2025/slides/session-6.html#setting-up",
    "href": "2025/slides/session-6.html#setting-up",
    "title": "Session 6: Hands-on activity #3",
    "section": "Setting up",
    "text": "Setting up\n\nClick the software icon after download\nFor mac users, the system will issue warning, you must follow the following step:\n\nGo to setting and select Privacy & Security\nIf you have already attempted to open the software, there will be Open Anyway button.\nClick Open Anyway and that will allow Mac to open the software."
  },
  {
    "objectID": "2025/slides/session-6.html#selecting-indics",
    "href": "2025/slides/session-6.html#selecting-indics",
    "title": "Session 6: Hands-on activity #3",
    "section": "Selecting indics",
    "text": "Selecting indics\nYou can then wait for the TAALED app to start up.\n\nTAALED"
  },
  {
    "objectID": "2025/slides/session-6.html#options",
    "href": "2025/slides/session-6.html#options",
    "title": "Session 6: Hands-on activity #3",
    "section": "Options",
    "text": "Options\nWord analysis options\n\nAll words: Conduct analysis including all words.\nContent words: Conduct analysis with content words only.\nFunction words: Conduct analysis with function words only."
  },
  {
    "objectID": "2025/slides/session-6.html#options-1",
    "href": "2025/slides/session-6.html#options-1",
    "title": "Session 6: Hands-on activity #3",
    "section": "Options",
    "text": "Options\nIndex selection\nSelect the indices you need in the results. Three variants of MTLD are available.\n\nMTLD Original:\nMTLD MA Bi: Moving Average Bidirectional\nMTLD MA Wrap: If there is words left in the final factor, come back to the first part and complete the analysis."
  },
  {
    "objectID": "2025/slides/session-6.html#options-2",
    "href": "2025/slides/session-6.html#options-2",
    "title": "Session 6: Hands-on activity #3",
    "section": "Options",
    "text": "Options\nInput and output options\n\nYou can choose the input folder by selection\n\nOutput option\n\nTicking the Individual Item Output button allows you to have POS analysis\n\nparent_cw_nn\nand_fw\nteacher_cw_nn\ndisagree_cw_vb\nthat_fw"
  },
  {
    "objectID": "2025/slides/session-6.html#run-the-analysis",
    "href": "2025/slides/session-6.html#run-the-analysis",
    "title": "Session 6: Hands-on activity #3",
    "section": "Run the analysis",
    "text": "Run the analysis\n\nPress Process Texts and wait the following display.\n\n\nAnalysis complete"
  },
  {
    "objectID": "2025/slides/session-6.html#lets-take-a-look-at-the-csv-file",
    "href": "2025/slides/session-6.html#lets-take-a-look-at-the-csv-file",
    "title": "Session 6: Hands-on activity #3",
    "section": "Letâ€™s take a look at the csv file",
    "text": "Letâ€™s take a look at the csv file\n\nWhatâ€™s CSV?\n\nCSV (Comma Separated Values) file is a file extension like others (txt, docx).\nIt allows table like representation of data (like excel) separated by comma.\n\n\nThe Raw data (if you open it with text editor) should look like the following:\nfilename,basic_ntokens,basic_ntypes,basic_ncontent_tokens,basic_ncontent_types,basic_nfunction_tokens,basic_nfunction_types,lexical_density_types,lexical_density_tokens,maas_ttr_aw,mattr50_aw,hdd42_aw,mtld_original_aw,mtld_ma_bi_aw,mtld_ma_wrap_aw\nW_CHN_PTJ0_004_B1_2_ORIG.txt,267,124,135,75,132,49,0.6048387096774194,0.5056179775280899,0.056571333957257205,0.7793577981651377,0.7981161136859327,68.68659119235562,65.84622666144406,61.50561797752809\nW_JPN_SMK0_015_B1_2_ORIG.txt,302,138,129,82,173,56,0.5942028985507246,0.4271523178807947,0.05530143602594381,0.777865612648221,0.7974803670481457,68.38677597714803,67.6645170484911,65.22185430463576"
  },
  {
    "objectID": "2025/slides/session-6.html#opening-csv-file-in-a-spreadsheet-software",
    "href": "2025/slides/session-6.html#opening-csv-file-in-a-spreadsheet-software",
    "title": "Session 6: Hands-on activity #3",
    "section": "Opening csv file in a spreadsheet software",
    "text": "Opening csv file in a spreadsheet software\n\nYou can open csv file in Excel (or any other spreadsheet software)"
  },
  {
    "objectID": "2025/slides/session-6.html#questions",
    "href": "2025/slides/session-6.html#questions",
    "title": "Session 6: Hands-on activity #3",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "2025/slides/session-6.html#lemmatize",
    "href": "2025/slides/session-6.html#lemmatize",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lemmatize?",
    "text": "Lemmatize?\n\nWhat do you think is the effect of lemmatization on the lexical diversity measures?\nShould we lemmatize? Why or why not?"
  },
  {
    "objectID": "2025/slides/session-6.html#computing-simple-lexical-sophisitcation-measures",
    "href": "2025/slides/session-6.html#computing-simple-lexical-sophisitcation-measures",
    "title": "Session 6: Hands-on activity #3",
    "section": "Computing simple Lexical Sophisitcation measures",
    "text": "Computing simple Lexical Sophisitcation measures"
  },
  {
    "objectID": "2025/slides/session-6.html#lexical-sophistication-1",
    "href": "2025/slides/session-6.html#lexical-sophistication-1",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lexical sophistication",
    "text": "Lexical sophistication\nThere are a number of lexical sophistication measures for English (+ 300).\n\n12 categories of measures (Eguchi & Kyle, 2020)\n\nFrequency\nRange\nPsycholinguistic Norm"
  },
  {
    "objectID": "2025/slides/session-6.html#typical-operationalization",
    "href": "2025/slides/session-6.html#typical-operationalization",
    "title": "Session 6: Hands-on activity #3",
    "section": "Typical operationalization",
    "text": "Typical operationalization\n\nOperationalization means\nTypically, lexical sophistication (LS) is calculated as an average:\nTypical LS score = \\[Total \\; LS \\; score \\over nToken \\; with \\; LS \\; score\\]\nAverage is just a convenient choice."
  },
  {
    "objectID": "2025/slides/session-6.html#using-an-emulation-of-taales",
    "href": "2025/slides/session-6.html#using-an-emulation-of-taales",
    "title": "Session 6: Hands-on activity #3",
    "section": "Using an emulation of TAALES",
    "text": "Using an emulation of TAALES\n\nSince the desktop version of TAALES is unstable these days, we will use a simple web application."
  },
  {
    "objectID": "2025/syllabus/index.html",
    "href": "2025/syllabus/index.html",
    "title": "Course Syllabus",
    "section": "",
    "text": "Course Title: Linguistic Data Analysis I\nCredits: 2\nFormat: Intensive 5-day course (15 sessions)\nLanguage: English\n Classroom: 113 Lecture room â‘£",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#course-information",
    "href": "2025/syllabus/index.html#course-information",
    "title": "Course Syllabus",
    "section": "",
    "text": "Course Title: Linguistic Data Analysis I\nCredits: 2\nFormat: Intensive 5-day course (15 sessions)\nLanguage: English\n Classroom: 113 Lecture room â‘£",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#instructor-information",
    "href": "2025/syllabus/index.html#instructor-information",
    "title": "Course Syllabus",
    "section": "Instructor Information",
    "text": "Instructor Information\nInstructor: Masaki Eguchi, Ph.D.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#course-description",
    "href": "2025/syllabus/index.html#course-description",
    "title": "Course Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces the foundations of corpus linguistics and the analysis of learner language through corpus linguistic approaches. It covers key concepts in corpus linguistics, including what corpora are, how they are used to answer (applied) linguistic research questions, and how to design corpus-based analyses to address substantive research questions in second language research. The primary language of analysis in this course is English, but students are encouraged to apply the concepts introduced to the languages they work with in their own research.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#learning-objectives",
    "href": "2025/syllabus/index.html#learning-objectives",
    "title": "Course Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this course, students will be able to:\n\nExplain what corpus linguistics is and how corpus linguistics can help learn linguistic phenomena\nSearch for and select available corpora relevant to their own research\nDiscuss design issues related to language corpora for specific research purposes\nApply introductory corpus linguistic analyses (e.g., frequency analysis, concordancing, POS tagging) to preprocessed corpora\nEvaluate the benefits and drawbacks of a corpus linguistic approach to linguistic analysis",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#course-components",
    "href": "2025/syllabus/index.html#course-components",
    "title": "Course Syllabus",
    "section": "Course Components",
    "text": "Course Components\n\nLectures and tutorials\nDaily Hands-on activities\nMini-corpus labs\nFinal project\n\n\n\n\n\n\n\nNavigation\n\n\n\n\nğŸ“… Detailed Schedule",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#required-materials",
    "href": "2025/syllabus/index.html#required-materials",
    "title": "Course Syllabus",
    "section": "Required Materials",
    "text": "Required Materials\n\nTextbook\n\nDurrant, P. (2023). Corpus linguistics for writing development: A guide for research. Routledge. https://doi.org/10.4324/9781003152682\nStefanowitsch, A. (2020). Corpus linguistics: A guide to the methodology. Zenodo. https://doi.org/10.5281/ZENODO.3735822 (This is an open source textbook, so itâ€™s freely available online)\n\nOther required/Optional readings are provided through Google Classroom.\n\n\nSoftwares (Free)\n\nWeb application for simple text analyses\n\nSimple Text Analyzer: A web app created for you.\n\n\n\nConcordancing Software\n\nAntConc: Corpus analysis toolkit for Concordancing\n\n\n\nLexical Profiling Software\n\nAntWordProfiler: Corpus analysis toolkit for Lexical Profiling\nLexTutor: Corpus analysis toolkit online\n\n\n\nStatistics\n\nJASP: Statistical analysis software\n\n\n\nOthers\n\nGoogle Colaboratory: Follow the instruction here to enable the tool.\nText Editor: VS Code recommended",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#assignments-and-grading",
    "href": "2025/syllabus/index.html#assignments-and-grading",
    "title": "Course Syllabus",
    "section": "Assignments and Grading",
    "text": "Assignments and Grading\nYou can find detailed information about each assignment in assignments page (under construction).\nWe have two possible choices for the grade distribution for this course. I will explain the details of each choice in the first day of the course. The class as a whole decides which one we will incorporate in this course. Once decided, all students will follow the decided plan.\n\nGrade Distribution â€“ Option A\n\nFor option A, pairs of students will develop their own mini-research and present their final project on the final day.\n\n\n\n\nAssignment\n\nPercent\n\n\n\n\nHands-on Assignments\n(4 Ã— 15%)\n60%\n\n\nClass Participation\n\n20%\n\n\nFinal Project\n\n20%\n\n\n\n\n\nGrade Distribution â€“ Option B\n\nFor option B, pairs of students will present on hands-on assignments on the final day.\n\n\n\n\nAssignment\n\nPercent\n\n\n\n\nHands-on Assignments\n(4 Ã— 15%)\n60%\n\n\nClass Participation\n\n20%\n\n\nFinal Presentation on Selected Hands-on Assignment\n\n20%\n\n\n\n\n\nGrading Scale\nWe follow the grading system at Tohoku University.\n\n\n\nGrade\nRange\nGrade Point\n\n\n\n\nAA\n100-90%\n4.0\n\n\nA\n89-80%\n3.0\n\n\nB\n79-70%\n2.0\n\n\nC\n69-60%\n1.0\n\n\nD\n59-0%\n0.0",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#daily-structure",
    "href": "2025/syllabus/index.html#daily-structure",
    "title": "Course Syllabus",
    "section": "Daily Structure",
    "text": "Daily Structure\nEach day follows this general pattern:\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 1\n\n\n12:00-13:00\nLunch break\n\n\n13:00-14:30\nSession 2\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 3\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#attendance-policy",
    "href": "2025/syllabus/index.html#attendance-policy",
    "title": "Course Syllabus",
    "section": "Attendance Policy",
    "text": "Attendance Policy\n\nDue to the intensive nature of the course, attendance and participation are crucial to your success in this course.\nHowever, in case of emergency, do not hesitate to reach out to the instructor for possible accomodation. I may be able to accommodate depending on the situation.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#assignment-submission",
    "href": "2025/syllabus/index.html#assignment-submission",
    "title": "Course Syllabus",
    "section": "Assignment Submission",
    "text": "Assignment Submission\n\nDeadlines\n\nAll assignments are due at 10:30 AM on the specified day\nLate submissions will receive a 10% penalty per day\nExtensions may be granted for documented emergencies.\n\n\n\nSubmission Format\n\nSubmit all assignments via the course management system (Google Classroom)\nUse the provided templates when available\nFile naming convention: LastName_Assignment#.ext\nAcceptable formats: .docx, .pdf, .ipynb (for Python notebooks)\n\n\n\nPlagiarism\n\n\nCollaboration",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#technology-policy",
    "href": "2025/syllabus/index.html#technology-policy",
    "title": "Course Syllabus",
    "section": "Technology Policy",
    "text": "Technology Policy\n\nRequired Technology\n\nBring a laptop to every session.\nEnsure all required software is installed.\n\n\n\nClassroom Etiquette\n\nLaptops should be used for course activities only",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#communication",
    "href": "2025/syllabus/index.html#communication",
    "title": "Course Syllabus",
    "section": "Communication",
    "text": "Communication\n\nCourse related communications will happen via Google Classroom.\n\n\nCourse Announcements\n\nCourse announcements are made through Google Classroom.\n\n\n\nMaterials Sharing\n\nMaterials (e.g., slides) are shared through this website.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#accommodations",
    "href": "2025/syllabus/index.html#accommodations",
    "title": "Course Syllabus",
    "section": "Accommodations",
    "text": "Accommodations",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#learning-objectives",
    "href": "2025/sessions/day4/session11.html#learning-objectives",
    "title": "Session 11",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nUnderstand NLP tasks such as POS tagging and dependency parsing\nUnderstand how automated parsing works\nConduct POS tagging using spaCy library in Python (through Google Colab)\nConduct Dependency parsing using spaCy library in Python (through Google Colab)\nConduct multi-lingual Part-Of-Speech (POS) tagging using TagAnt",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#key-concepts",
    "href": "2025/sessions/day4/session11.html#key-concepts",
    "title": "Session 11",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nPOS tagging\nDependency parsing\nPrecision, Recall, and F1 score",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#required-readings",
    "href": "2025/sessions/day4/session11.html#required-readings",
    "title": "Session 11",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nSkim Durrant Ch 6 (Ignore R codes if you are not familiar)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#recommended-readings",
    "href": "2025/sessions/day4/session11.html#recommended-readings",
    "title": "Session 11",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nKyle, K., & Eguchi, M. (2024). Evaluating NLP models with written and spoken L2 samples. Research Methods in Applied Linguistics, 3(2), 100120. https://doi.org/10.1016/j.rmal.2024.100120",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#notes",
    "href": "2025/sessions/day4/session11.html#notes",
    "title": "Session 11",
    "section": "ğŸ“ Notes",
    "text": "ğŸ“ Notes\n\nAll the python codes are prepared by the instructor and shared with the students. This session does not require ability to code.\nThe decision to use Python programming language rather than already available software is based on consideration that there is very little tools which provide stable access to the language analysis described here.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#tools-used",
    "href": "2025/sessions/day4/session11.html#tools-used",
    "title": "Session 11",
    "section": "ğŸ› ï¸ Tools Used",
    "text": "ğŸ› ï¸ Tools Used\n\nTagAnt\nSimple Text Analyzer: A web app created for you.\nGoogle Colaboratory",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#materials",
    "href": "2025/sessions/day4/session11.html#materials",
    "title": "Session 11",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#reflection",
    "href": "2025/sessions/day4/session11.html#reflection",
    "title": "Session 11",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html",
    "href": "2025/sessions/day4/index.html",
    "title": "Day 4: Analyzing Grammar",
    "section": "",
    "text": "Day 4 introduces grammatical analysis in corpus linguistics, exploring complexity measures and computational tools for parsing and analysis.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#overview",
    "href": "2025/sessions/day4/index.html#overview",
    "title": "Day 4: Analyzing Grammar",
    "section": "",
    "text": "Day 4 introduces grammatical analysis in corpus linguistics, exploring complexity measures and computational tools for parsing and analysis.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#key-concepts",
    "href": "2025/sessions/day4/index.html#key-concepts",
    "title": "Day 4: Analyzing Grammar",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nGrammatical complexity\nPredictive measures versus Descriptive measures\nPOS tagging\nDependency parsing\nPrecision, Recall, and F1 score\nSyntactic sophistication and fine-grained measures",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#preparation",
    "href": "2025/sessions/day4/index.html#preparation",
    "title": "Day 4: Analyzing Grammar",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 4:\n\nRead:\n\nDurrant Ch. 5\nKyle, K., & Crossley, S. A. (2018). Measuring Syntactic Complexity in L2 Writing Using Fineâ€Grained Clausal and Phrasal Indices. The Modern Language Journal, 102(2), 333â€“349.\n\nSkim:\n\nDurrant Ch. 6 (Ignore R codes if you are not familiar)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#schedule",
    "href": "2025/sessions/day4/index.html#schedule",
    "title": "Day 4: Analyzing Grammar",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 10: Grammar â€” Overview\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 11: POS Tagging and Parsing\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 12: Linguistic Complexity Analysis\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#assignments",
    "href": "2025/sessions/day4/index.html#assignments",
    "title": "Day 4: Analyzing Grammar",
    "section": "Assignments",
    "text": "Assignments\n\nDue Tomorrow: Hands-on Assignment 4\nComplete grammatical analysis exercises using Python notebooks and TagAnt",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#reflection",
    "href": "2025/sessions/day4/index.html#reflection",
    "title": "Day 4: Analyzing Grammar",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#learning-objectives",
    "href": "2025/sessions/day3/session9.html#learning-objectives",
    "title": "Session 9",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\nJustify choices of lexical richness measures to investigate a research questions\nConduct a simple statistical analysis of selected corpus on small sets of lexical measures using JASP software",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#key-concepts",
    "href": "2025/sessions/day3/session9.html#key-concepts",
    "title": "Session 9",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nLinear regression analysis (group comparison or prediction)\nIntroduction to in-class mini-project",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#required-readings",
    "href": "2025/sessions/day3/session9.html#required-readings",
    "title": "Session 9",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nNo new reading! (please re-read Eguchi & Kyle, 2020)",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day3/session9.html#dive-deeper---recommended-readings",
    "title": "Session 9",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings\n\nPaquot, M. (2019). The phraseological dimension in interlanguage complexity research. Second Language Research, 35(1), 121â€“145. https://doi.org/10.1177/0267658317694221\nEguchi, M., & Kyle, K. (2023). L2 collocation profiles and their relationship with vocabulary proficiency: A learner corpus approach. Journal of Second Language Writing, 60, 100975. https://doi.org/10.1016/j.jslw.2023.100975",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#materials",
    "href": "2025/sessions/day3/session9.html#materials",
    "title": "Session 9",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#reflection",
    "href": "2025/sessions/day3/session9.html#reflection",
    "title": "Session 9",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now:\n\nProvide reasons for your choice of lexical richness measures.\nConduct preliminary analysis to understand how text differ from one aother in relation to learnerâ€™s proficiency, learner groups, etc.\nBrainstorm your ideas for your final project.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#learning-objectives",
    "href": "2025/sessions/day3/session7.html#learning-objectives",
    "title": "Session 7",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, students will be able to:\n\n\nExplain different types of multiword units: collocation, n-grams, lexical bundles\nDemonstrate how major association strengths measures (t-score, Mutual Information, and LogDice) are calculated using examples",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#key-concepts",
    "href": "2025/sessions/day3/session7.html#key-concepts",
    "title": "Session 7",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nTypes of multiword units\nAssociation strengths\nThree approaches:\n\nContext window\nDependency bigram",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#required-readings",
    "href": "2025/sessions/day3/session7.html#required-readings",
    "title": "Session 7",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nDurrant (2023) Ch. 7\nGablasova, D., Brezina, V., & McEnery, T. (2017). Collocations in Corpusâ€Based Language Learning Research: Identifying, Comparing, and Interpreting the Evidence. Language Learning, 67(S1), 155â€“179. https://doi.org/10.1111/lang.12225\n\n\nğŸŒŠ Dive Deeper - Recommended Readings\n\nPaquot, M. (2019). The phraseological dimension in interlanguage complexity research. Second Language Research, 35(1), 121â€“145. https://doi.org/10.1177/0267658317694221",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#materials",
    "href": "2025/sessions/day3/session7.html#materials",
    "title": "Session 7",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#reflection",
    "href": "2025/sessions/day3/session7.html#reflection",
    "title": "Session 7",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now:\n\nDescribe major types of multiword units and how they differ from each other\n\nN-gram, Lexical Collocations, Colligations, lexical bundle\n\nDescribe benefits and drawbacks of major Strengths Of Association (SOA) measures\nDiscuss two approaches to identify collocation from the text: window-based approach and dependency-based approach.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#learning-objectives",
    "href": "2025/sessions/day2/session4.html#learning-objectives",
    "title": "Session 4",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nExplain the purposes of linguistic measures\nList commonly used lexical measures in second language acquisition research\nExplain sub-constructs of lexical richness measures\n\nLexical Diversity\nLexical Sophistication",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#key-concepts",
    "href": "2025/sessions/day2/session4.html#key-concepts",
    "title": "Session 4",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nLexical Richness\n\nDistinction between text internal vs external measures\n\nLexical Diversity\n\nType-Token Ratio\nMeasure of Textual Lexical Diversity (MTLD)\n\nLexical Sophistication\n\nFrequency\nConcreteness\nPhonological neighbors",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#required-readings",
    "href": "2025/sessions/day2/session4.html#required-readings",
    "title": "Session 4",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nDurrant Ch. 3\n(Skim) Eguchi, M., & Kyle, K. (2020). Continuing to Explore the Multidimensional Nature of Lexical Sophistication: The Case of Oral Proficiency Interviews. The Modern Language Journal, 104(2), 381â€“400. https://doi.org/10.1111/modl.12637",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#materials",
    "href": "2025/sessions/day2/session4.html#materials",
    "title": "Session 4",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#reflection",
    "href": "2025/sessions/day2/session4.html#reflection",
    "title": "Session 4",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#learning-objectives",
    "href": "2025/sessions/day2/session6.html#learning-objectives",
    "title": "Session 6",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\nBy the end of this session, you will be able to:\n\n\nCompute simple lexical diversity measures using spreadsheet software\nCompute advanced lexical diversity measures using TAALED\nExplain how modern lexical diversity measures are calculated\nCalculate simple lexical sophistication measures using dedicated web application\nDescribe how lexical sophistication measures behave on a single input text.\nDiscuss benefits and drawbacks of lexical richness measures.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#key-concepts",
    "href": "2025/sessions/day2/session6.html#key-concepts",
    "title": "Session 6",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nlexical diversity\nlexical sophistication\nLearner corpus research",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#tools-used",
    "href": "2025/sessions/day2/session6.html#tools-used",
    "title": "Session 6",
    "section": "ğŸ› ï¸ Tools Used",
    "text": "ğŸ› ï¸ Tools Used\n\nSimple Text Analyzer: A web app created for you.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#required-readings",
    "href": "2025/sessions/day2/session6.html#required-readings",
    "title": "Session 6",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\n(Skim) Durrant Ch. 4 (Ignore R codes if you are not familiar)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day2/session6.html#dive-deeper---recommended-readings",
    "title": "Session 6",
    "section": "ğŸŒŠ Dive Deeper - Recommended Readings",
    "text": "ğŸŒŠ Dive Deeper - Recommended Readings\n\nBestgen, Y. (2025). Estimating lexical diversity using the moving average type-token ratio (MATTR): Pros and cons. Research Methods in Applied Linguistics, 4(1), 100168. https://doi.org/10.1016/j.rmal.2024.100168\nKyle, K., Crossley, S., & Berger, C. (2018). The tool for the automatic analysis of lexical sophistication (TAALES): Version 2.0. Behavior Research Methods, 50(3), 1030â€“1046. https://doi.org/10.3758/s13428-017-0924-4\nKyle, K., Crossley, S. A., & Jarvis, S. (2021). Assessing the Validity of Lexical Diversity Indices Using Direct Judgements. Language Assessment Quarterly, 18(2), 154â€“170. https://doi.org/10.1080/15434303.2020.1844205\nKyle, K., Sung, H., Eguchi, M., & Zenker, F. (2024). Evaluating evidence for the reliability and validity of lexical diversity indices in L2 oral task responses. Studies in Second Language Acquisition, 46(1), 278â€“299. https://doi.org/10.1017/S0272263123000402",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#materials",
    "href": "2025/sessions/day2/session6.html#materials",
    "title": "Session 6",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#reflection",
    "href": "2025/sessions/day2/session6.html#reflection",
    "title": "Session 6",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session15.html",
    "href": "2025/sessions/day5/session15.html",
    "title": "Session 15",
    "section": "",
    "text": "Final project time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 15"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session15.html#one-liner",
    "href": "2025/sessions/day5/session15.html#one-liner",
    "title": "Session 15",
    "section": "",
    "text": "Final project time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 15"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html",
    "href": "2025/sessions/day5/session13.html",
    "title": "Session 13",
    "section": "",
    "text": "We will explore how powerful and limited Large Language Models (LLMs) can be at the same time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#one-liner",
    "href": "2025/sessions/day5/session13.html#one-liner",
    "title": "Session 13",
    "section": "",
    "text": "We will explore how powerful and limited Large Language Models (LLMs) can be at the same time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#learning-objectives",
    "href": "2025/sessions/day5/session13.html#learning-objectives",
    "title": "Session 13",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nDescribe how LLMs are trained generally and what LLMs do to produce language.\nExplain the benefits and drawbacks of using LLMs for linguistic annotation.\nDemonstrate/discuss potential impacts of prompts on the LLMs performance on linguistic annotation.\nDesign an experiment to investigate LLMs output accuracy on a given annotation task.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#key-concepts",
    "href": "2025/sessions/day5/session13.html#key-concepts",
    "title": "Session 13",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nLarge Language Models (LLMs) and Language Generation\nPrompt engineering\nFine-tuning",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#required-readings",
    "href": "2025/sessions/day5/session13.html#required-readings",
    "title": "Session 13",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nMizumoto, A., Shintani, N., Sasaki, M., & Teng, M. F. (2024). Testing the viability of ChatGPT as a companion in L2 writing accuracy assessment. Research Methods in Applied Linguistics, 3(2), 100116. https://doi.org/10.1016/j.rmal.2024.100116\n(Skim) Kim, M., & Lu, X. (2024). Exploring the potential of using ChatGPT for rhetorical move-step analysis: The impact of prompt refinement, few-shot learning, and fine-tuning. Journal of English for Academic Purposes, 71, 101422. https://doi.org/10.1016/j.jeap.2024.101422",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#recommended-readings",
    "href": "2025/sessions/day5/session13.html#recommended-readings",
    "title": "Session 13",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nMizumoto, A. (2025). Automated analysis of common errors in L2 learner production: Prototype web application development. Studies in Second Language Acquisition, 1â€“18. https://doi.org/10.1017/S0272263125100934\nYu, D., Li, L., Su, H., & Fuoli, M. (2024). Assessing the potential of LLM-assisted annotation for corpus-based pragmatics and discourse analysis: The case of apology. International Journal of Corpus Linguistics, 29(4), 534â€“561. https://doi.org/10.1075/ijcl.23087.yu",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#materials",
    "href": "2025/sessions/day5/session13.html#materials",
    "title": "Session 13",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)\n\n\n\nOther resources\n\nPrompting engineering guide\nPost by Mark Davies on how similar LLMâ€™s â€œintrospectionsâ€ are to corpus data",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#reflection",
    "href": "2025/sessions/day5/session13.html#reflection",
    "title": "Session 13",
    "section": "Reflection",
    "text": "Reflection\nYou can now:",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html",
    "href": "2025/sessions/day1/index.html",
    "title": "Day 1: Introduction and Foundations",
    "section": "",
    "text": "Day 1 introduces basic concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#overview",
    "href": "2025/sessions/day1/index.html#overview",
    "title": "Day 1: Introduction and Foundations",
    "section": "",
    "text": "Day 1 introduces basic concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#key-concepts",
    "href": "2025/sessions/day1/index.html#key-concepts",
    "title": "Day 1: Introduction and Foundations",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nlinguistic intuition\nscientific hypothesis and data\nKWIC\nConcordancing",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#preparation",
    "href": "2025/sessions/day1/index.html#preparation",
    "title": "Day 1: Introduction and Foundations",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 1:\n\nRead:\n\nStefanowitsch (2020) Ch. 1. Freely available online\nStefanowitsch (2020) Ch. 2. Freely available online\n\nSkim:\n\nDurrant (2023) Ch. 1.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#schedule",
    "href": "2025/sessions/day1/index.html#schedule",
    "title": "Day 1: Introduction and Foundations",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 1: Introduction\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 2: Corpus foundation\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 3: First Hands-on Activity\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#assignments",
    "href": "2025/sessions/day1/index.html#assignments",
    "title": "Day 1: Introduction and Foundations",
    "section": "Assignments",
    "text": "Assignments\n\nDue Tomorrow: Hands-on Assignment 1\nComplete basic corpus search exercise",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#reflection",
    "href": "2025/sessions/day1/index.html#reflection",
    "title": "Day 1: Introduction and Foundations",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html",
    "href": "2025/sessions/day1/session2.html",
    "title": "Session 2",
    "section": "",
    "text": "Session 2 covers foundational concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#one-liner",
    "href": "2025/sessions/day1/session2.html#one-liner",
    "title": "Session 2",
    "section": "",
    "text": "Session 2 covers foundational concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#learning-objectives",
    "href": "2025/sessions/day1/session2.html#learning-objectives",
    "title": "Session 2",
    "section": "ğŸ¯ Learning Objectives",
    "text": "ğŸ¯ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nDefine corpus linguistics as an empirical methodology\nExplain key limitations of introspection in linguistic research\nDescribe the role of frequency data and patterns in corpus analysis\nIdentify and explain the basic steps in corpus-based research\nReflect on their own stance toward data, intuition, and linguistic evidence",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#key-concepts",
    "href": "2025/sessions/day1/session2.html#key-concepts",
    "title": "Session 2",
    "section": "ğŸ”‘ Key Concepts",
    "text": "ğŸ”‘ Key Concepts\n\nCorpus linguistics\nBalanced Corpus\nReference Corpus\nLearner Corpus\nCorpus representativeness\nLinguistic intuition vs data",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#required-readings",
    "href": "2025/sessions/day1/session2.html#required-readings",
    "title": "Session 2",
    "section": "ğŸ“š Required Readings",
    "text": "ğŸ“š Required Readings\n\nStefanowitsch (2020) Ch. 1 Freely available online\nStefanowitsch (2020) Ch. 2 Freely available online\n(Skim) Durrant (2023) Ch. 1",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#materials",
    "href": "2025/sessions/day1/session2.html#materials",
    "title": "Session 2",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session (Under construction)",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#reflection",
    "href": "2025/sessions/day1/session2.html#reflection",
    "title": "Session 2",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/notebooks/session-5.html",
    "href": "2025/notebooks/session-5.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nShow code\nimport spacy\nfrom sudachipy import dictionary, tokenizer\nfrom spacy.tokens import Doc\n\n# 1. Standard pipeline (for reliable POS/DEP):\nstd_nlp = spacy.load(\"ja_core_news_sm\")\n\n# 2. Alternate segmentation pipeline (no tagging):\nsudachi = dictionary.Dictionary().create()\nMODE = tokenizer.Tokenizer.SplitMode.A  # A=short, C=long\n\nalt_nlp = spacy.blank(\"ja\")\n\ndef sudachi_tokenizer_func(text):\n    ms = sudachi.tokenize(text, MODE)\n    words = [m.surface() for m in ms]\n    spaces = [False]*len(words)\n    return Doc(alt_nlp.vocab, words=words, spaces=spaces)\n\nalt_nlp.tokenizer = sudachi_tokenizer_func\n\ntext = \"å½¼ã¯æ˜¨æ—¥æœ¬ã‚’è²·ã£ã¦èª­ã¿å§‹ã‚ã¾ã—ãŸã€‚\"\ndoc_std = std_nlp(text)\ndoc_alt = alt_nlp(text)\n\nprint(\"STD tokens:\", [t.text for t in doc_std])\nprint(\"ALT tokens (mode A):\", [t.text for t in doc_alt])\nprint(\"STD POS:\", [t.pos_ for t in doc_std])\n\n\nSTD tokens: ['å½¼', 'ã¯', 'æ˜¨', 'æ—¥æœ¬', 'ã‚’', 'è²·ã£', 'ã¦', 'èª­ã¿', 'å§‹ã‚', 'ã¾ã—', 'ãŸ', 'ã€‚']\nALT tokens (mode A): ['å½¼', 'ã¯', 'æ˜¨', 'æ—¥æœ¬', 'ã‚’', 'è²·ã£', 'ã¦', 'èª­ã¿', 'å§‹ã‚', 'ã¾ã—', 'ãŸ', 'ã€‚']\nSTD POS: ['PRON', 'ADP', 'NOUN', 'PROPN', 'ADP', 'VERB', 'SCONJ', 'VERB', 'VERB', 'AUX', 'AUX', 'PUNCT']\n\n\n\n\nShow code\nimport spacy\nfrom sudachipy import tokenizer, dictionary\nfrom spacy.tokens import Doc\n\nsudachi = dictionary.Dictionary().create()\nMODE = tokenizer.Tokenizer.SplitMode.C  # change to A for short proxy\n\nnlp = spacy.blank(\"ja\")\n\ndef sudachi_tokenizer_func(text):\n    sudachi_tokens = sudachi.tokenize(text, MODE)\n    words = [m.surface() for m in sudachi_tokens]\n    spaces = [False]*len(words)\n    return Doc(nlp.vocab, words=words, spaces=spaces)\n\n# nlp.tokenizer = sudachi_tokenizer_func\n\ndoc = nlp(\"ä»Šå¹´ã®å¹²æ”¯ã¯åºšå­ã§ã™ã€‚æ±äº¬ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯ãŸã®ã—ã¿ã ãªã‚ã€‚\")\nprint([t.text for t in doc])\nprint([(t.norm_, t.pos_, t.tag_) for t in doc])\n\n\n['ä»Šå¹´', 'ã®', 'å¹²æ”¯', 'ã¯', 'åºšå­', 'ã§ã™', 'ã€‚', 'æ±äº¬', 'ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯', 'ãŸã®ã—', 'ã¿', 'ã ', 'ãªã‚', 'ã€‚']\n[('ä»Šå¹´', 'NOUN', 'åè©-æ™®é€šåè©-å‰¯è©å¯èƒ½'), ('ã®', 'ADP', 'åŠ©è©-æ ¼åŠ©è©'), ('å¹²æ”¯', 'NOUN', 'åè©-æ™®é€šåè©-ä¸€èˆ¬'), ('ã¯', 'ADP', 'åŠ©è©-ä¿‚åŠ©è©'), ('åºšå­', 'NOUN', 'åè©-æ™®é€šåè©-ä¸€èˆ¬'), ('ã§ã™', 'AUX', 'åŠ©å‹•è©'), ('ã€‚', 'PUNCT', 'è£œåŠ©è¨˜å·-å¥ç‚¹'), ('æ±äº¬', 'PROPN', 'åè©-å›ºæœ‰åè©-åœ°å-ä¸€èˆ¬'), ('ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯', 'NOUN', 'åè©-æ™®é€šåè©-ä¸€èˆ¬'), ('æ¥½ã—ã„', 'ADJ', 'å½¢å®¹è©-ä¸€èˆ¬'), ('å‘³', 'PART', 'æ¥å°¾è¾-åè©çš„-ä¸€èˆ¬'), ('ã ', 'AUX', 'åŠ©å‹•è©'), ('ãª', 'PART', 'åŠ©è©-çµ‚åŠ©è©'), ('ã€‚', 'PUNCT', 'è£œåŠ©è¨˜å·-å¥ç‚¹')]\n\n\n\n\nShow code\nimport spacy\nnlp = spacy.load(\"ja_ginza\")\ndoc = nlp(\"ã²ã”ã‚ æ—¥ã”ã‚ æ—¥é ƒ å‘‘ã¿ å‘‘ã‚“ã§ é£²ã‚“ã§ æ›¸ãã‚ã‚‰ã‚ã™\")\nfor tok in doc:\n    print(tok.text, tok.lemma_)  # lemma_ ~ Sudachi dictionary form\n\n\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:75: SyntaxWarning: invalid escape sequence '\\s'\n  relative_imports = re.findall(\"^\\s*import\\s+\\.(\\S+)\\s*$\", content, flags=re.MULTILINE)\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:77: SyntaxWarning: invalid escape sequence '\\s'\n  relative_imports += re.findall(\"^\\s*from\\s+\\.(\\S+)\\s+import\", content, flags=re.MULTILINE)\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:119: SyntaxWarning: invalid escape sequence '\\s'\n  imports = re.findall(\"^\\s*import\\s+(\\S+)\\s*$\", content, flags=re.MULTILINE)\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:121: SyntaxWarning: invalid escape sequence '\\s'\n  imports += re.findall(\"^\\s*from\\s+(\\S+)\\s+import\", content, flags=re.MULTILINE)\n\n\nã²ã”ã‚ ã²ã”ã‚\næ—¥ã”ã‚ æ—¥ã”ã‚\næ—¥é ƒ æ—¥é ƒ\nå‘‘ã¿ å‘‘ã‚€\nå‘‘ã‚“ å‘‘ã‚€\nã§ ã§\né£²ã‚“ é£²ã‚€\nã§ ã§\næ›¸ãã‚ã‚‰ã‚ã™ æ›¸ãã‚ã‚‰ã‚ã™\n\n\n\n\nShow code\nimport spacy\nfrom spacy.tokens import Doc\nfrom fugashi import Tagger\n\n# Initialize UniDic Tagger\ntagger = Tagger()   # fugashi auto-loads UniDic if installed\n\nnlp = spacy.blank(\"ja\")        # blank Japanese pipeline\n\n# Register custom token extensions\nfrom spacy.tokens import Token\nToken.set_extension(\"unidic_lemma\", default=None, force=True)\nToken.set_extension(\"unidic_reading\", default=None, force=True)\nToken.set_extension(\"unidic_pos\", default=None, force=True)\nToken.set_extension(\"unidic_feats\", default=None, force=True)\n\ndef mecab_tokenizer(text):\n    words = []\n    spaces = []\n    lemmas = []\n    analyses = tagger(text)\n    for m in analyses:\n        surface = m.surface\n        words.append(surface)\n        spaces.append(False)  # Japanese generally no spaces\n    doc = Doc(nlp.vocab, words=words, spaces=spaces)\n    # Attach UniDic info\n    for tok, m in zip(doc, analyses):\n        # m.feature: tuple with UniDic columns. Structure depends on UniDic version.\n        # Typical indices (verify!) e.g. lemma at feature[10], reading at feature[9].\n        \n        feats = m.feature\n        # Safer: use fugashi attribute helpers\n        tok._.unidic_lemma = m.feature[10]  # or m.feature[10]\n        tok._.unidic_reading = m.feature[10]  # unified katakana reading\n        tok._.unidic_pos = \",\".join(m.feature[:4])  # hierarchical POS tuple\n        tok._.unidic_feats = feats\n    return doc\n\nnlp.tokenizer = mecab_tokenizer\n\n# (Optionally add your own components after this, e.g. a statistical tagger trained on this segmentation)\ndoc = nlp(\"æ—¥ã”ã‚ ã²ã”ã‚ æ—¥é ƒ å±…ã‚‹ ã„ã‚‹ æ›¸ãã‚ã‚‰ã‚ã™\")\nfor t in doc:\n    print(t.text, t._.unidic_lemma, t._.unidic_reading, t._.unidic_pos)\n\n\næ—¥ã”ã‚ æ—¥ã”ã‚ æ—¥ã”ã‚ åè©,æ™®é€šåè©,å‰¯è©å¯èƒ½,*\nã²ã”ã‚ ã²ã”ã‚ ã²ã”ã‚ åè©,æ™®é€šåè©,å‰¯è©å¯èƒ½,*\næ—¥é ƒ æ—¥é ƒ æ—¥é ƒ åè©,æ™®é€šåè©,å‰¯è©å¯èƒ½,*\nå±…ã‚‹ å±…ã‚‹ å±…ã‚‹ å‹•è©,éè‡ªç«‹å¯èƒ½,*,*\nã„ã‚‹ ã„ã‚‹ ã„ã‚‹ å‹•è©,éè‡ªç«‹å¯èƒ½,*,*\næ›¸ãã‚ã‚‰ã‚ã™ æ›¸ãã‚ã‚‰ã‚ã™ æ›¸ãã‚ã‚‰ã‚ã™ å‹•è©,ä¸€èˆ¬,*,*\n\n\n\n\nShow code\nprint(dir(m))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 print(dir(m))\n\nNameError: name 'm' is not defined\n\n\n\n\n\nShow code\nimport spacy\nfrom fugashi import Tagger\n\nnlp = spacy.load(\"ja_ginza\")\ntagger = Tagger()\n\n# Register extension fields (only if not already)\nfrom spacy.tokens import Token\nfor field in [\"unidic_lemma\",\"unidic_reading\",\"unidic_pos\"]:\n    if not Token.has_extension(field):\n        Token.set_extension(field, default=None)\n\ndef unicdic_enricher(doc):\n    text = doc.text\n    # Build a char-&gt;token index map (start offsets)\n    char2token = {}\n    for i, tok in enumerate(doc):\n        for pos in range(tok.idx, tok.idx + len(tok.text)):\n            char2token.setdefault(pos, i)\n    # Collect MeCab tokens with offsets\n    cursor = 0\n    for m in tagger(text):\n        surf = m.surface\n        start = text.find(surf, cursor)\n        cursor = start + len(surf)\n        # Find spaCy token that *starts* here (approx.)\n        if start in char2token:\n            i = char2token[start]\n            # Only annotate if exact surface match (avoid mid-token)\n            if doc[i].text.startswith(surf):\n                doc[i]._.unidic_lemma = m.dictionary_form\n                doc[i]._.unidic_reading = m.reading\n                doc[i]._.unidic_pos = \",\".join(m.pos)\n    return doc\n\nnlp.add_pipe(unicdic_enricher, name=\"unidic_enricher\", last=True)\n\ndoc = nlp(\"æ—¥ã”ã‚ ã²ã”ã‚ æ—¥é ƒ å±…ã‚‹ ã„ã‚‹ æ›¸ãã‚ã‚‰ã‚ã™\")\nfor t in doc:\n    print(t.text, t.lemma_, t._.unidic_lemma)\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[16], line 36\n     33                 doc[i]._.unidic_pos = \",\".join(m.pos)\n     34     return doc\n---&gt; 36 nlp.add_pipe(unicdic_enricher, name=\"unidic_enricher\", last=True)\n     38 doc = nlp(\"æ—¥ã”ã‚ ã²ã”ã‚ æ—¥é ƒ å±…ã‚‹ ã„ã‚‹ æ›¸ãã‚ã‚‰ã‚ã™\")\n     39 for t in doc:\n\nFile ~/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/spacy/language.py:811, in Language.add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\n    809     bad_val = repr(factory_name)\n    810     err = Errors.E966.format(component=bad_val, name=name)\n--&gt; 811     raise ValueError(err)\n    812 name = name if name is not None else factory_name\n    813 if name in self.component_names:\n\nValueError: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got &lt;function unicdic_enricher at 0x13ca313a0&gt; (name: 'unidic_enricher').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
  },
  {
    "objectID": "resources/code-examples/python/index.html",
    "href": "resources/code-examples/python/index.html",
    "title": "Python Notebooks",
    "section": "",
    "text": "Python notebooks for corpus analysis tasks."
  },
  {
    "objectID": "resources/code-examples/python/index.html#available-notebooks",
    "href": "resources/code-examples/python/index.html#available-notebooks",
    "title": "Python Notebooks",
    "section": "",
    "text": "Python notebooks for corpus analysis tasks."
  },
  {
    "objectID": "resources/code-examples/python/index.html#how-to-use",
    "href": "resources/code-examples/python/index.html#how-to-use",
    "title": "Python Notebooks",
    "section": "How to Use",
    "text": "How to Use\n\nClick on any notebook link\nOpen in Google Colab\nMake a copy to your Google Drive\nRun cells sequentially"
  },
  {
    "objectID": "resources/code-examples/index.html",
    "href": "resources/code-examples/index.html",
    "title": "Code Examples",
    "section": "",
    "text": "Python Notebooks\nSpreadsheet Templates",
    "crumbs": [
      "Resources",
      "Code Examples"
    ]
  },
  {
    "objectID": "resources/code-examples/index.html#available-resources",
    "href": "resources/code-examples/index.html#available-resources",
    "title": "Code Examples",
    "section": "",
    "text": "Python Notebooks\nSpreadsheet Templates",
    "crumbs": [
      "Resources",
      "Code Examples"
    ]
  },
  {
    "objectID": "resources/tools/japanese-nlp.html",
    "href": "resources/tools/japanese-nlp.html",
    "title": "Japanese NLP",
    "section": "",
    "text": "This page provides some advanced resources on Japanese NLP."
  },
  {
    "objectID": "resources/tools/japanese-nlp.html#demo-pages",
    "href": "resources/tools/japanese-nlp.html#demo-pages",
    "title": "Japanese NLP",
    "section": "Demo pages",
    "text": "Demo pages\n\nSpacy GiNZA morphological analysis\nWeb app for UniDic morphological analysis"
  },
  {
    "objectID": "resources/tools/japanese-nlp.html#using-unidic-with-fugashi",
    "href": "resources/tools/japanese-nlp.html#using-unidic-with-fugashi",
    "title": "Japanese NLP",
    "section": "Using Unidic with Fugashi",
    "text": "Using Unidic with Fugashi\n\n\nFugashi\n\n# Core tools\npip install fugashi unidic-lite  # quick start (smaller)\n# OR for full UniDic (larger, closer to BCCWJ)\npip install fugashi unidic\npython -m unidic download  # downloads the full UniDic dictionary"
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html",
    "href": "resources/tools/byu-corpora-guide.html",
    "title": "English Corpora Guide",
    "section": "",
    "text": "The EnglishCorpora.org, formally BYU (Brigham Young University) corpora, provide web-based interfaces to some of the largest and most widely-used corpora in the world. These include COCA (Corpus of Contemporary American English), BNC (British National Corpus), and many others.\nYou will need a usable account for english-corpora.org on on Day 1.",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html#overview",
    "href": "resources/tools/byu-corpora-guide.html#overview",
    "title": "English Corpora Guide",
    "section": "",
    "text": "The EnglishCorpora.org, formally BYU (Brigham Young University) corpora, provide web-based interfaces to some of the largest and most widely-used corpora in the world. These include COCA (Corpus of Contemporary American English), BNC (British National Corpus), and many others.\nYou will need a usable account for english-corpora.org on on Day 1.",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html#available-corpora",
    "href": "resources/tools/byu-corpora-guide.html#available-corpora",
    "title": "English Corpora Guide",
    "section": "Available Corpora",
    "text": "Available Corpora\n\nMajor English Corpora\n\nCOCA (Corpus of Contemporary American English): 1 billion words, 1990-2019\nBNC (British National Corpus): 100 million words\nGloWbE (Global Web-Based English): 1.9 billion words\nNOW (News on the Web): 14+ billion words, updated daily\nCOHA (Corpus of Historical American English): 400 million words, 1810-2009\n\n\n\nSpecialized Corpora\n\nSOAP (Corpus of American Soap Operas): 100 million words\nTIME (TIME Magazine Corpus): 100 million words, 1920s-2000s\nWikipedia Corpus: 1.9 billion words",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html#registration-and-access",
    "href": "resources/tools/byu-corpora-guide.html#registration-and-access",
    "title": "English Corpora Guide",
    "section": "Registration and Access",
    "text": "Registration and Access\n\nFree Access\n\nVisit english-corpora.org\nClick on desired corpus\nRegister for free account\nLimited to 20 queries per day\n\n\n\nAcademic License (Do not purchase for this class)\n\nExtended query limits\nDownload capabilities\nAvailable through institution",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html",
    "href": "resources/tools/python-setup.html",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "",
    "text": "In this 5-day intensive course, we will use Python through Google colaboratory, browser based environment that requires no installation on your local computer.\nYou can follow the step here to enable Colaboratory through your gmail account.\nThe following steps should be completed before we start grammar analysis on Day 4.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#overview",
    "href": "resources/tools/python-setup.html#overview",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "",
    "text": "In this 5-day intensive course, we will use Python through Google colaboratory, browser based environment that requires no installation on your local computer.\nYou can follow the step here to enable Colaboratory through your gmail account.\nThe following steps should be completed before we start grammar analysis on Day 4.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#log-in-to-your-google-account.",
    "href": "resources/tools/python-setup.html#log-in-to-your-google-account.",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Log in to your google account.",
    "text": "Log in to your google account.\nYou will need a google account, so log in to your google account.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#go-to-google-drive",
    "href": "resources/tools/python-setup.html#go-to-google-drive",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Go to Google Drive",
    "text": "Go to Google Drive\nGo to google drive.\n\n\n\ngoogle drive",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#hit-new-and-find-connect-more-apps",
    "href": "resources/tools/python-setup.html#hit-new-and-find-connect-more-apps",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Hit new and find connect more apps",
    "text": "Hit new and find connect more apps\n\n\n\nconnect-more-app",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#search-colaboratory-on-the-marketplace",
    "href": "resources/tools/python-setup.html#search-colaboratory-on-the-marketplace",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Search Colaboratory on the marketplace",
    "text": "Search Colaboratory on the marketplace\n\n\n\nsearch-market-place",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#install-colaboratory",
    "href": "resources/tools/python-setup.html#install-colaboratory",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Install Colaboratory",
    "text": "Install Colaboratory\nClick on Colaboratory, and hit install button. \nHit continue when it prompts permission.\n\n\n\nsearch-market-place\n\n\nFollow the instruction of the pop-up instruction.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#installation-success",
    "href": "resources/tools/python-setup.html#installation-success",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Installation success!!",
    "text": "Installation success!!\n\n\n\nsuccess",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#now-you-can-create-google-colab-notebook-from-google-drive.",
    "href": "resources/tools/python-setup.html#now-you-can-create-google-colab-notebook-from-google-drive.",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Now you can create google colab notebook from Google Drive.",
    "text": "Now you can create google colab notebook from Google Drive.\nNow you can create a new google colab notebook.\n\n\n\ncreate a note",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/corpora/index.html",
    "href": "resources/corpora/index.html",
    "title": "Corpora Resources",
    "section": "",
    "text": "Available Corpora\nLearner Corpora",
    "crumbs": [
      "Resources",
      "Corpora",
      "Corpora Resources"
    ]
  },
  {
    "objectID": "resources/corpora/index.html#resources",
    "href": "resources/corpora/index.html#resources",
    "title": "Corpora Resources",
    "section": "",
    "text": "Available Corpora\nLearner Corpora",
    "crumbs": [
      "Resources",
      "Corpora",
      "Corpora Resources"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html",
    "href": "resources/corpora/available-corpora.html",
    "title": "Available Corpora",
    "section": "",
    "text": "Content to be added.\n\nCorpusMate\nMontclair State University - CORAL lab",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#placeholder",
    "href": "resources/corpora/available-corpora.html#placeholder",
    "title": "Available Corpora",
    "section": "",
    "text": "Content to be added.\n\nCorpusMate\nMontclair State University - CORAL lab",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#vocabulary-list",
    "href": "resources/corpora/available-corpora.html#vocabulary-list",
    "title": "Available Corpora",
    "section": "Vocabulary list",
    "text": "Vocabulary list\n\nç¾ä»£æ—¥æœ¬èªæ›¸ãè¨€è‘‰å‡è¡¡ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆBCCWJï¼‰ã€€å…¬é–‹ãƒ‡ãƒ¼ã‚¿\næ—¥æœ¬èªè©±ã—è¨€è‘‰ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆCSJ)\nå›½èªç ”æ—¥æœ¬èªã‚¦ã‚§ãƒ–ã‚³ãƒ¼ãƒ‘ã‚¹",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#corpus-data",
    "href": "resources/corpora/available-corpora.html#corpus-data",
    "title": "Available Corpora",
    "section": "Corpus data",
    "text": "Corpus data\n\nwortschatz corpus",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#databases",
    "href": "resources/corpora/available-corpora.html#databases",
    "title": "Available Corpora",
    "section": "Databases",
    "text": "Databases\n\nCollection of age of acquisition ratings for over 5,000 Japanese words\nJALEX: Japanese Version of Lexical Decision Database\nAWD-J: AWD-J: Abstractness of Word Database for Japanese common words",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  }
]
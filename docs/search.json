[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linguistic Data Analysis I",
    "section": "",
    "text": "This intensive 5-day graduate course introduces students to corpus linguistics and learner language analysis. Through hands-on activities and practical applications, you‚Äôll learn to use computational tools to analyze linguistic data, with a special focus on learner corpora.\n\n\n\n\n\n\nQuick Links\n\n\n\n\nüìã Course Syllabus\nüìÖ Schedule\nüíª Sessions\nüìù Assignments\nüîß Resources"
  },
  {
    "objectID": "index.html#welcome-to-linguistic-data-analysis-i",
    "href": "index.html#welcome-to-linguistic-data-analysis-i",
    "title": "Linguistic Data Analysis I",
    "section": "",
    "text": "This intensive 5-day graduate course introduces students to corpus linguistics and learner language analysis. Through hands-on activities and practical applications, you‚Äôll learn to use computational tools to analyze linguistic data, with a special focus on learner corpora.\n\n\n\n\n\n\nQuick Links\n\n\n\n\nüìã Course Syllabus\nüìÖ Schedule\nüíª Sessions\nüìù Assignments\nüîß Resources"
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Linguistic Data Analysis I",
    "section": "Course Overview",
    "text": "Course Overview\n\n\n\nWhat You‚Äôll Learn\n\nCorpus analysis techniques\nLearner language analysis methods\nPractical applications with real corpora\nResearch methodology in corpus linguistics\n\n\n\n\nKey Tools\n\nAntConc - Corpus analysis software\nBYU Corpora - Online corpus interfaces\nPython - Text processing (via Google Colab)\nJASP - Statistical analysis"
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Linguistic Data Analysis I",
    "section": "Course Structure",
    "text": "Course Structure\nThe course is organized into 5 days:\n\n\n\nDay\nTheme\nSessions\n\n\n\n\nDay 1\nIntroduction & Corpus Basics\n\n\n\nDay 2\nAnalysis of Vocabulary & Multiword Units (1)\n\n\n\nDay 3\nAnalysis of Vocabulary & Multiword Units (2)\n\n\n\nDay 4\nAnalysis of Grammar\n\n\n\nDay 5\nAdvanced Topics & Projects"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Linguistic Data Analysis I",
    "section": "Getting Started",
    "text": "Getting Started\n\nReview the syllabus for course policies and expectations\nCheck the detailed schedule for session timings\nInstall required software using our setup guides\nBrowse the resources section for helpful materials"
  },
  {
    "objectID": "index.html#instructor-information",
    "href": "index.html#instructor-information",
    "title": "Linguistic Data Analysis I",
    "section": "Instructor Information",
    "text": "Instructor Information\nInstructor: Masaki Eguchi, Ph.D.\nEmail: You can contact me through Google Classroom"
  },
  {
    "objectID": "index.html#course-communication",
    "href": "index.html#course-communication",
    "title": "Linguistic Data Analysis I",
    "section": "Course Communication",
    "text": "Course Communication\n\n\n\n\n\n\nStay Connected\n\n\n\n\nCourse Website: This site\nCommunication: Google Classroom\nAssignment Submission: Google Classroom"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Linguistic Data Analysis I",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis course builds on materials and approaches from:\n\nDr.¬†Kris Kyle (University of Oregon) for his previous corpus linguistics/NLP classes from University of Hawai‚Äôi and Oregon.\nDr.¬†Andrew Heiss (Georgia State University) for his Quarto-based materials and website settings, which significantly enhanced the accessibility of the course content."
  },
  {
    "objectID": "resources/corpora/available-corpora.html",
    "href": "resources/corpora/available-corpora.html",
    "title": "Available Corpora",
    "section": "",
    "text": "Content to be added.\n\nCorpusMate\nMontclair State University - CORAL lab",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#placeholder",
    "href": "resources/corpora/available-corpora.html#placeholder",
    "title": "Available Corpora",
    "section": "",
    "text": "Content to be added.\n\nCorpusMate\nMontclair State University - CORAL lab",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#japanese-corpus",
    "href": "resources/corpora/available-corpora.html#japanese-corpus",
    "title": "Available Corpora",
    "section": "Japanese corpus",
    "text": "Japanese corpus\n\nNINJAL-LWP for TWC\nÂêçÂ§ß‰ºöË©±„Ç≥„Éº„Éë„Çπ\nÊó•Êú¨Ë™ûÂØæË©±„Ç≥„Éº„Éë„Çπ‰∏ÄË¶ß\nÈñ¢Ë•øÂºÅ„Ç≥„Éº„Éë„Çπ\nAozorabunko-data",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#vocabulary-list",
    "href": "resources/corpora/available-corpora.html#vocabulary-list",
    "title": "Available Corpora",
    "section": "Vocabulary list",
    "text": "Vocabulary list\n\nÁèæ‰ª£Êó•Êú¨Ë™ûÊõ∏„ÅçË®ÄËëâÂùáË°°„Ç≥„Éº„Éë„ÇπÔºàBCCWJÔºâ„ÄÄÂÖ¨Èñã„Éá„Éº„Çø\nÊó•Êú¨Ë™ûË©±„ÅóË®ÄËëâ„Ç≥„Éº„Éë„ÇπÔºàCSJ)\nÂõΩË™ûÁ†îÊó•Êú¨Ë™û„Ç¶„Çß„Éñ„Ç≥„Éº„Éë„Çπ",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#corpus-data",
    "href": "resources/corpora/available-corpora.html#corpus-data",
    "title": "Available Corpora",
    "section": "Corpus data",
    "text": "Corpus data\n\nwortschatz corpus",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/available-corpora.html#databases",
    "href": "resources/corpora/available-corpora.html#databases",
    "title": "Available Corpora",
    "section": "Databases",
    "text": "Databases\n\nCollection of age of acquisition ratings for over 5,000 Japanese words\nJALEX: Japanese Version of Lexical Decision Database\nAWD-J: AWD-J: Abstractness of Word Database for Japanese common words",
    "crumbs": [
      "Resources",
      "Corpora",
      "Available Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/index.html",
    "href": "resources/corpora/index.html",
    "title": "Corpora Resources",
    "section": "",
    "text": "Available Corpora\nLearner Corpora",
    "crumbs": [
      "Resources",
      "Corpora",
      "Corpora Resources"
    ]
  },
  {
    "objectID": "resources/corpora/index.html#resources",
    "href": "resources/corpora/index.html#resources",
    "title": "Corpora Resources",
    "section": "",
    "text": "Available Corpora\nLearner Corpora",
    "crumbs": [
      "Resources",
      "Corpora",
      "Corpora Resources"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html",
    "href": "resources/tools/python-setup.html",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "",
    "text": "In this 5-day intensive course, we will use Python through Google colaboratory, browser based environment that requires no installation on your local computer.\nYou can follow the step here to enable Colaboratory through your gmail account.\nThe following steps should be completed before we start grammar analysis on Day 4.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#overview",
    "href": "resources/tools/python-setup.html#overview",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "",
    "text": "In this 5-day intensive course, we will use Python through Google colaboratory, browser based environment that requires no installation on your local computer.\nYou can follow the step here to enable Colaboratory through your gmail account.\nThe following steps should be completed before we start grammar analysis on Day 4.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#log-in-to-your-google-account.",
    "href": "resources/tools/python-setup.html#log-in-to-your-google-account.",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Log in to your google account.",
    "text": "Log in to your google account.\nYou will need a google account, so log in to your google account.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#go-to-google-drive",
    "href": "resources/tools/python-setup.html#go-to-google-drive",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Go to Google Drive",
    "text": "Go to Google Drive\nGo to google drive.\n\n\n\ngoogle drive",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#hit-new-and-find-connect-more-apps",
    "href": "resources/tools/python-setup.html#hit-new-and-find-connect-more-apps",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Hit new and find connect more apps",
    "text": "Hit new and find connect more apps\n\n\n\nconnect-more-app",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#search-colaboratory-on-the-marketplace",
    "href": "resources/tools/python-setup.html#search-colaboratory-on-the-marketplace",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Search Colaboratory on the marketplace",
    "text": "Search Colaboratory on the marketplace\n\n\n\nsearch-market-place",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#install-colaboratory",
    "href": "resources/tools/python-setup.html#install-colaboratory",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Install Colaboratory",
    "text": "Install Colaboratory\nClick on Colaboratory, and hit install button. \nHit continue when it prompts permission.\n\n\n\nsearch-market-place\n\n\nFollow the instruction of the pop-up instruction.",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#installation-success",
    "href": "resources/tools/python-setup.html#installation-success",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Installation success!!",
    "text": "Installation success!!\n\n\n\nsuccess",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/python-setup.html#now-you-can-create-google-colab-notebook-from-google-drive.",
    "href": "resources/tools/python-setup.html#now-you-can-create-google-colab-notebook-from-google-drive.",
    "title": "Python (Colaboratory) Setup Guide",
    "section": "Now you can create google colab notebook from Google Drive.",
    "text": "Now you can create google colab notebook from Google Drive.\nNow you can create a new google colab notebook.\n\n\n\ncreate a note",
    "crumbs": [
      "Resources",
      "Tools",
      "Python (Colaboratory) Setup Guide"
    ]
  },
  {
    "objectID": "resources/tools/index.html",
    "href": "resources/tools/index.html",
    "title": "Tools and Software",
    "section": "",
    "text": "English-Corpora.org Guide\nAntConc Guide\nPython (google colab) Setup\nJASP Guide",
    "crumbs": [
      "Resources",
      "Tools",
      "Tools and Software"
    ]
  },
  {
    "objectID": "resources/tools/index.html#available-guides",
    "href": "resources/tools/index.html#available-guides",
    "title": "Tools and Software",
    "section": "",
    "text": "English-Corpora.org Guide\nAntConc Guide\nPython (google colab) Setup\nJASP Guide",
    "crumbs": [
      "Resources",
      "Tools",
      "Tools and Software"
    ]
  },
  {
    "objectID": "resources/tools/index.html#other-useful-corpus-resources",
    "href": "resources/tools/index.html#other-useful-corpus-resources",
    "title": "Tools and Software",
    "section": "Other useful corpus resources",
    "text": "Other useful corpus resources\n\nCIABATTTA\nCorpus and Repository of Writing",
    "crumbs": [
      "Resources",
      "Tools",
      "Tools and Software"
    ]
  },
  {
    "objectID": "resources/tools/antconc-guide.html",
    "href": "resources/tools/antconc-guide.html",
    "title": "AntConc Guide",
    "section": "",
    "text": "We will use AntConc, a one of the most widely used corpus tool developed by Laurence ANTHONY (Waseda University).\nThanksfully, Laurence has shared tutorials on basic features of AntConc on Youtube.\nYou will need AntConc on Day 2 and 3.\nPlease complete the following steps before Day 2.\n\nDownload AntConc\n\n\nVisit AntConc Website; download the software to your computer.\n\n\nWatch the following tutorial videos\n\n\nLaurence Anthony‚Äôs intro to AntConc\n\nGetting started (10 mins)\nCorpus manager Basics (18 mins)\nKWIC tool basics (17 mins)\n\n\nIf you are unsure about these steps, do not hesitate to reach out to me through google classroom or through email.",
    "crumbs": [
      "Resources",
      "Tools",
      "AntConc Guide"
    ]
  },
  {
    "objectID": "resources/tools/antconc-guide.html#overview",
    "href": "resources/tools/antconc-guide.html#overview",
    "title": "AntConc Guide",
    "section": "",
    "text": "We will use AntConc, a one of the most widely used corpus tool developed by Laurence ANTHONY (Waseda University).\nThanksfully, Laurence has shared tutorials on basic features of AntConc on Youtube.\nYou will need AntConc on Day 2 and 3.\nPlease complete the following steps before Day 2.\n\nDownload AntConc\n\n\nVisit AntConc Website; download the software to your computer.\n\n\nWatch the following tutorial videos\n\n\nLaurence Anthony‚Äôs intro to AntConc\n\nGetting started (10 mins)\nCorpus manager Basics (18 mins)\nKWIC tool basics (17 mins)\n\n\nIf you are unsure about these steps, do not hesitate to reach out to me through google classroom or through email.",
    "crumbs": [
      "Resources",
      "Tools",
      "AntConc Guide"
    ]
  },
  {
    "objectID": "resources/tools/antconc-guide.html#tagant",
    "href": "resources/tools/antconc-guide.html#tagant",
    "title": "AntConc Guide",
    "section": "TagAnt",
    "text": "TagAnt\nWe will use another tool called, TagAnt. Please download this from the following website.\n\nTagAnt Website; download the software to your computer.",
    "crumbs": [
      "Resources",
      "Tools",
      "AntConc Guide"
    ]
  },
  {
    "objectID": "resources/code-examples/python/index.html",
    "href": "resources/code-examples/python/index.html",
    "title": "Python Notebooks",
    "section": "",
    "text": "Python notebooks for corpus analysis tasks."
  },
  {
    "objectID": "resources/code-examples/python/index.html#available-notebooks",
    "href": "resources/code-examples/python/index.html#available-notebooks",
    "title": "Python Notebooks",
    "section": "",
    "text": "Python notebooks for corpus analysis tasks."
  },
  {
    "objectID": "resources/code-examples/python/index.html#how-to-use",
    "href": "resources/code-examples/python/index.html#how-to-use",
    "title": "Python Notebooks",
    "section": "How to Use",
    "text": "How to Use\n\nClick on any notebook link\nOpen in Google Colab\nMake a copy to your Google Drive\nRun cells sequentially"
  },
  {
    "objectID": "resources/code-examples/python/ld_parallel_analysis.html",
    "href": "resources/code-examples/python/ld_parallel_analysis.html",
    "title": "Running Parallel Analysis for Lexical Diversity",
    "section": "",
    "text": "The following tutorial is taken from TAALED repository\nPlease refer to the tutorial there for more information.\n\n\nShow code\nfrom taaled import ld\nfrom pylats import lats\n\n#for creating an output filename\nfrom datetime import datetime \nfrom datetime import date\n\n#for finding a list of texts\nimport glob\n\n\n\n\nShow code\n\nfilenames = glob.glob(\"../../../corpus_data/sample_for_ld_demo/*.txt\")\n\n\n\n\nShow code\nfilenames\n\n\n['../../../corpus_data/sample_for_ld_demo/GRA_PTJ0_112_ORIG.txt',\n '../../../corpus_data/sample_for_ld_demo/GRA_PTJ0_070_ORIG.txt',\n '../../../corpus_data/sample_for_ld_demo/GRA_PTJ0_124_ORIG.txt']\n\n\n\n\nShow code\ndef outname_creator(fldname,isprll,other = None):\n    day = date.today().strftime(\"%Y%m%d\") #get date\n    time = datetime.now().strftime(\"%H%M%S\")\n    ldv = \"taaledv\" + ld.version\n    # latsv = \"pylatsv\" + lats.version\n    if isprll == True: # we are going to think about only the \"nopa\" option in this tutorial.\n        pa = \"pa\"\n    else:\n        pa = \"nopa\"\n\n    if other == None:\n        outn = \"_\".join([day,time,fldname,pa]) + \".txt\"\n    else:\n        outn = \"_\".join([day,time,fldname,pa,other]) + \".txt\"\n\n    return(outn)\n    \n    \noutname = outname_creator(\"sample\",False)   \n\n\n\n\nShow code\nld.ldwrite(filenames,outname)\n\n\n1 of 3 ../../../corpus_data/sample_for_ld_demo/GRA_PTJ0_112_ORIG.txt\nPlease define a parameters class (e.g., params = lats.ld_params_en). Stopping program.\n3 files considered \n 0 files skipped due to length issues: \n [] \n 0 files successfully processed by TAALED\n\n\n\n\nShow code\nld.ldwrite(filenames, outname, \n           mn = 20,\n           mx = 200, \n           interval = 20, \n           prll = True, \n           params = lats.ld_params_en)\n\n\n1 of 3 ../../../corpus_data/sample_for_ld_demo/GRA_PTJ0_112_ORIG.txt\n2 of 3 ../../../corpus_data/sample_for_ld_demo/GRA_PTJ0_070_ORIG.txt\n3 of 3 ../../../corpus_data/sample_for_ld_demo/GRA_PTJ0_124_ORIG.txt\n3 files considered \n 0 files skipped due to length issues: \n [] \n 3 files successfully processed by TAALED\n\n\n\n\nShow code\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the parallel analysis data into a DataFrame\ndf = pd.read_csv(\"20250727_134615_sample_nopa.txt\", sep='\\t')\n\n# Display basic info about the DataFrame\nprint(\"DataFrame shape:\", df.shape)\nprint(\"\\nColumn names:\", df.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(df.head())\nprint(\"\\nUnique filenames:\", df['filename'].unique())\n\n\nDataFrame shape: (30, 15)\n\nColumn names: ['filename', 'length', 'ntokens', 'ntypes', 'mtld', 'mtld92', 'mtldo', 'mattr', 'mattr11', 'ttr', 'rttr', 'lttr', 'maas', 'msttr', 'hdd']\n\nFirst few rows:\n                filename  length  ntokens  ntypes       mtld     mtld92  \\\n0  GRA_PTJ0_112_ORIG.txt      20       20    17.4  55.440000  15.982000   \n1  GRA_PTJ0_112_ORIG.txt      40       40    28.0  36.717737  14.683333   \n2  GRA_PTJ0_112_ORIG.txt      60       60    37.0  47.302778  15.595000   \n3  GRA_PTJ0_112_ORIG.txt      80       80    45.5  38.655250  16.159091   \n4  GRA_PTJ0_112_ORIG.txt     100      100    51.0  42.347917  19.952381   \n\n       mtldo     mattr   mattr11       ttr      rttr      lttr      maas  \\\n0  55.440000  0.870000  0.949091  0.870000  3.890758  0.952698  0.036358   \n1  40.681200  0.700000  0.955152  0.700000  4.427189  0.902472  0.060877   \n2  40.174376  0.661212  0.953333  0.616667  4.776679  0.881690  0.066535   \n3  38.790414  0.671290  0.950000  0.568750  5.087055  0.871207  0.067676   \n4  43.210139  0.660784  0.956566  0.510000  5.100000  0.853743  0.073128   \n\n      msttr       hdd  \n0  0.870000  0.000000  \n1  0.700000  0.000000  \n2  0.613333  0.690449  \n3  0.670000  0.706104  \n4  0.675000  0.693756  \n\nUnique filenames: ['GRA_PTJ0_112_ORIG.txt' 'GRA_PTJ0_070_ORIG.txt' 'GRA_PTJ0_124_ORIG.txt']\n\n\n\n\nShow code\n# Set up the plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Create four separate plots for each lexical diversity metric\n\n# 1. TTR (Type-Token Ratio) Plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df, x='length', y='ttr', hue='filename', marker='o', linewidth=2, markersize=6)\nplt.title('Type-Token Ratio (TTR) by Text Length', fontsize=14, fontweight='bold')\nplt.xlabel('Text Length (words)', fontsize=12)\nplt.ylabel('TTR', fontsize=12)\nplt.legend(title='Filename', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# 2. RTTR (Root Type-Token Ratio) Plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df, x='length', y='rttr', hue='filename', marker='o', linewidth=2, markersize=6)\nplt.title('Root Type-Token Ratio (RTTR) by Text Length', fontsize=14, fontweight='bold')\nplt.xlabel('Text Length (words)', fontsize=12)\nplt.ylabel('RTTR', fontsize=12)\nplt.legend(title='Filename', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# 3. Maas Index Plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df, x='length', y='maas', hue='filename', marker='o', linewidth=2, markersize=6)\nplt.title('Maas Index by Text Length', fontsize=14, fontweight='bold')\nplt.xlabel('Text Length (words)', fontsize=12)\nplt.ylabel('Maas Index', fontsize=12)\nplt.legend(title='Filename', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# 4. Log TTR Index Plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df, x='length', y='lttr', hue='filename', marker='o', linewidth=2, markersize=6)\nplt.title('Log TTR Index by Text Length', fontsize=14, fontweight='bold')\nplt.xlabel('Text Length (words)', fontsize=12)\nplt.ylabel('Log TTR Index', fontsize=12)\nplt.legend(title='Filename', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n# 5. MTLD (Measure of Textual Lexical Diversity) Plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df, x='length', y='mtldo', hue='filename', marker='o', linewidth=2, markersize=6)\nplt.title('Measure of Textual Lexical Diversity (MTLD) by Text Length', fontsize=14, fontweight='bold')\nplt.xlabel('Text Length (words)', fontsize=12)\nplt.ylabel('MTLD', fontsize=12)\nplt.legend(title='Filename', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# 6. MTLD (Measure of Textual Lexical Diversity) Plot\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df, x='length', y='mattr', hue='filename', marker='o', linewidth=2, markersize=6)\nplt.title('Mean Average Type-Token Ratio by Text Length', fontsize=14, fontweight='bold')\nplt.xlabel('Text Length (words)', fontsize=12)\nplt.ylabel('MATTR', fontsize=12)\nplt.legend(title='Filename', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Summary statistics for each metric by filename\nprint(\"\\n\" + \"=\"*60)\nprint(\"SUMMARY STATISTICS BY FILENAME\")\nprint(\"=\"*60)\n\nmetrics = ['ttr', 'rttr', 'maas', 'mtld']\nfor metric in metrics:\n    print(f\"\\n{metric.upper()} Summary:\")\n    print(df.groupby('filename')[metric].agg(['mean', 'std', 'min', 'max']).round(4))\n\n\n\n============================================================\nSUMMARY STATISTICS BY FILENAME\n============================================================\n\nTTR Summary:\n                         mean     std    min     max\nfilename                                            \nGRA_PTJ0_070_ORIG.txt  0.5825  0.1273  0.448  0.8750\nGRA_PTJ0_112_ORIG.txt  0.5095  0.1395  0.368  0.8708\nGRA_PTJ0_124_ORIG.txt  0.5968  0.1314  0.476  0.8958\n\nRTTR Summary:\n                         mean     std     min     max\nfilename                                             \nGRA_PTJ0_070_ORIG.txt  6.0991  0.8230  3.9131  7.1005\nGRA_PTJ0_112_ORIG.txt  5.2486  0.4906  3.8945  5.8186\nGRA_PTJ0_124_ORIG.txt  6.2516  0.8723  4.0063  7.5262\n\nMAAS Summary:\n                         mean     std     min     max\nfilename                                             \nGRA_PTJ0_070_ORIG.txt  0.0550  0.0082  0.0353  0.0630\nGRA_PTJ0_112_ORIG.txt  0.0692  0.0101  0.0364  0.0781\nGRA_PTJ0_124_ORIG.txt  0.0522  0.0100  0.0285  0.0627\n\nMTLD Summary:\n                          mean      std      min      max\nfilename                                                 \nGRA_PTJ0_070_ORIG.txt  50.9351  10.4496  39.9083  78.8492\nGRA_PTJ0_112_ORIG.txt  41.9704   4.3081  36.3750  53.0657\nGRA_PTJ0_124_ORIG.txt  65.3780   4.3214  59.2700  73.0065"
  },
  {
    "objectID": "2025/notebooks/session-5.html",
    "href": "2025/notebooks/session-5.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nShow code\nimport spacy\nfrom sudachipy import dictionary, tokenizer\nfrom spacy.tokens import Doc\n\n# 1. Standard pipeline (for reliable POS/DEP):\nstd_nlp = spacy.load(\"ja_core_news_sm\")\n\n# 2. Alternate segmentation pipeline (no tagging):\nsudachi = dictionary.Dictionary().create()\nMODE = tokenizer.Tokenizer.SplitMode.A  # A=short, C=long\n\nalt_nlp = spacy.blank(\"ja\")\n\ndef sudachi_tokenizer_func(text):\n    ms = sudachi.tokenize(text, MODE)\n    words = [m.surface() for m in ms]\n    spaces = [False]*len(words)\n    return Doc(alt_nlp.vocab, words=words, spaces=spaces)\n\nalt_nlp.tokenizer = sudachi_tokenizer_func\n\ntext = \"ÂΩº„ÅØÊò®Êó•Êú¨„ÇíË≤∑„Å£„Å¶Ë™≠„ÅøÂßã„ÇÅ„Åæ„Åó„Åü„ÄÇ\"\ndoc_std = std_nlp(text)\ndoc_alt = alt_nlp(text)\n\nprint(\"STD tokens:\", [t.text for t in doc_std])\nprint(\"ALT tokens (mode A):\", [t.text for t in doc_alt])\nprint(\"STD POS:\", [t.pos_ for t in doc_std])\n\n\nSTD tokens: ['ÂΩº', '„ÅØ', 'Êò®', 'Êó•Êú¨', '„Çí', 'Ë≤∑„Å£', '„Å¶', 'Ë™≠„Åø', 'Âßã„ÇÅ', '„Åæ„Åó', '„Åü', '„ÄÇ']\nALT tokens (mode A): ['ÂΩº', '„ÅØ', 'Êò®', 'Êó•Êú¨', '„Çí', 'Ë≤∑„Å£', '„Å¶', 'Ë™≠„Åø', 'Âßã„ÇÅ', '„Åæ„Åó', '„Åü', '„ÄÇ']\nSTD POS: ['PRON', 'ADP', 'NOUN', 'PROPN', 'ADP', 'VERB', 'SCONJ', 'VERB', 'VERB', 'AUX', 'AUX', 'PUNCT']\n\n\n\n\nShow code\nimport spacy\nfrom sudachipy import tokenizer, dictionary\nfrom spacy.tokens import Doc\n\nsudachi = dictionary.Dictionary().create()\nMODE = tokenizer.Tokenizer.SplitMode.C  # change to A for short proxy\n\nnlp = spacy.blank(\"ja\")\n\ndef sudachi_tokenizer_func(text):\n    sudachi_tokens = sudachi.tokenize(text, MODE)\n    words = [m.surface() for m in sudachi_tokens]\n    spaces = [False]*len(words)\n    return Doc(nlp.vocab, words=words, spaces=spaces)\n\n# nlp.tokenizer = sudachi_tokenizer_func\n\ndoc = nlp(\"‰ªäÂπ¥„ÅÆÂπ≤ÊîØ„ÅØÂ∫öÂ≠ê„Åß„Åô„ÄÇÊù±‰∫¨„Ç™„É™„É≥„Éî„ÉÉ„ÇØ„Åü„ÅÆ„Åó„Åø„Å†„Å™„ÅÇ„ÄÇ\")\nprint([t.text for t in doc])\nprint([(t.norm_, t.pos_, t.tag_) for t in doc])\n\n\n['‰ªäÂπ¥', '„ÅÆ', 'Âπ≤ÊîØ', '„ÅØ', 'Â∫öÂ≠ê', '„Åß„Åô', '„ÄÇ', 'Êù±‰∫¨', '„Ç™„É™„É≥„Éî„ÉÉ„ÇØ', '„Åü„ÅÆ„Åó', '„Åø', '„Å†', '„Å™„ÅÇ', '„ÄÇ']\n[('‰ªäÂπ¥', 'NOUN', 'ÂêçË©û-ÊôÆÈÄöÂêçË©û-ÂâØË©ûÂèØËÉΩ'), ('„ÅÆ', 'ADP', 'Âä©Ë©û-Ê†ºÂä©Ë©û'), ('Âπ≤ÊîØ', 'NOUN', 'ÂêçË©û-ÊôÆÈÄöÂêçË©û-‰∏ÄËà¨'), ('„ÅØ', 'ADP', 'Âä©Ë©û-‰øÇÂä©Ë©û'), ('Â∫öÂ≠ê', 'NOUN', 'ÂêçË©û-ÊôÆÈÄöÂêçË©û-‰∏ÄËà¨'), ('„Åß„Åô', 'AUX', 'Âä©ÂãïË©û'), ('„ÄÇ', 'PUNCT', 'Ë£úÂä©Ë®òÂè∑-Âè•ÁÇπ'), ('Êù±‰∫¨', 'PROPN', 'ÂêçË©û-Âõ∫ÊúâÂêçË©û-Âú∞Âêç-‰∏ÄËà¨'), ('„Ç™„É™„É≥„Éî„ÉÉ„ÇØ', 'NOUN', 'ÂêçË©û-ÊôÆÈÄöÂêçË©û-‰∏ÄËà¨'), ('Ê•Ω„Åó„ÅÑ', 'ADJ', 'ÂΩ¢ÂÆπË©û-‰∏ÄËà¨'), ('Âë≥', 'PART', 'Êé•Â∞æËæû-ÂêçË©ûÁöÑ-‰∏ÄËà¨'), ('„Å†', 'AUX', 'Âä©ÂãïË©û'), ('„Å™', 'PART', 'Âä©Ë©û-ÁµÇÂä©Ë©û'), ('„ÄÇ', 'PUNCT', 'Ë£úÂä©Ë®òÂè∑-Âè•ÁÇπ')]\n\n\n\n\nShow code\nimport spacy\nnlp = spacy.load(\"ja_ginza\")\ndoc = nlp(\"„Å≤„Åî„Çç Êó•„Åî„Çç Êó•È†É Âëë„Åø Âëë„Çì„Åß È£≤„Çì„Åß Êõ∏„Åç„ÅÇ„Çâ„Çè„Åô\")\nfor tok in doc:\n    print(tok.text, tok.lemma_)  # lemma_ ~ Sudachi dictionary form\n\n\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:75: SyntaxWarning: invalid escape sequence '\\s'\n  relative_imports = re.findall(\"^\\s*import\\s+\\.(\\S+)\\s*$\", content, flags=re.MULTILINE)\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:77: SyntaxWarning: invalid escape sequence '\\s'\n  relative_imports += re.findall(\"^\\s*from\\s+\\.(\\S+)\\s+import\", content, flags=re.MULTILINE)\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:119: SyntaxWarning: invalid escape sequence '\\s'\n  imports = re.findall(\"^\\s*import\\s+(\\S+)\\s*$\", content, flags=re.MULTILINE)\n/Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:121: SyntaxWarning: invalid escape sequence '\\s'\n  imports += re.findall(\"^\\s*from\\s+(\\S+)\\s+import\", content, flags=re.MULTILINE)\n\n\n„Å≤„Åî„Çç „Å≤„Åî„Çç\nÊó•„Åî„Çç Êó•„Åî„Çç\nÊó•È†É Êó•È†É\nÂëë„Åø Âëë„ÇÄ\nÂëë„Çì Âëë„ÇÄ\n„Åß „Åß\nÈ£≤„Çì È£≤„ÇÄ\n„Åß „Åß\nÊõ∏„Åç„ÅÇ„Çâ„Çè„Åô Êõ∏„Åç„ÅÇ„Çâ„Çè„Åô\n\n\n\n\nShow code\nimport spacy\nfrom spacy.tokens import Doc\nfrom fugashi import Tagger\n\n# Initialize UniDic Tagger\ntagger = Tagger()   # fugashi auto-loads UniDic if installed\n\nnlp = spacy.blank(\"ja\")        # blank Japanese pipeline\n\n# Register custom token extensions\nfrom spacy.tokens import Token\nToken.set_extension(\"unidic_lemma\", default=None, force=True)\nToken.set_extension(\"unidic_reading\", default=None, force=True)\nToken.set_extension(\"unidic_pos\", default=None, force=True)\nToken.set_extension(\"unidic_feats\", default=None, force=True)\n\ndef mecab_tokenizer(text):\n    words = []\n    spaces = []\n    lemmas = []\n    analyses = tagger(text)\n    for m in analyses:\n        surface = m.surface\n        words.append(surface)\n        spaces.append(False)  # Japanese generally no spaces\n    doc = Doc(nlp.vocab, words=words, spaces=spaces)\n    # Attach UniDic info\n    for tok, m in zip(doc, analyses):\n        # m.feature: tuple with UniDic columns. Structure depends on UniDic version.\n        # Typical indices (verify!) e.g. lemma at feature[10], reading at feature[9].\n        \n        feats = m.feature\n        # Safer: use fugashi attribute helpers\n        tok._.unidic_lemma = m.feature[10]  # or m.feature[10]\n        tok._.unidic_reading = m.feature[10]  # unified katakana reading\n        tok._.unidic_pos = \",\".join(m.feature[:4])  # hierarchical POS tuple\n        tok._.unidic_feats = feats\n    return doc\n\nnlp.tokenizer = mecab_tokenizer\n\n# (Optionally add your own components after this, e.g. a statistical tagger trained on this segmentation)\ndoc = nlp(\"Êó•„Åî„Çç „Å≤„Åî„Çç Êó•È†É Â±Ö„Çã „ÅÑ„Çã Êõ∏„Åç„ÅÇ„Çâ„Çè„Åô\")\nfor t in doc:\n    print(t.text, t._.unidic_lemma, t._.unidic_reading, t._.unidic_pos)\n\n\nÊó•„Åî„Çç Êó•„Åî„Çç Êó•„Åî„Çç ÂêçË©û,ÊôÆÈÄöÂêçË©û,ÂâØË©ûÂèØËÉΩ,*\n„Å≤„Åî„Çç „Å≤„Åî„Çç „Å≤„Åî„Çç ÂêçË©û,ÊôÆÈÄöÂêçË©û,ÂâØË©ûÂèØËÉΩ,*\nÊó•È†É Êó•È†É Êó•È†É ÂêçË©û,ÊôÆÈÄöÂêçË©û,ÂâØË©ûÂèØËÉΩ,*\nÂ±Ö„Çã Â±Ö„Çã Â±Ö„Çã ÂãïË©û,ÈùûËá™Á´ãÂèØËÉΩ,*,*\n„ÅÑ„Çã „ÅÑ„Çã „ÅÑ„Çã ÂãïË©û,ÈùûËá™Á´ãÂèØËÉΩ,*,*\nÊõ∏„Åç„ÅÇ„Çâ„Çè„Åô Êõ∏„Åç„ÅÇ„Çâ„Çè„Åô Êõ∏„Åç„ÅÇ„Çâ„Çè„Åô ÂãïË©û,‰∏ÄËà¨,*,*\n\n\n\n\nShow code\nprint(dir(m))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 print(dir(m))\n\nNameError: name 'm' is not defined\n\n\n\n\n\nShow code\nimport spacy\nfrom fugashi import Tagger\n\nnlp = spacy.load(\"ja_ginza\")\ntagger = Tagger()\n\n# Register extension fields (only if not already)\nfrom spacy.tokens import Token\nfor field in [\"unidic_lemma\",\"unidic_reading\",\"unidic_pos\"]:\n    if not Token.has_extension(field):\n        Token.set_extension(field, default=None)\n\ndef unicdic_enricher(doc):\n    text = doc.text\n    # Build a char-&gt;token index map (start offsets)\n    char2token = {}\n    for i, tok in enumerate(doc):\n        for pos in range(tok.idx, tok.idx + len(tok.text)):\n            char2token.setdefault(pos, i)\n    # Collect MeCab tokens with offsets\n    cursor = 0\n    for m in tagger(text):\n        surf = m.surface\n        start = text.find(surf, cursor)\n        cursor = start + len(surf)\n        # Find spaCy token that *starts* here (approx.)\n        if start in char2token:\n            i = char2token[start]\n            # Only annotate if exact surface match (avoid mid-token)\n            if doc[i].text.startswith(surf):\n                doc[i]._.unidic_lemma = m.dictionary_form\n                doc[i]._.unidic_reading = m.reading\n                doc[i]._.unidic_pos = \",\".join(m.pos)\n    return doc\n\nnlp.add_pipe(unicdic_enricher, name=\"unidic_enricher\", last=True)\n\ndoc = nlp(\"Êó•„Åî„Çç „Å≤„Åî„Çç Êó•È†É Â±Ö„Çã „ÅÑ„Çã Êõ∏„Åç„ÅÇ„Çâ„Çè„Åô\")\nfor t in doc:\n    print(t.text, t.lemma_, t._.unidic_lemma)\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[16], line 36\n     33                 doc[i]._.unidic_pos = \",\".join(m.pos)\n     34     return doc\n---&gt; 36 nlp.add_pipe(unicdic_enricher, name=\"unidic_enricher\", last=True)\n     38 doc = nlp(\"Êó•„Åî„Çç „Å≤„Åî„Çç Êó•È†É Â±Ö„Çã „ÅÑ„Çã Êõ∏„Åç„ÅÇ„Çâ„Çè„Åô\")\n     39 for t in doc:\n\nFile ~/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/spacy/language.py:811, in Language.add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\n    809     bad_val = repr(factory_name)\n    810     err = Errors.E966.format(component=bad_val, name=name)\n--&gt; 811     raise ValueError(err)\n    812 name = name if name is not None else factory_name\n    813 if name in self.component_names:\n\nValueError: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got &lt;function unicdic_enricher at 0x13ca313a0&gt; (name: 'unidic_enricher').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
  },
  {
    "objectID": "2025/notebooks/session-6.html",
    "href": "2025/notebooks/session-6.html",
    "title": "Session 6 ‚Äî Computing simple lexical diversity and sophistication index",
    "section": "",
    "text": "Show code\nlow_diversity = \"The dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.\"\nShow code\n\nhigh_diversity = \"A curious fox trotted briskly through the meadow, leaping over mossy logs, sniffing wildflowers, and vanishing into golden twilight.\"\nShow code\ndef count_token_type(text: str):\n    # delete punctuation\n    text = text.replace(\".\", \"\")\n    text = text.replace(\",\", \"\")\n    text = text.replace(\"?\", \"\")\n\n    \n    token_list = text.strip()\n    token_list = text.split(\" \")\n\n    token = len(token_list)\n    type = len(set(token_list))\n    return (token, type)\nShow code\ncount_token_type(low_diversity)\n\n\n(19, 8)\nShow code\ncount_token_type(high_diversity)\n\n\n(19, 19)\nShow code\nlow_diversity2 = \"The dog ran. The dog jumped. The dog barked. The dog played. The dog ran quickly. The dog jumped so high. The dog barked very loudly. The dog played, sat, and rolled. The dog sneezed. The dog ate the food.\"\nShow code\ncount_token_type(low_diversity2)\n\n\n(40, 18)\nShow code\nlow_diversity3 = \"The parrot squawked loudly. The parrot chirped again. A toucan perched nearby. The parrot fluttered. Wings flapped softly. The parrot chirped again. Feathers shimmered under sunlight. The crow cawed. The parrot glided low. The air shimmered. The owl blinked slowly. The parrot perched again. The owl blinked slowly. The parrot shrieked. The parrot chirped nearby again. The parrot squawked again.\"\nShow code\ncount_token_type(low_diversity3)\n\n\n(60, 27)"
  },
  {
    "objectID": "2025/notebooks/session-6.html#using-lexical-diversity-package",
    "href": "2025/notebooks/session-6.html#using-lexical-diversity-package",
    "title": "Session 6 ‚Äî Computing simple lexical diversity and sophistication index",
    "section": "Using Lexical Diversity package",
    "text": "Using Lexical Diversity package\nThere is a package called TAALED maintained by Dr.¬†Kris Kyle.\n\n\nShow code\nfrom taaled import ld\nfrom pylats import lats"
  },
  {
    "objectID": "2025/sessions/day1/session2.html",
    "href": "2025/sessions/day1/session2.html",
    "title": "Session 2",
    "section": "",
    "text": "Session 2 covers foundational concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#one-liner",
    "href": "2025/sessions/day1/session2.html#one-liner",
    "title": "Session 2",
    "section": "",
    "text": "Session 2 covers foundational concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#learning-objectives",
    "href": "2025/sessions/day1/session2.html#learning-objectives",
    "title": "Session 2",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nDefine corpus linguistics as an empirical methodology\nExplain key limitations of introspection in linguistic research\nDescribe the role of frequency data and patterns in corpus analysis\nIdentify and explain the basic steps in corpus-based research\nReflect on their own stance toward data, intuition, and linguistic evidence",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#key-concepts",
    "href": "2025/sessions/day1/session2.html#key-concepts",
    "title": "Session 2",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nCorpus linguistics\nBalanced Corpus\nReference Corpus\nLearner Corpus\nCorpus representativeness\nLinguistic intuition vs data",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#required-readings",
    "href": "2025/sessions/day1/session2.html#required-readings",
    "title": "Session 2",
    "section": "üìö Required Readings",
    "text": "üìö Required Readings\n\nStefanowitsch (2020) Ch. 1 Freely available online\nStefanowitsch (2020) Ch. 2 Freely available online\n(Skim) Durrant (2023) Ch. 1",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day1/session2.html#dive-deeper---recommended-readings",
    "title": "Session 2",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#materials",
    "href": "2025/sessions/day1/session2.html#materials",
    "title": "Session 2",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\nüìä View Interactive Slides",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session2.html#reflection",
    "href": "2025/sessions/day1/session2.html#reflection",
    "title": "Session 2",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 2"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html",
    "href": "2025/sessions/day1/index.html",
    "title": "Day 1: Introduction and Foundations",
    "section": "",
    "text": "Day 1 introduces basic concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#overview",
    "href": "2025/sessions/day1/index.html#overview",
    "title": "Day 1: Introduction and Foundations",
    "section": "",
    "text": "Day 1 introduces basic concepts of corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#key-concepts",
    "href": "2025/sessions/day1/index.html#key-concepts",
    "title": "Day 1: Introduction and Foundations",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nlinguistic intuition\nscientific hypothesis and data\nKWIC\nConcordancing",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#preparation",
    "href": "2025/sessions/day1/index.html#preparation",
    "title": "Day 1: Introduction and Foundations",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 1:\n\nRead:\n\nStefanowitsch (2020) Ch. 1. Freely available online\nStefanowitsch (2020) Ch. 2. Freely available online\n\nSkim:\n\nDurrant (2023) Ch. 1.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#schedule",
    "href": "2025/sessions/day1/index.html#schedule",
    "title": "Day 1: Introduction and Foundations",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 1: Introduction\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 2: Corpus foundation\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 3: First Hands-on Activity\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#assignments",
    "href": "2025/sessions/day1/index.html#assignments",
    "title": "Day 1: Introduction and Foundations",
    "section": "Assignments",
    "text": "Assignments\n\nDue On 8/5 Tuesday: Corpus Lab Assignment 1\nComplete basic corpus search exercise",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day1/index.html#reflection",
    "href": "2025/sessions/day1/index.html#reflection",
    "title": "Day 1: Introduction and Foundations",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Day 1: Introduction and Foundations"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html",
    "href": "2025/sessions/day5/session13.html",
    "title": "Session 13",
    "section": "",
    "text": "We will explore how powerful and limited Large Language Models (LLMs) can be at the same time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#one-liner",
    "href": "2025/sessions/day5/session13.html#one-liner",
    "title": "Session 13",
    "section": "",
    "text": "We will explore how powerful and limited Large Language Models (LLMs) can be at the same time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#learning-objectives",
    "href": "2025/sessions/day5/session13.html#learning-objectives",
    "title": "Session 13",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nDescribe how LLMs are trained generally and what LLMs do to produce language.\nExplain the benefits and drawbacks of using LLMs for linguistic annotation.\nDemonstrate/discuss potential impacts of prompts on the LLMs performance on linguistic annotation.\nDesign an experiment to investigate LLMs output accuracy on a given annotation task.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#key-concepts",
    "href": "2025/sessions/day5/session13.html#key-concepts",
    "title": "Session 13",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nLarge Language Models (LLMs) and Language Generation\nPrompt engineering\nFine-tuning",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#required-readings",
    "href": "2025/sessions/day5/session13.html#required-readings",
    "title": "Session 13",
    "section": "üìö Required Readings",
    "text": "üìö Required Readings\n\nMizumoto, A., Shintani, N., Sasaki, M., & Teng, M. F. (2024). Testing the viability of ChatGPT as a companion in L2 writing accuracy assessment. Research Methods in Applied Linguistics, 3(2), 100116. https://doi.org/10.1016/j.rmal.2024.100116\n(Skim) Kim, M., & Lu, X. (2024). Exploring the potential of using ChatGPT for rhetorical move-step analysis: The impact of prompt refinement, few-shot learning, and fine-tuning. Journal of English for Academic Purposes, 71, 101422. https://doi.org/10.1016/j.jeap.2024.101422",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day5/session13.html#dive-deeper---recommended-readings",
    "title": "Session 13",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings\n\nMizumoto, A. (2025). Automated analysis of common errors in L2 learner production: Prototype web application development. Studies in Second Language Acquisition, 1‚Äì18. https://doi.org/10.1017/S0272263125100934\nYu, D., Li, L., Su, H., & Fuoli, M. (2024). Assessing the potential of LLM-assisted annotation for corpus-based pragmatics and discourse analysis: The case of apology. International Journal of Corpus Linguistics, 29(4), 534‚Äì561. https://doi.org/10.1075/ijcl.23087.yu",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#materials",
    "href": "2025/sessions/day5/session13.html#materials",
    "title": "Session 13",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\n\n\nOther resources\n\nPrompting engineering guide\nPost by Mark Davies on how similar LLM‚Äôs ‚Äúintrospections‚Äù are to corpus data",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session13.html#reflection",
    "href": "2025/sessions/day5/session13.html#reflection",
    "title": "Session 13",
    "section": "Reflection",
    "text": "Reflection\nYou can now:",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 13"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session15.html",
    "href": "2025/sessions/day5/session15.html",
    "title": "Session 15",
    "section": "",
    "text": "Final project time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 15"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session15.html#one-liner",
    "href": "2025/sessions/day5/session15.html#one-liner",
    "title": "Session 15",
    "section": "",
    "text": "Final project time.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 15"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html",
    "href": "2025/sessions/day2/session6.html",
    "title": "Session 6",
    "section": "",
    "text": "You will learn how to derive major lexical diversity and sophistication indices.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#one-liner",
    "href": "2025/sessions/day2/session6.html#one-liner",
    "title": "Session 6",
    "section": "",
    "text": "You will learn how to derive major lexical diversity and sophistication indices.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#learning-objectives",
    "href": "2025/sessions/day2/session6.html#learning-objectives",
    "title": "Session 6",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\nBy the end of this session, you will be able to:\n\n\nCompute simple lexical diversity measures using spreadsheet software\nCompute advanced lexical diversity measures using TAALED\nExplain how modern lexical diversity measures are calculated\nCalculate simple lexical sophistication measures using dedicated web application\nDescribe how lexical sophistication measures behave on a single input text.\nDiscuss benefits and drawbacks of lexical richness measures.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#key-concepts",
    "href": "2025/sessions/day2/session6.html#key-concepts",
    "title": "Session 6",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nlexical diversity\nlexical sophistication\nLearner corpus research",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#tools-used",
    "href": "2025/sessions/day2/session6.html#tools-used",
    "title": "Session 6",
    "section": "üõ†Ô∏è Tools Used",
    "text": "üõ†Ô∏è Tools Used\n\nSimple Text Analyzer: A web app created for you.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#required-readings",
    "href": "2025/sessions/day2/session6.html#required-readings",
    "title": "Session 6",
    "section": "üìö Required Readings",
    "text": "üìö Required Readings\n\n(Skim) Durrant Ch. 4 (Ignore R codes if you are not familiar)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day2/session6.html#dive-deeper---recommended-readings",
    "title": "Session 6",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings\n\nBestgen, Y. (2025). Estimating lexical diversity using the moving average type-token ratio (MATTR): Pros and cons. Research Methods in Applied Linguistics, 4(1), 100168. https://doi.org/10.1016/j.rmal.2024.100168\nKyle, K., Crossley, S., & Berger, C. (2018). The tool for the automatic analysis of lexical sophistication (TAALES): Version 2.0. Behavior Research Methods, 50(3), 1030‚Äì1046. https://doi.org/10.3758/s13428-017-0924-4\nKyle, K., Crossley, S. A., & Jarvis, S. (2021). Assessing the Validity of Lexical Diversity Indices Using Direct Judgements. Language Assessment Quarterly, 18(2), 154‚Äì170. https://doi.org/10.1080/15434303.2020.1844205\nKyle, K., Sung, H., Eguchi, M., & Zenker, F. (2024). Evaluating evidence for the reliability and validity of lexical diversity indices in L2 oral task responses. Studies in Second Language Acquisition, 46(1), 278‚Äì299. https://doi.org/10.1017/S0272263123000402",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#materials",
    "href": "2025/sessions/day2/session6.html#materials",
    "title": "Session 6",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\nüìä View Interactive Slides",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session6.html#reflection",
    "href": "2025/sessions/day2/session6.html#reflection",
    "title": "Session 6",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 6"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html",
    "href": "2025/sessions/day2/session4.html",
    "title": "Session 4",
    "section": "",
    "text": "We will overview the history and current state of lexical measurements.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#one-liner",
    "href": "2025/sessions/day2/session4.html#one-liner",
    "title": "Session 4",
    "section": "",
    "text": "We will overview the history and current state of lexical measurements.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#learning-objectives",
    "href": "2025/sessions/day2/session4.html#learning-objectives",
    "title": "Session 4",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nExplain the purposes of linguistic measures\nList commonly used lexical measures in second language acquisition research\nExplain sub-constructs of lexical richness measures\n\nLexical Diversity\nLexical Sophistication",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#key-concepts",
    "href": "2025/sessions/day2/session4.html#key-concepts",
    "title": "Session 4",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nLexical Richness\n\nDistinction between text internal vs external measures\n\nLexical Diversity\n\nType-Token Ratio\nMeasure of Textual Lexical Diversity (MTLD)\n\nLexical Sophistication\n\nFrequency\nConcreteness\nPhonological neighbors",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#required-readings",
    "href": "2025/sessions/day2/session4.html#required-readings",
    "title": "Session 4",
    "section": "üìö Required Readings",
    "text": "üìö Required Readings\n\nDurrant Ch. 3\n(Skim) Eguchi, M., & Kyle, K. (2020). Continuing to Explore the Multidimensional Nature of Lexical Sophistication: The Case of Oral Proficiency Interviews. The Modern Language Journal, 104(2), 381‚Äì400. https://doi.org/10.1111/modl.12637",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day2/session4.html#dive-deeper---recommended-readings",
    "title": "Session 4",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#materials",
    "href": "2025/sessions/day2/session4.html#materials",
    "title": "Session 4",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\nüìä View Interactive Slides",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session4.html#reflection",
    "href": "2025/sessions/day2/session4.html#reflection",
    "title": "Session 4",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 4"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html",
    "href": "2025/sessions/day3/session7.html",
    "title": "Session 7",
    "section": "",
    "text": "We will go over foundational concepts for multiword units measurements.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#one-liner",
    "href": "2025/sessions/day3/session7.html#one-liner",
    "title": "Session 7",
    "section": "",
    "text": "We will go over foundational concepts for multiword units measurements.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#learning-objectives",
    "href": "2025/sessions/day3/session7.html#learning-objectives",
    "title": "Session 7",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\nBy the end of this session, students will be able to:\n\n\nExplain different types of multiword units: collocation, n-grams, lexical bundles\nDemonstrate how major association strengths measures (t-score, Mutual Information, and LogDice) are calculated using examples",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#key-concepts",
    "href": "2025/sessions/day3/session7.html#key-concepts",
    "title": "Session 7",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nTypes of multiword units\nAssociation strengths\nThree approaches:\n\nContext window\nDependency bigram",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#required-readings",
    "href": "2025/sessions/day3/session7.html#required-readings",
    "title": "Session 7",
    "section": "üìö Required Readings",
    "text": "üìö Required Readings\n\nDurrant (2023) Ch. 7\nGablasova, D., Brezina, V., & McEnery, T. (2017). Collocations in Corpus‚ÄêBased Language Learning Research: Identifying, Comparing, and Interpreting the Evidence. Language Learning, 67(S1), 155‚Äì179. https://doi.org/10.1111/lang.12225",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day3/session7.html#dive-deeper---recommended-readings",
    "title": "Session 7",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings\n\nPaquot, M. (2019). The phraseological dimension in interlanguage complexity research. Second Language Research, 35(1), 121‚Äì145. https://doi.org/10.1177/0267658317694221\nStephanie Evert‚Äôs website on collocation measures\n\nThis webpage provides formulas to calculate various Strengths Of Association measures.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#materials",
    "href": "2025/sessions/day3/session7.html#materials",
    "title": "Session 7",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\nüìä View Interactive Slides",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session7.html#reflection",
    "href": "2025/sessions/day3/session7.html#reflection",
    "title": "Session 7",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now:\n\nDescribe major types of multiword units and how they differ from each other\n\nN-gram, Lexical Collocations, Colligations, lexical bundle\n\nDescribe benefits and drawbacks of major Strengths Of Association (SOA) measures\nDiscuss two approaches to identify collocation from the text: window-based approach and dependency-based approach.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 7"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html",
    "href": "2025/sessions/day3/session9.html",
    "title": "Session 9",
    "section": "",
    "text": "You will conduct learner corpus mini-research.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#one-liner",
    "href": "2025/sessions/day3/session9.html#one-liner",
    "title": "Session 9",
    "section": "",
    "text": "You will conduct learner corpus mini-research.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#learning-objectives",
    "href": "2025/sessions/day3/session9.html#learning-objectives",
    "title": "Session 9",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\nBy the end of this session, students will be able to:\n\n\nJustify choices of lexical richness measures to investigate a research questions\nConduct a simple statistical analysis of selected corpus on small sets of lexical measures using JASP software",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#key-concepts",
    "href": "2025/sessions/day3/session9.html#key-concepts",
    "title": "Session 9",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nLinear regression analysis (group comparison or prediction)\nIntroduction to in-class mini-project",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#required-readings",
    "href": "2025/sessions/day3/session9.html#required-readings",
    "title": "Session 9",
    "section": "üìö Required Readings",
    "text": "üìö Required Readings\n\nNo new reading! (please re-read Eguchi & Kyle, 2020)",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day3/session9.html#dive-deeper---recommended-readings",
    "title": "Session 9",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings\n\nEguchi, M., & Kyle, K. (2023). L2 collocation profiles and their relationship with vocabulary proficiency: A learner corpus approach. Journal of Second Language Writing, 60, 100975. https://doi.org/10.1016/j.jslw.2023.100975\nKyle, K., & Eguchi, M. (2023). Assessing spoken lexical and lexicogrammatical proficiency using features of word, bigram, and dependency bigram use. The Modern Language Journal, 107(2), 531‚Äì564. https://doi.org/10.1111/modl.12845\nKyle, K., Crossley, S. A., & Jarvis, S. (2021). Assessing the Validity of Lexical Diversity Indices Using Direct Judgements. Language Assessment Quarterly, 18(2), 154‚Äì170. https://doi.org/10.1080/15434303.2020.1844205\nPaquot, M. (2019). The phraseological dimension in interlanguage complexity research. Second Language Research, 35(1), 121‚Äì145. https://doi.org/10.1177/0267658317694221\nSiyanova‚ÄêChanturia, A., & Spina, S. (2020). Multi‚ÄêWord Expressions in Second Language Writing: A Large‚ÄêScale Longitudinal Learner Corpus Study. Language Learning, 70(2), 420‚Äì463. https://doi.org/10.1111/lang.12383",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#materials",
    "href": "2025/sessions/day3/session9.html#materials",
    "title": "Session 9",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\nüìä View Interactive Slides",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session9.html#reflection",
    "href": "2025/sessions/day3/session9.html#reflection",
    "title": "Session 9",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now:\n\nProvide reasons for your choice of lexical richness measures.\nConduct preliminary analysis to understand how text differ from one aother in relation to learner‚Äôs proficiency, learner groups, etc.\nBrainstorm your ideas for your final project.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 9"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html",
    "href": "2025/sessions/day4/index.html",
    "title": "Day 4: Analyzing Grammar",
    "section": "",
    "text": "Day 4 introduces grammatical analysis in corpus linguistics, exploring complexity measures and computational tools for parsing and analysis.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#overview",
    "href": "2025/sessions/day4/index.html#overview",
    "title": "Day 4: Analyzing Grammar",
    "section": "",
    "text": "Day 4 introduces grammatical analysis in corpus linguistics, exploring complexity measures and computational tools for parsing and analysis.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#key-concepts",
    "href": "2025/sessions/day4/index.html#key-concepts",
    "title": "Day 4: Analyzing Grammar",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nGrammatical complexity\nPredictive measures versus Descriptive measures\nPOS tagging\nDependency parsing\nPrecision, Recall, and F1 score\nSyntactic sophistication and fine-grained measures",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#preparation",
    "href": "2025/sessions/day4/index.html#preparation",
    "title": "Day 4: Analyzing Grammar",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 4:\n\nRead:\n\nDurrant Ch. 5\nKyle, K., & Crossley, S. A. (2018). Measuring Syntactic Complexity in L2 Writing Using Fine‚ÄêGrained Clausal and Phrasal Indices. The Modern Language Journal, 102(2), 333‚Äì349.\n\nSkim:\n\nDurrant Ch. 6 (Ignore R codes if you are not familiar)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#schedule",
    "href": "2025/sessions/day4/index.html#schedule",
    "title": "Day 4: Analyzing Grammar",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 10: Grammar ‚Äî Overview\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 11: POS Tagging and Parsing\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 12: Linguistic Complexity Analysis\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#assignments",
    "href": "2025/sessions/day4/index.html#assignments",
    "title": "Day 4: Analyzing Grammar",
    "section": "Assignments",
    "text": "Assignments\n\nDue 8/8 (Fri): Corpus Lab Assignment 4\nComplete grammatical analysis exercises using Python notebooks and TagAnt",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/index.html#reflection",
    "href": "2025/sessions/day4/index.html#reflection",
    "title": "Day 4: Analyzing Grammar",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Day 4: Analyzing Grammar"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html",
    "href": "2025/sessions/day4/session11.html",
    "title": "Session 11",
    "section": "",
    "text": "You will learn how to conduct POS tagging and dependency parsing.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#one-liner",
    "href": "2025/sessions/day4/session11.html#one-liner",
    "title": "Session 11",
    "section": "",
    "text": "You will learn how to conduct POS tagging and dependency parsing.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#learning-objectives",
    "href": "2025/sessions/day4/session11.html#learning-objectives",
    "title": "Session 11",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nUnderstand NLP tasks such as POS tagging and dependency parsing\nUnderstand how automated parsing works\nConduct POS tagging using spaCy library in Python (through Google Colab)\nConduct Dependency parsing using spaCy library in Python (through Google Colab)\nConduct multi-lingual Part-Of-Speech (POS) tagging using TagAnt",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#key-concepts",
    "href": "2025/sessions/day4/session11.html#key-concepts",
    "title": "Session 11",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nPOS tagging\nDependency parsing\nPrecision, Recall, and F1 score",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#required-readings",
    "href": "2025/sessions/day4/session11.html#required-readings",
    "title": "Session 11",
    "section": "üìö Required Readings",
    "text": "üìö Required Readings\n\nSkim Durrant Ch 6 (Ignore R codes if you are not familiar)",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#recommended-readings",
    "href": "2025/sessions/day4/session11.html#recommended-readings",
    "title": "Session 11",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nKyle, K., & Eguchi, M. (2024). Evaluating NLP models with written and spoken L2 samples. Research Methods in Applied Linguistics, 3(2), 100120. https://doi.org/10.1016/j.rmal.2024.100120",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#notes",
    "href": "2025/sessions/day4/session11.html#notes",
    "title": "Session 11",
    "section": "üìù Notes",
    "text": "üìù Notes\n\nAll the python codes are prepared by the instructor and shared with the students. This session does not require ability to code.\nThe decision to use Python programming language rather than already available software is based on consideration that there is very little tools which provide stable access to the language analysis described here.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#tools-used",
    "href": "2025/sessions/day4/session11.html#tools-used",
    "title": "Session 11",
    "section": "üõ†Ô∏è Tools Used",
    "text": "üõ†Ô∏è Tools Used\n\nTagAnt\nSimple Text Analyzer: A web app created for you.\nGoogle Colaboratory",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day4/session11.html#dive-deeper---recommended-readings",
    "title": "Session 11",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#materials",
    "href": "2025/sessions/day4/session11.html#materials",
    "title": "Session 11",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session11.html#reflection",
    "href": "2025/sessions/day4/session11.html#reflection",
    "title": "Session 11",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 11"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html",
    "href": "2025/syllabus/index.html",
    "title": "Course Syllabus",
    "section": "",
    "text": "Course Title: Linguistic Data Analysis I\nCredits: 2\nFormat: Intensive 5-day course (15 sessions)\nLanguage: English\n Classroom: 113 Lecture room",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#course-information",
    "href": "2025/syllabus/index.html#course-information",
    "title": "Course Syllabus",
    "section": "",
    "text": "Course Title: Linguistic Data Analysis I\nCredits: 2\nFormat: Intensive 5-day course (15 sessions)\nLanguage: English\n Classroom: 113 Lecture room",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#instructor-information",
    "href": "2025/syllabus/index.html#instructor-information",
    "title": "Course Syllabus",
    "section": "Instructor Information",
    "text": "Instructor Information\nInstructor: Masaki Eguchi, Ph.D.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#course-description",
    "href": "2025/syllabus/index.html#course-description",
    "title": "Course Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces the foundations of corpus linguistics and the analysis of learner language through corpus linguistic approaches. It covers key concepts in corpus linguistics, including what corpora are, how they are used to answer (applied) linguistic research questions, and how to design corpus-based analyses to address substantive research questions in second language research. The primary language of analysis in this course is English, but students are encouraged to apply the concepts introduced to the languages they work with in their own research.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#learning-objectives",
    "href": "2025/syllabus/index.html#learning-objectives",
    "title": "Course Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this course, students will be able to:\n\nExplain what corpus linguistics is and how corpus linguistics can help learn linguistic phenomena\nSearch for and select available corpora relevant to their own research\nDiscuss design issues related to language corpora for specific research purposes\nApply introductory corpus linguistic analyses (e.g., frequency analysis, concordancing, POS tagging) to preprocessed corpora\nEvaluate the benefits and drawbacks of a corpus linguistic approach to linguistic analysis",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#course-components",
    "href": "2025/syllabus/index.html#course-components",
    "title": "Course Syllabus",
    "section": "Course Components",
    "text": "Course Components\n\nLectures and tutorials\nDaily Hands-on activities\nMini-corpus labs\nFinal project\n\n\n\n\n\n\n\nNavigation\n\n\n\n\nüìÖ Detailed Schedule",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#required-materials",
    "href": "2025/syllabus/index.html#required-materials",
    "title": "Course Syllabus",
    "section": "Required Materials",
    "text": "Required Materials\n\nTextbook\n\nDurrant, P. (2023). Corpus linguistics for writing development: A guide for research. Routledge. https://doi.org/10.4324/9781003152682\nStefanowitsch, A. (2020). Corpus linguistics: A guide to the methodology. Zenodo. https://doi.org/10.5281/ZENODO.3735822 (This is an open source textbook, so it‚Äôs freely available online)\n\nOther required/Optional readings are provided through Google Classroom.\n\n\nSoftwares (Free)\n\nWeb application for simple text analyses\n\nSimple Text Analyzer: A web app created for you.\n\n\n\nConcordancing Software\n\nAntConc: Corpus analysis toolkit for Concordancing\n\n\n\nLexical Profiling Software\n\nAntWordProfiler: Corpus analysis toolkit for Lexical Profiling\nLexTutor: Corpus analysis toolkit online\n\n\n\nStatistics\n\nJASP: Statistical analysis software\n\n\n\nOthers\n\nGoogle Colaboratory: Follow the instruction here to enable the tool.\nText Editor: VS Code recommended",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#assignments-and-grading",
    "href": "2025/syllabus/index.html#assignments-and-grading",
    "title": "Course Syllabus",
    "section": "Assignments and Grading",
    "text": "Assignments and Grading\nYou can find detailed information about each assignment in assignments page (under construction).\nWe have two possible choices for the grade distribution for this course. I will explain the details of each choice in the first day of the course. The class as a whole decides which one we will incorporate in this course. Once decided, all students will follow the decided plan.\n\nGrade Distribution ‚Äì Option A\n\nFor option A, pairs of students will develop their own mini-research and present their final project on the final day.\n\n\n\n\nAssignment\n\nPercent\n\n\n\n\nCorpus Lab Assignments\n(4 √ó 15%)\n60%\n\n\nClass Participation\n\n20%\n\n\nFinal Project\n\n20%\n\n\n\n\n\nGrade Distribution ‚Äì Option B\n\nFor option B, pairs of students will present on Corpus Lab assignments on the final day.\n\n\n\n\nAssignment\n\nPercent\n\n\n\n\nCorpus Lab Assignments\n(4 √ó 15%)\n60%\n\n\nClass Participation\n\n20%\n\n\nFinal Presentation on Selected Corpus Lab Assignment\n\n20%\n\n\n\n\n\nGrading Scale\nWe follow the grading system at Tohoku University.\n\n\n\nGrade\nRange\nGrade Point\n\n\n\n\nAA\n100-90%\n4.0\n\n\nA\n89-80%\n3.0\n\n\nB\n79-70%\n2.0\n\n\nC\n69-60%\n1.0\n\n\nD\n59-0%\n0.0",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#daily-structure",
    "href": "2025/syllabus/index.html#daily-structure",
    "title": "Course Syllabus",
    "section": "Daily Structure",
    "text": "Daily Structure\nEach day follows this general pattern:\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 1\n\n\n12:00-13:00\nLunch break\n\n\n13:00-14:30\nSession 2\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 3\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#attendance-policy",
    "href": "2025/syllabus/index.html#attendance-policy",
    "title": "Course Syllabus",
    "section": "Attendance Policy",
    "text": "Attendance Policy\n\nDue to the intensive nature of the course, attendance and participation are crucial to your success in this course.\nHowever, in case of emergency, do not hesitate to reach out to the instructor for possible accomodation. I may be able to accommodate depending on the situation.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#assignment-submission",
    "href": "2025/syllabus/index.html#assignment-submission",
    "title": "Course Syllabus",
    "section": "Assignment Submission",
    "text": "Assignment Submission\n\nDeadlines\n\nAll assignments are due at 10:30 AM on the specified day\nLate submissions will receive a 10% penalty per day\nExtensions may be granted for documented emergencies.\n\n\n\nSubmission Format\n\nSubmit all assignments via the course management system (Google Classroom)\nUse the provided templates when available\nFile naming convention: LastName_Assignment#.ext\nAcceptable formats: .docx, .pdf, .ipynb (for Python notebooks)\n\n\n\nCollaboration\n\nAs this is a very intensive course, collaboration is encouraged to gain most out of the time we spend. I will help each of you during the class time and office hours, but I encourage you to also help each other in:\n\nsetting up the tools\nrecalling class materials\nthinking about the approaches to corpus lab\n\nHowever, you MUST write your own write-up of the assignments, meaning that you MUST make outlines, draft, and finalize the written submission by yourselves.\n\n\n\nPlagiarism\nYou must present your own write-up. Do not copy each other‚Äôs work or ask someone to write for you.\nAbout the use of AI tools such as ChatGPT, do not use it to make drafts. You can use it to polish your language.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#technology-policy",
    "href": "2025/syllabus/index.html#technology-policy",
    "title": "Course Syllabus",
    "section": "Technology Policy",
    "text": "Technology Policy\n\nRequired Technology\n\nBring a laptop to every session.\nEnsure all required software is installed.\n\n\n\nClassroom Etiquette\n\nLaptops should be used for course activities only",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#communication",
    "href": "2025/syllabus/index.html#communication",
    "title": "Course Syllabus",
    "section": "Communication",
    "text": "Communication\n\nCourse related communications will happen via Google Classroom.\n\n\nCourse Announcements\n\nCourse announcements are made through Google Classroom.\n\n\n\nMaterials Sharing\n\nMaterials (e.g., slides) are shared through this website.",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/syllabus/index.html#accommodations",
    "href": "2025/syllabus/index.html#accommodations",
    "title": "Course Syllabus",
    "section": "Accommodations",
    "text": "Accommodations",
    "crumbs": [
      "Syllabus",
      "Course Syllabus"
    ]
  },
  {
    "objectID": "2025/slides/session-6.html#learning-objectives",
    "href": "2025/slides/session-6.html#learning-objectives",
    "title": "Session 6: Hands-on activity #3",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nBy the end of this session, you will be able to:\n\n\n\nCompute simple lexical diversity measures using spreadsheet software\nCompute advanced lexical diversity measures using TAALED\nExplain how modern lexical diversity measures are calculated\nCalculate simple lexical sophistication measures using dedicated web application\nDescribe how lexical sophistication measures behave on a single input text.\nDiscuss benefits and drawbacks of lexical richness measures."
  },
  {
    "objectID": "2025/slides/session-6.html#terminology",
    "href": "2025/slides/session-6.html#terminology",
    "title": "Session 6: Hands-on activity #3",
    "section": "Terminology",
    "text": "Terminology\nIn this presentation, I will use the following terms:\n\nInput text: The text you want to analyze (e.g., learner produce text)."
  },
  {
    "objectID": "2025/slides/session-6.html#lexical-diversity-ld",
    "href": "2025/slides/session-6.html#lexical-diversity-ld",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lexical Diversity (LD)",
    "text": "Lexical Diversity (LD)\n\nLexical Diversity is computed internally to text.\ne.g., Type-Token Ratio:\n\nCount the number of unique word (Type) in the input text\nCount the number of total word (Token) in the input text\nDevide Type by token."
  },
  {
    "objectID": "2025/slides/session-6.html#relationships-between-cttr-and-year",
    "href": "2025/slides/session-6.html#relationships-between-cttr-and-year",
    "title": "Session 6: Hands-on activity #3",
    "section": "Relationships between CTTR and Year",
    "text": "Relationships between CTTR and Year\n\nDurrant (2023) introduced:\n\n\\(CTTR = {nType \\over \\sqrt{2 * nToken}}\\)"
  },
  {
    "objectID": "2025/slides/session-6.html#new-slide",
    "href": "2025/slides/session-6.html#new-slide",
    "title": "Session 6: Hands-on activity #3",
    "section": "new slide",
    "text": "new slide\n\nFigure 4.19"
  },
  {
    "objectID": "2025/slides/session-6.html#lexical-diversity",
    "href": "2025/slides/session-6.html#lexical-diversity",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lexical diversity",
    "text": "Lexical diversity\n\nLearner‚Äôs proficiency has positive relations with lexical diversity\nOther features such as genres can also mediate the relationships"
  },
  {
    "objectID": "2025/slides/session-6.html#lexical-sophistication-ls",
    "href": "2025/slides/session-6.html#lexical-sophistication-ls",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lexical sophistication (LS)",
    "text": "Lexical sophistication (LS)\n\nLexical Sophistication requires external resources to compute.\n\ne.g., Corpus frequency lists, SemD score, etc.\n\nTo derive an LS index:\n\nCompile a reference word list\nFor each word in the input text, retrieve index score from the reference list\nAverage the scores (out of item awarded the score)"
  },
  {
    "objectID": "2025/slides/session-6.html#instruction-for-task-2",
    "href": "2025/slides/session-6.html#instruction-for-task-2",
    "title": "Session 6: Hands-on activity #3",
    "section": "Instruction for Task 2",
    "text": "Instruction for Task 2\n\nYour task is to replicate Durrant‚Äôs Figure 4.19 with two different indices ‚Äì MTLD and MATTR.\n\n\nFigure 4.19"
  },
  {
    "objectID": "2025/slides/session-6.html#simple-text-example",
    "href": "2025/slides/session-6.html#simple-text-example",
    "title": "Session 6: Hands-on activity #3",
    "section": "Simple Text Example",
    "text": "Simple Text Example\nCound the type and token of the following texts.\n\n\n\n\n\n\n\n\nID\nText\n\n\n\n\nText 1\n‚ÄúThe dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.‚Äù\n\n\nText 2\n‚ÄúA curious fox trotted briskly through the meadow, leaping over mossy logs, sniffing wildflowers, and vanishing into golden twilight.‚Äù\n\n\n\n\nNote : Texts were generated by GPT for illustration purposes."
  },
  {
    "objectID": "2025/slides/session-6.html#simple-text-example-1",
    "href": "2025/slides/session-6.html#simple-text-example-1",
    "title": "Session 6: Hands-on activity #3",
    "section": "Simple Text Example",
    "text": "Simple Text Example\n\n\n\n\n\n\n\n\n\n\nID\nText\nToken\nType\n\n\n\n\nText 1\n‚ÄúThe dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.‚Äù\n19\n8\n\n\nText 2\n‚ÄúA curious fox trotted briskly through the meadow, leaping over mossy logs, sniffing wildflowers, and vanishing into golden twilight.‚Äù\n19\n19\n\n\n\n\nNote : Texts were generated by GPT for illustration purposes."
  },
  {
    "objectID": "2025/slides/session-6.html#impact-of-text-lengths",
    "href": "2025/slides/session-6.html#impact-of-text-lengths",
    "title": "Session 6: Hands-on activity #3",
    "section": "Impact of Text lengths",
    "text": "Impact of Text lengths\n\n\n\n\n\n\n\n\n\n\nID\nText\nToken\nType\n\n\n\n\nText 1a\n‚ÄúThe dog ran. The dog jumped. The dog played. The dog barked. The dog ran again and jumped again.‚Äù\n19\n8\n\n\nText 1b\n‚ÄúThe dog ran. The dog jumped. The dog barked. The dog played. The dog ran quickly. The dog jumped so high. The dog barked very loudly. The dog played, sat, and rolled. The dog sneezed. The dog ate the food.‚Äù\n40\n18\n\n\nText 1c\n‚ÄúThe parrot squawked loudly. The parrot chirped again. A toucan perched nearby. The parrot fluttered. Wings flapped softly. The parrot chirped again. Feathers shimmered under sunlight. The crow cawed. The parrot glided low. The air shimmered. The owl blinked slowly. The parrot perched again. The owl blinked slowly. The parrot shrieked. The parrot chirped nearby again. The parrot squawked again.‚Äù\n60\n27\n\n\n\nNote : Texts were generated by GPT for illustration purposes."
  },
  {
    "objectID": "2025/slides/session-6.html#lets-calculate-some-classical-lexical-diversity-indices",
    "href": "2025/slides/session-6.html#lets-calculate-some-classical-lexical-diversity-indices",
    "title": "Session 6: Hands-on activity #3",
    "section": "Let‚Äôs calculate some classical Lexical Diversity indices",
    "text": "Let‚Äôs calculate some classical Lexical Diversity indices\n\nOpen Google Spreadsheet for lexical-diversity-hand-calculation\nCopy the spreadsheet to your Drive before working on it.\nCalculate the lexical diversity indices on the next page."
  },
  {
    "objectID": "2025/slides/session-6.html#speadsheet-looks-like-this",
    "href": "2025/slides/session-6.html#speadsheet-looks-like-this",
    "title": "Session 6: Hands-on activity #3",
    "section": "Speadsheet looks like this",
    "text": "Speadsheet looks like this\n\nGoogle Spreadsheet"
  },
  {
    "objectID": "2025/slides/session-6.html#these-are-real-example-from-icnale",
    "href": "2025/slides/session-6.html#these-are-real-example-from-icnale",
    "title": "Session 6: Hands-on activity #3",
    "section": "These are real example from ICNALE",
    "text": "These are real example from ICNALE\n\nDownload ICNALE data here"
  },
  {
    "objectID": "2025/slides/session-6.html#parallel-analysis",
    "href": "2025/slides/session-6.html#parallel-analysis",
    "title": "Session 6: Hands-on activity #3",
    "section": "Parallel Analysis",
    "text": "Parallel Analysis\n\n\nThrough this worksheet, we will look at how Kyle et al.¬†showcased the impact of text lengths on lexical sophistication indices.\nParallel analysis is a technique to investigate the impact of text length\n\nChunking texts into smaller lengths (20, 40, 60 etc.)\nCalculate index for each sliding window with same lengths\nAverage them into a single value\n\nbecause the original texts are the same, the values should NOT change due to different text lengths."
  },
  {
    "objectID": "2025/slides/session-6.html#some-classic-lexical-diversity",
    "href": "2025/slides/session-6.html#some-classic-lexical-diversity",
    "title": "Session 6: Hands-on activity #3",
    "section": "Some classic lexical diversity",
    "text": "Some classic lexical diversity\nWe calculate this for illustration but NEVER use these in your study.\n\n\\(TTR = {nType \\over nToken}\\)\n\\(RootTTR = {nType \\over \\sqrt{nToken}}\\)\n\\(LogTTR = {\\log(nType) \\over \\log(nToken)}\\)\n\\(Maas = {\\log(nTokens) - \\log(nTypes) \\over \\log(nToken)^2}\\)\n\n\n\nuse the following\n\nHigh: GRA_PTJ0_124_ORIG.txt\nMid diversity: GRA_PTJ0_070_ORIG.txt\nLow diversity: GRA_PTJ0_112_ORIG.txt"
  },
  {
    "objectID": "2025/slides/session-6.html#summary",
    "href": "2025/slides/session-6.html#summary",
    "title": "Session 6: Hands-on activity #3",
    "section": "Summary",
    "text": "Summary\n\nThrough worksheet example of parallel analysis, we demonstrated that the old indices of LD is flawed."
  },
  {
    "objectID": "2025/slides/session-6.html#what-should-we-actually-use-then",
    "href": "2025/slides/session-6.html#what-should-we-actually-use-then",
    "title": "Session 6: Hands-on activity #3",
    "section": "What should we actually use then?",
    "text": "What should we actually use then?\n\nThe Measure of Textual Lexical Diversity (MTLD)\nMoving-Average Type Token Ratio (MATTR)\n\n‚Üí These are shown as more robust indices of LD."
  },
  {
    "objectID": "2025/slides/session-6.html#operationalization-of-mtld-and-mattr",
    "href": "2025/slides/session-6.html#operationalization-of-mtld-and-mattr",
    "title": "Session 6: Hands-on activity #3",
    "section": "Operationalization of MTLD and MATTR",
    "text": "Operationalization of MTLD and MATTR\nIn pair, recall how MTLD and MATTR is operationalized."
  },
  {
    "objectID": "2025/slides/session-6.html#using-taaled-desktop-version",
    "href": "2025/slides/session-6.html#using-taaled-desktop-version",
    "title": "Session 6: Hands-on activity #3",
    "section": "Using TAALED desktop version",
    "text": "Using TAALED desktop version\n\nWe can use Tool for the Automatic Analysis of Lexical Diversity (TAALED)\nDownload it to your computer and we will use it to compute modern LD measures\n\n\nTAALED"
  },
  {
    "objectID": "2025/slides/session-6.html#setting-up",
    "href": "2025/slides/session-6.html#setting-up",
    "title": "Session 6: Hands-on activity #3",
    "section": "Setting up",
    "text": "Setting up\n\nClick the software icon after download\nFor mac users, the system will issue warning, you must follow the following step:\n\nGo to setting and select Privacy & Security\nIf you have already attempted to open the software, there will be Open Anyway button.\nClick Open Anyway and that will allow Mac to open the software."
  },
  {
    "objectID": "2025/slides/session-6.html#selecting-indices",
    "href": "2025/slides/session-6.html#selecting-indices",
    "title": "Session 6: Hands-on activity #3",
    "section": "Selecting indices",
    "text": "Selecting indices\nYou can then wait for the TAALED app to start up.\n\nTAALED"
  },
  {
    "objectID": "2025/slides/session-6.html#options",
    "href": "2025/slides/session-6.html#options",
    "title": "Session 6: Hands-on activity #3",
    "section": "Options",
    "text": "Options\nWord analysis options\n\nAll words: Conduct analysis including all words.\nContent words: Conduct analysis with content words only.\nFunction words: Conduct analysis with function words only."
  },
  {
    "objectID": "2025/slides/session-6.html#options-1",
    "href": "2025/slides/session-6.html#options-1",
    "title": "Session 6: Hands-on activity #3",
    "section": "Options",
    "text": "Options\nIndex selection\nSelect the indices you need in the results. Three variants of MTLD are available.\n\nMTLD Original:\nMTLD MA Bi: Moving Average Bidirectional\nMTLD MA Wrap: If there is words left in the final factor, come back to the first part and complete the analysis."
  },
  {
    "objectID": "2025/slides/session-6.html#options-2",
    "href": "2025/slides/session-6.html#options-2",
    "title": "Session 6: Hands-on activity #3",
    "section": "Options",
    "text": "Options\nInput and output options\n\nYou can choose the input folder by selection\n\nOutput option\n\nTicking the Individual Item Output button allows you to have POS analysis\n\nparent_cw_nn\nand_fw\nteacher_cw_nn\ndisagree_cw_vb\nthat_fw"
  },
  {
    "objectID": "2025/slides/session-6.html#run-the-analysis",
    "href": "2025/slides/session-6.html#run-the-analysis",
    "title": "Session 6: Hands-on activity #3",
    "section": "Run the analysis",
    "text": "Run the analysis\n\nPress Process Texts and wait the following display.\n\n\nAnalysis complete"
  },
  {
    "objectID": "2025/slides/session-6.html#lets-take-a-look-at-the-csv-file",
    "href": "2025/slides/session-6.html#lets-take-a-look-at-the-csv-file",
    "title": "Session 6: Hands-on activity #3",
    "section": "Let‚Äôs take a look at the csv file",
    "text": "Let‚Äôs take a look at the csv file\n\nWhat‚Äôs CSV?\n\nCSV (Comma Separated Values) file is a file extension like others (txt, docx).\nIt allows table like representation of data (like excel) separated by comma.\n\n\nThe Raw data (if you open it with text editor) should look like the following:\nfilename,basic_ntokens,basic_ntypes,basic_ncontent_tokens,basic_ncontent_types,basic_nfunction_tokens,basic_nfunction_types,lexical_density_types,lexical_density_tokens,maas_ttr_aw,mattr50_aw,hdd42_aw,mtld_original_aw,mtld_ma_bi_aw,mtld_ma_wrap_aw\nW_CHN_PTJ0_004_B1_2_ORIG.txt,267,124,135,75,132,49,0.6048387096774194,0.5056179775280899,0.056571333957257205,0.7793577981651377,0.7981161136859327,68.68659119235562,65.84622666144406,61.50561797752809\nW_JPN_SMK0_015_B1_2_ORIG.txt,302,138,129,82,173,56,0.5942028985507246,0.4271523178807947,0.05530143602594381,0.777865612648221,0.7974803670481457,68.38677597714803,67.6645170484911,65.22185430463576"
  },
  {
    "objectID": "2025/slides/session-6.html#opening-csv-file-in-a-spreadsheet-software",
    "href": "2025/slides/session-6.html#opening-csv-file-in-a-spreadsheet-software",
    "title": "Session 6: Hands-on activity #3",
    "section": "Opening csv file in a spreadsheet software",
    "text": "Opening csv file in a spreadsheet software\n\nYou can open csv file in Excel (or any other spreadsheet software)"
  },
  {
    "objectID": "2025/slides/session-6.html#plotting-the-gig-corpus",
    "href": "2025/slides/session-6.html#plotting-the-gig-corpus",
    "title": "Session 6: Hands-on activity #3",
    "section": "Plotting the GiG corpus",
    "text": "Plotting the GiG corpus\nThe goal of the Corpus Lab, Task 2, is to replicate analysis on GiG.\nYou will need to have access to both metadata file.\nThe corpus data is here."
  },
  {
    "objectID": "2025/slides/session-6.html#gig-meta-data",
    "href": "2025/slides/session-6.html#gig-meta-data",
    "title": "Session 6: Hands-on activity #3",
    "section": "GiG meta data",
    "text": "GiG meta data\n\nGiG metadata documents the necessary data to use for plotting\n\nYear Group (X-axis in Figure 4.19)\nGenre (grouping variable in Figure 4.19)\n\n\n\nGiG Metadata"
  },
  {
    "objectID": "2025/slides/session-6.html#using-corpus-data-visualizer-for-plotting",
    "href": "2025/slides/session-6.html#using-corpus-data-visualizer-for-plotting",
    "title": "Session 6: Hands-on activity #3",
    "section": "Using Corpus Data Visualizer for plotting",
    "text": "Using Corpus Data Visualizer for plotting\nSee in-class demonstration."
  },
  {
    "objectID": "2025/slides/session-6.html#writing-up-research-question-hypothesis-and-results",
    "href": "2025/slides/session-6.html#writing-up-research-question-hypothesis-and-results",
    "title": "Session 6: Hands-on activity #3",
    "section": "Writing up research question, hypothesis, and results",
    "text": "Writing up research question, hypothesis, and results\n\n\nResearch question?\nHypothesis: Write your own.\nResults: Write your own.\n\n\n\n\n\n\n\nSuccess Criteria\n\n\nYour submission ‚Ä¶\n\nincludes a spreatsheet that contains lexical diversity (i.e., TTR, Root TTR, Log TTR, MAAS) scores for the sample texts\nprovides two sets of plots that describe trends of lexical diversity across year groups and genre.\nprovides 200-300 word replication report on lexical diversity trends in GiG corpus, presented as Figure 4.19 in Durrant (2023)."
  },
  {
    "objectID": "2025/slides/session-6.html#questions",
    "href": "2025/slides/session-6.html#questions",
    "title": "Session 6: Hands-on activity #3",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "2025/slides/session-6.html#lexical-sophistication",
    "href": "2025/slides/session-6.html#lexical-sophistication",
    "title": "Session 6: Hands-on activity #3",
    "section": "Lexical sophistication",
    "text": "Lexical sophistication\nThere are a number of lexical sophistication measures for English (+ 300).\n\n12 categories of measures (Eguchi & Kyle, 2020)\n\nFrequency\nRange\nPsycholinguistic Norm\nHypernymy\nN-gram Frequency/Range/SOA\netc."
  },
  {
    "objectID": "2025/slides/session-6.html#typical-operationalization",
    "href": "2025/slides/session-6.html#typical-operationalization",
    "title": "Session 6: Hands-on activity #3",
    "section": "Typical operationalization",
    "text": "Typical operationalization\n\nTypically, lexical sophistication (LS) is calculated as an average:\nTypical LS score = \\[Total \\; LS \\; score \\over nToken \\; with \\; LS \\; score\\]\nAverage is just a convenient choice."
  },
  {
    "objectID": "2025/slides/session-6.html#using-an-emulation-of-taales",
    "href": "2025/slides/session-6.html#using-an-emulation-of-taales",
    "title": "Session 6: Hands-on activity #3",
    "section": "Using an emulation of TAALES",
    "text": "Using an emulation of TAALES\n\nSince the desktop version of TAALES is unstable these days, we will use a simple web application.\nLet‚Äôs use simple text analyzer"
  },
  {
    "objectID": "2025/slides/session-6.html#simple-text-analyzer",
    "href": "2025/slides/session-6.html#simple-text-analyzer",
    "title": "Session 6: Hands-on activity #3",
    "section": "Simple Text Analyzer",
    "text": "Simple Text Analyzer\n!"
  },
  {
    "objectID": "2025/slides/session-6.html#analyzing-single-text-in-simple-text-analzer",
    "href": "2025/slides/session-6.html#analyzing-single-text-in-simple-text-analzer",
    "title": "Session 6: Hands-on activity #3",
    "section": "Analyzing single-text in simple text analzer",
    "text": "Analyzing single-text in simple text analzer\n!"
  },
  {
    "objectID": "2025/slides/session-6.html#analyzing-single-text-in-simple-text-analzer-1",
    "href": "2025/slides/session-6.html#analyzing-single-text-in-simple-text-analzer-1",
    "title": "Session 6: Hands-on activity #3",
    "section": "Analyzing single-text in simple text analzer",
    "text": "Analyzing single-text in simple text analzer\n\nChoose single text mode\nChoose Paste text\nChoose Reference list (e.g., COCA Spoken Frequency)\nChoose if you apply log transformation to frequency values\nHit Analyze Text"
  },
  {
    "objectID": "2025/slides/session-6.html#check-impact-of-log-transformation",
    "href": "2025/slides/session-6.html#check-impact-of-log-transformation",
    "title": "Session 6: Hands-on activity #3",
    "section": "Check impact of log transformation",
    "text": "Check impact of log transformation\n\nHow would you explain the results of the log transformation?\nWhich one do you recommend?\n\n\n\n\n\n\nNot log Transformed\n\n\n\n\n\n\nLog Transformed"
  },
  {
    "objectID": "2025/slides/session-6.html#on-log-transformation",
    "href": "2025/slides/session-6.html#on-log-transformation",
    "title": "Session 6: Hands-on activity #3",
    "section": "On log transformation",
    "text": "On log transformation\n\nLog transformation allows the text-internal frequency distribution to approach normal distribution.\nThis is generally recommended because the ‚Äúmean‚Äù score behaves well on normally distributed data."
  },
  {
    "objectID": "2025/slides/session-6.html#uploading-your-frequency-list",
    "href": "2025/slides/session-6.html#uploading-your-frequency-list",
    "title": "Session 6: Hands-on activity #3",
    "section": "Uploading your frequency list",
    "text": "Uploading your frequency list\nThe simple text analyzer can accept a word frequency list.\nNow let‚Äôs upload BROWN frequency list."
  },
  {
    "objectID": "2025/slides/session-6.html#comparing-lexical-characteristics-of-two-texts",
    "href": "2025/slides/session-6.html#comparing-lexical-characteristics-of-two-texts",
    "title": "Session 6: Hands-on activity #3",
    "section": "Comparing lexical characteristics of two texts",
    "text": "Comparing lexical characteristics of two texts\n\nGoals\n\nCompare and contrast two texts along with several lexical diversity metrics\nObserve differences in single-word and multiword sophistication\n\nData\n\nChoose two texts from the ICNALE GRA\nIf you are unsure, use choose from the following three files:\n\nGRA_PTJ0_124_ORIG.txt\nGRA_PTJ0_070_ORIG.txt\nGRA_PTJ0_112_ORIG.txt"
  },
  {
    "objectID": "2025/slides/session-6.html#step-1-qualitatively-compare-two-files",
    "href": "2025/slides/session-6.html#step-1-qualitatively-compare-two-files",
    "title": "Session 6: Hands-on activity #3",
    "section": "Step 1: Qualitatively compare two files",
    "text": "Step 1: Qualitatively compare two files\n\nBefore we actually obtain lexical sophistication measures, compare two texts in terms of their lexical use.\nIn pairs, describe the strengths and weakeness in vocabulary use."
  },
  {
    "objectID": "2025/slides/session-6.html#step-2-hypothesis",
    "href": "2025/slides/session-6.html#step-2-hypothesis",
    "title": "Session 6: Hands-on activity #3",
    "section": "Step 2: Hypothesis",
    "text": "Step 2: Hypothesis\n\nIn what way is one text more lexically sophisticated than the other?\n\nTry to come up with characteristics that describe the quality words to describe them"
  },
  {
    "objectID": "2025/slides/session-6.html#step-3-pick-two-or-three-lexical-sophistication-variables",
    "href": "2025/slides/session-6.html#step-3-pick-two-or-three-lexical-sophistication-variables",
    "title": "Session 6: Hands-on activity #3",
    "section": "Step 3: Pick two or three lexical sophistication variables",
    "text": "Step 3: Pick two or three lexical sophistication variables\nEnter the text into analyzer\nWe can also compare two texts in simple text analyzer.\n!\nPick appropriate lexical sophistication indices"
  },
  {
    "objectID": "2025/slides/session-6.html#step-4-run-analyses",
    "href": "2025/slides/session-6.html#step-4-run-analyses",
    "title": "Session 6: Hands-on activity #3",
    "section": "Step 4: Run analyses",
    "text": "Step 4: Run analyses\nPlots that compares two lists\n!"
  },
  {
    "objectID": "2025/slides/session-6.html#step-5-interpret-the-findings",
    "href": "2025/slides/session-6.html#step-5-interpret-the-findings",
    "title": "Session 6: Hands-on activity #3",
    "section": "Step 5: Interpret the findings",
    "text": "Step 5: Interpret the findings\n\nLet‚Äôs discuss how the two text differ in lexical use from one another.\nUse the tables with token information and visualization to (dis)confirm your hypothesis\n\n\n\n\n\n\n\nSuccess Criteria\n\n\nYour submission ‚Ä¶\n\nincludes plots from one frequency index and one other type of index\nprovides desciption of how two texts differ in terms of the selected lexical sophistication indices."
  },
  {
    "objectID": "2025/slides/session-6.html#corpus-lab-2-submission",
    "href": "2025/slides/session-6.html#corpus-lab-2-submission",
    "title": "Session 6: Hands-on activity #3",
    "section": "Corpus Lab 2: Submission",
    "text": "Corpus Lab 2: Submission\n\nJapanese Word Frequency List and small write-up (5 points)\nReplication of Durrant‚Äôs analysis from Figure 4.19 (5 points)\n\nresearch question,\nhypothesis,\nplots, and\nresults\n\nComparison of two texts in terms of lexical sophistication (5 points)"
  },
  {
    "objectID": "2025/slides/session-5.html#learning-objectives",
    "href": "2025/slides/session-5.html#learning-objectives",
    "title": "Session 5: Hands-on activity #2",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\nBy the end of this session, you will be able to:\n\n\n\nCompute frequency of a single-word lexical item in reference corpora\nDerive vocabulary frequency list using concordancing software (e.g., AntConc)\nApply tokenization on the Japanese language corpus for frequency analysis\nConduct Lexical Profiling using a web-application or desktop application (e.g., AntWordProfiler)"
  },
  {
    "objectID": "2025/slides/session-5.html#tasks",
    "href": "2025/slides/session-5.html#tasks",
    "title": "Session 5: Hands-on activity #2",
    "section": "Tasks",
    "text": "Tasks\n\nLoading a corpus to AntConc\nCreating a frequency list\nVisualize frequency distributions\nTokenize non-English language\n(If time left) Vocabulary profiling"
  },
  {
    "objectID": "2025/slides/session-5.html#open-antconc",
    "href": "2025/slides/session-5.html#open-antconc",
    "title": "Session 5: Hands-on activity #2",
    "section": "Open AntConc",
    "text": "Open AntConc\n\nAntConc"
  },
  {
    "objectID": "2025/slides/session-5.html#antconc-window",
    "href": "2025/slides/session-5.html#antconc-window",
    "title": "Session 5: Hands-on activity #2",
    "section": "AntConc window",
    "text": "AntConc window\n\nAntConc2"
  },
  {
    "objectID": "2025/slides/session-5.html#load-a-corpus",
    "href": "2025/slides/session-5.html#load-a-corpus",
    "title": "Session 5: Hands-on activity #2",
    "section": "Load a corpus",
    "text": "Load a corpus\nNow, let‚Äôs load a corpus.\n\nWe will use BROWN Corpus.\n\n\nLoad-corpus"
  },
  {
    "objectID": "2025/slides/session-5.html#word-frequency",
    "href": "2025/slides/session-5.html#word-frequency",
    "title": "Session 5: Hands-on activity #2",
    "section": "Word Frequency",
    "text": "Word Frequency\nLet‚Äôs now create a word frequency list from a corpus\n\nSelect Word analysis option\nSet Min. Freq and Min. Range\n\n\nMin. Freq = the number of times the word should occur in the corpus\nMin. Range = the number of files in which the word should occur\n\n\nHit Start"
  },
  {
    "objectID": "2025/slides/session-5.html#lets-try",
    "href": "2025/slides/session-5.html#lets-try",
    "title": "Session 5: Hands-on activity #2",
    "section": "Let‚Äôs try",
    "text": "Let‚Äôs try\n\nSet min. frequency = 3; min. range = 3"
  },
  {
    "objectID": "2025/slides/session-5.html#saving-the-frequency-list",
    "href": "2025/slides/session-5.html#saving-the-frequency-list",
    "title": "Session 5: Hands-on activity #2",
    "section": "Saving the frequency list",
    "text": "Saving the frequency list\n\nFrom File hit save the current results\n\n\nsave-list"
  },
  {
    "objectID": "2025/slides/session-5.html#frequency-list",
    "href": "2025/slides/session-5.html#frequency-list",
    "title": "Session 5: Hands-on activity #2",
    "section": "Frequency list",
    "text": "Frequency list\n\nWe will use the BROWN frequency list in the next session."
  },
  {
    "objectID": "2025/slides/session-5.html#visualizing-frequency-distributions",
    "href": "2025/slides/session-5.html#visualizing-frequency-distributions",
    "title": "Session 5: Hands-on activity #2",
    "section": "Visualizing frequency distributions",
    "text": "Visualizing frequency distributions\n\nLet‚Äôs now understand the distributions of words in BROWN corpus.\nVisit our simple-text-analyzer tool.\nHit Frequency analysis and upload the frequency list."
  },
  {
    "objectID": "2025/slides/session-5.html#frequency-plot",
    "href": "2025/slides/session-5.html#frequency-plot",
    "title": "Session 5: Hands-on activity #2",
    "section": "Frequency Plot",
    "text": "Frequency Plot\n\n\nX-Axis = Group of 500/1,000/2,000 words\nY-Axis = Average Frequency across all words in the band\n\n\n\nBROWN frequency"
  },
  {
    "objectID": "2025/slides/session-5.html#discussion",
    "href": "2025/slides/session-5.html#discussion",
    "title": "Session 5: Hands-on activity #2",
    "section": "Discussion",
    "text": "Discussion\n\nWhat do you learn from the previous bar graph?\nFor each frequency band, what are the characteristics of the sample words you see?"
  },
  {
    "objectID": "2025/slides/session-5.html#so-far",
    "href": "2025/slides/session-5.html#so-far",
    "title": "Session 5: Hands-on activity #2",
    "section": "So far‚Ä¶",
    "text": "So far‚Ä¶\n\nUp to this point, we only dealt with the English language.\nLet‚Äôs try doing the same for Japanese.\nBut we need to do something extra‚Ä¶\nCan you guess what that is?\n\n\nI am planning to eat Oysters after this intensive course.\n„Åì„ÅÆÁü≠ÊúüÈõÜ‰∏≠Ë¨õÂ∫ß„ÅåÁµÇ„Çè„Å£„Åü„Çâ„ÄÅ„Ç´„Ç≠„ÇíÈ£ü„Åπ„Åü„ÅÑ„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ"
  },
  {
    "objectID": "2025/slides/session-5.html#tokenization",
    "href": "2025/slides/session-5.html#tokenization",
    "title": "Session 5: Hands-on activity #2",
    "section": "Tokenization",
    "text": "Tokenization\n\nEnglish is very convenient in corpus analysis because of the white spaces.\nAsian languages have completely different writing system from Indo-European language, and it makes it difficult to tokenize texts into words.\n\nEnglish text\nI am planning to eat Oysters after this intensive course.\nJapanese text\n„Åì„ÅÆÁü≠ÊúüÈõÜ‰∏≠Ë¨õÂ∫ß„ÅåÁµÇ„Çè„Å£„Åü„Çâ„ÄÅ„Ç´„Ç≠„ÇíÈ£ü„Åπ„Åü„ÅÑ„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ"
  },
  {
    "objectID": "2025/slides/session-5.html#tokenization-1",
    "href": "2025/slides/session-5.html#tokenization-1",
    "title": "Session 5: Hands-on activity #2",
    "section": "Tokenization",
    "text": "Tokenization\n\nTokenization = segmenting running text into words\nIt needs more advanced statistical algorithms for Asian languages.\n\nHow would you chunk these\n\n„Åì„ÅÆÁü≠ÊúüÈõÜ‰∏≠Ë¨õÂ∫ß„ÅåÁµÇ„Çè„Å£„Åü„Çâ„ÄÅ„Ç´„Ç≠„ÇíÈ£ü„Åπ„Åü„ÅÑ„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ"
  },
  {
    "objectID": "2025/slides/session-5.html#tokenization-with-tagant",
    "href": "2025/slides/session-5.html#tokenization-with-tagant",
    "title": "Session 5: Hands-on activity #2",
    "section": "Tokenization with TagAnt",
    "text": "Tokenization with TagAnt\n\nTagAnt is a free tool (again developped by Laurence ANTHONY) to conduct tokenization (and POS tagging).\nIt uses modern natural language processing tool (called spaCy) to tokenize input texts."
  },
  {
    "objectID": "2025/slides/session-5.html#tokenizing-japanese",
    "href": "2025/slides/session-5.html#tokenizing-japanese",
    "title": "Session 5: Hands-on activity #2",
    "section": "Tokenizing Japanese",
    "text": "Tokenizing Japanese\n\nDownload and open TagAnt."
  },
  {
    "objectID": "2025/slides/session-5.html#tokenizing-japanese-1",
    "href": "2025/slides/session-5.html#tokenizing-japanese-1",
    "title": "Session 5: Hands-on activity #2",
    "section": "Tokenizing Japanese",
    "text": "Tokenizing Japanese\n\nInput text\nSelect language.\nSelect Output format."
  },
  {
    "objectID": "2025/slides/session-5.html#result-of-tagant-segmentation",
    "href": "2025/slides/session-5.html#result-of-tagant-segmentation",
    "title": "Session 5: Hands-on activity #2",
    "section": "Result of TagAnt segmentation",
    "text": "Result of TagAnt segmentation\nYou can choose from two output formats.\n\n\n\n\n\nHorizontal display\n\n\n\n\n\n\nVertical display"
  },
  {
    "objectID": "2025/slides/session-5.html#part-of-speech-tagging-in-tagant",
    "href": "2025/slides/session-5.html#part-of-speech-tagging-in-tagant",
    "title": "Session 5: Hands-on activity #2",
    "section": "Part-Of-Speech Tagging in TagAnt",
    "text": "Part-Of-Speech Tagging in TagAnt\n\nTagAnt can do more than tokenization.\nIt allows you to automatically annotate the token for Part-Of-Speech (POS).\nPOS = Grammatical category of lexical items (NOUN, VERB, etc.)"
  },
  {
    "objectID": "2025/slides/session-5.html#choosing-the-right-format-for-pos-representation",
    "href": "2025/slides/session-5.html#choosing-the-right-format-for-pos-representation",
    "title": "Session 5: Hands-on activity #2",
    "section": "Choosing the right format for POS representation",
    "text": "Choosing the right format for POS representation\n\nYou can ask TagAnt for different output formats.\nFor now, let‚Äôs choose word+POS.\n\n\n„Åì„ÅÆÁü≠ÊúüÈõÜ‰∏≠Ë¨õÂ∫ß„ÅåÁµÇ„Çè„Å£„Åü„Çâ„ÄÅ„Ç´„Ç≠„ÇíÈ£ü„Åπ„Åü„ÅÑ„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n„Åì„ÅÆ_DET Áü≠Êúü_NOUN ÈõÜ‰∏≠_NOUN Ë¨õÂ∫ß_NOUN „Åå_ADP ÁµÇ„Çè„Å£_VERB „Åü„Çâ_AUX „ÄÅ_PUNCT „Ç´„Ç≠_NOUN „Çí_ADP È£ü„Åπ_VERB „Åü„ÅÑ_AUX „Å®_ADP ÊÄù„Å£_VERB „Å¶_SCONJ „ÅÑ_VERB „Åæ„Åô_AUX „ÄÇ_PUNCT"
  },
  {
    "objectID": "2025/slides/session-5.html#now-try-different-options",
    "href": "2025/slides/session-5.html#now-try-different-options",
    "title": "Session 5: Hands-on activity #2",
    "section": "Now try different options‚Ä¶",
    "text": "Now try different options‚Ä¶\n\nLet‚Äôs analyze your own example with the following:\n\nword+pos\nword+pos_tag\nword+lemma\nword+pos+lemma\n\nWhat do these choice give you? Share it with your neighbor."
  },
  {
    "objectID": "2025/slides/session-5.html#other-choices-and-expected-results",
    "href": "2025/slides/session-5.html#other-choices-and-expected-results",
    "title": "Session 5: Hands-on activity #2",
    "section": "Other choices and expected results",
    "text": "Other choices and expected results\n\n\n\n\n\n\n\nDisplay Information\nExample\n\n\n\n\nword\n„Ç´„Ç≠ „Çí È£ü„Åπ „Åü„ÅÑ\n\n\nword+pos\n„Ç´„Ç≠_NOUN „Çí_ADP È£ü„Åπ_VERB „Åü„ÅÑ_AUX\n\n\nword+pos_tag\n„Ç´„Ç≠_ÂêçË©û-ÊôÆÈÄöÂêçË©û-‰∏ÄËà¨ „Çí_Âä©Ë©û-Ê†ºÂä©Ë©û È£ü„Åπ_ÂãïË©û-‰∏ÄËà¨ „Åü„ÅÑ_Âä©ÂãïË©û\n\n\nword+lemma\n„Ç´„Ç≠_„Ç´„Ç≠ „Çí_„Çí È£ü„Åπ_È£ü„Åπ„Çã „Åü„ÅÑ_„Åü„ÅÑ"
  },
  {
    "objectID": "2025/slides/session-5.html#questions",
    "href": "2025/slides/session-5.html#questions",
    "title": "Session 5: Hands-on activity #2",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "2025/slides/session-5.html#task-1-compile-a-japanese-word-frequency-list",
    "href": "2025/slides/session-5.html#task-1-compile-a-japanese-word-frequency-list",
    "title": "Session 5: Hands-on activity #2",
    "section": "Task 1: Compile a Japanese Word Frequency list",
    "text": "Task 1: Compile a Japanese Word Frequency list\n\nTask\nCompile a Japanese frequency list based on a corpus.\nResource\n\nDownload a Japanese text Aozora 500 from Google Drive.\nUse AntConc, TagAnt, and Simple Text Analyzer.\n\nSubmission\n\nSubmit a frequency list .tsv or .txt.\nDescription of word frequency pattern in Japanese.\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\nYour submission ‚Ä¶\n\nincludes a frequency list of Japanese words based on Aozora 500\nprovides a description of word-frequency patterns in Aozora 500 using example words from each frequency bins"
  },
  {
    "objectID": "2025/slides/session-5.html#vocabulary-profiling",
    "href": "2025/slides/session-5.html#vocabulary-profiling",
    "title": "Session 5: Hands-on activity #2",
    "section": "Vocabulary Profiling",
    "text": "Vocabulary Profiling\n\nVocabulary profiling is a technique to use corpus frequency to understand characteristics of vocabulary use in the input text\nFor lexical sophistication measure\n\nLFP, Beyond 2000\n\nFor lexical coverage\n\nHow many words do readers/listeners need to know in order to comprehend the text (90, 95, or 98% coverage)"
  },
  {
    "objectID": "2025/slides/session-5.html#vocabulary-profiling-tools",
    "href": "2025/slides/session-5.html#vocabulary-profiling-tools",
    "title": "Session 5: Hands-on activity #2",
    "section": "Vocabulary Profiling tools",
    "text": "Vocabulary Profiling tools\n\nRANGE program\nVocabProfiler in LexTutor\nAntWordProfiler (desktop; multi-language)\nNew Word Levels Checker"
  },
  {
    "objectID": "2025/slides/session-5.html#what-weve-covered",
    "href": "2025/slides/session-5.html#what-weve-covered",
    "title": "Session 5: Hands-on activity #2",
    "section": "What we‚Äôve covered",
    "text": "What we‚Äôve covered\n\nCreating frequency list\nTokenizing Japanese texts\nConducting Part-Of-Speech tagging"
  },
  {
    "objectID": "2025/slides/session-1.html#session-1-agenda",
    "href": "2025/slides/session-1.html#session-1-agenda",
    "title": "Session 1: Introduction",
    "section": "Session¬†1¬†Agenda",
    "text": "Session¬†1¬†Agenda\n\nWelcome¬†& Course Logistics\n\nCourse website & Google¬†Classroom access\n\nIntroduction to Corpus Linguistics\n\nWhat is a corpus?\nTypes of linguistic corpora\nTypical research use cases"
  },
  {
    "objectID": "2025/slides/session-1.html#session-1-agenda-1",
    "href": "2025/slides/session-1.html#session-1-agenda-1",
    "title": "Session 1: Introduction",
    "section": "Session¬†1¬†Agenda",
    "text": "Session¬†1¬†Agenda\n\nCourse Roadmap\n\nObjectives & learning outcomes\nCourse structure and expectations"
  },
  {
    "objectID": "2025/slides/session-1.html#learning-objectives",
    "href": "2025/slides/session-1.html#learning-objectives",
    "title": "Session 1: Introduction",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, students will be able to:\n\nDefine what a corpus is and distinguish between types of corpora.\nDescribe key use cases of corpus linguistics in applied linguistics.\nUnderstand the course objectives, structure, and expectations."
  },
  {
    "objectID": "2025/slides/session-1.html#your-instructor",
    "href": "2025/slides/session-1.html#your-instructor",
    "title": "Session 1: Introduction",
    "section": "Your Instructor",
    "text": "Your Instructor\n\n\nMasaki Eguchi, Ph.D.\n\nDeveloping AI (InteLLA) and Speaking test at Equmenopolis, Inc.\nGuest Research Assistant Professor at Waseda University.\n\n\n\n\n\nPicture\n\n\n\n\n\nInteLLA"
  },
  {
    "objectID": "2025/slides/session-1.html#my-research",
    "href": "2025/slides/session-1.html#my-research",
    "title": "Session 1: Introduction",
    "section": "My research",
    "text": "My research\nSecond-language writing/speaking assessment using computational techniques\n\n\n\n\n\nEngagement Analyzer\n\n\n\n\n\n\nChatGPT for Essay Scoring"
  },
  {
    "objectID": "2025/slides/session-1.html#recent-work---validating-ai-speaking-test",
    "href": "2025/slides/session-1.html#recent-work---validating-ai-speaking-test",
    "title": "Session 1: Introduction",
    "section": "Recent work - Validating AI speaking test",
    "text": "Recent work - Validating AI speaking test\n\n\n\n\n\nInteLLA CA\n\n\n\n\n\n\nRasch"
  },
  {
    "objectID": "2025/slides/session-1.html#introduction-1st-round",
    "href": "2025/slides/session-1.html#introduction-1st-round",
    "title": "Session 1: Introduction",
    "section": "Introduction 1st round",
    "text": "Introduction 1st round\n\nBriefly introduce yourself\nName\nYear/Department\nThings you like to do during free time, OR\nFavorite things to do in Sendai."
  },
  {
    "objectID": "2025/slides/session-1.html#introduction-2nd-round",
    "href": "2025/slides/session-1.html#introduction-2nd-round",
    "title": "Session 1: Introduction",
    "section": "Introduction 2nd round",
    "text": "Introduction 2nd round\n\nResearch interests/topic\nWhy you are taking this course"
  },
  {
    "objectID": "2025/slides/session-1.html#what-is-the-first-thing-that-come-to-your-mind-about-corpus",
    "href": "2025/slides/session-1.html#what-is-the-first-thing-that-come-to-your-mind-about-corpus",
    "title": "Session 1: Introduction",
    "section": "What is the first thing that come to your mind about corpus?",
    "text": "What is the first thing that come to your mind about corpus?"
  },
  {
    "objectID": "2025/slides/session-1.html#a-linguistic-corpus",
    "href": "2025/slides/session-1.html#a-linguistic-corpus",
    "title": "Session 1: Introduction",
    "section": "A linguistic Corpus",
    "text": "A linguistic Corpus\nA linguistic corpus is:\n\nsearchable digital collection of\nreal-world language use\noften accompanied by meta-data describing the contextual parameters\n\n(To be explored more in session 2)"
  },
  {
    "objectID": "2025/slides/session-1.html#corpus-documents-language-use-in-real-world",
    "href": "2025/slides/session-1.html#corpus-documents-language-use-in-real-world",
    "title": "Session 1: Introduction",
    "section": "Corpus documents language use in real-world",
    "text": "Corpus documents language use in real-world\n\nCOCA example"
  },
  {
    "objectID": "2025/slides/session-1.html#what-do-corpora-contain",
    "href": "2025/slides/session-1.html#what-do-corpora-contain",
    "title": "Session 1: Introduction",
    "section": "What do corpora contain?",
    "text": "What do corpora contain?\n\n\nlanguage samples produced in the wild for specific communicative purposes\n\nWritten language:\n\nMagazines\nNews Paper\nBlog\n\nSpoken language:\n\ntranscriptions of spoken exchanges\n\nTV or radio shows\nConversations"
  },
  {
    "objectID": "2025/slides/session-1.html#how-do-we-use-corpus",
    "href": "2025/slides/session-1.html#how-do-we-use-corpus",
    "title": "Session 1: Introduction",
    "section": "How do we use corpus?",
    "text": "How do we use corpus?\nCorpus can give you answers on how people use language.\n\nHow often does X occur in Corpus A?\nHow often does X occure with Y?\nIn what context does X occur?"
  },
  {
    "objectID": "2025/slides/session-1.html#word-naming-time.-what-is-the-following-object",
    "href": "2025/slides/session-1.html#word-naming-time.-what-is-the-following-object",
    "title": "Session 1: Introduction",
    "section": "Word Naming time. What is the following object?",
    "text": "Word Naming time. What is the following object?\n\n\n\n\n\nWhat‚Äôs the name of these?\n\n\n\n\ncell phone\nmobile phone\nsmart phone"
  },
  {
    "objectID": "2025/slides/session-1.html#research-questions-about",
    "href": "2025/slides/session-1.html#research-questions-about",
    "title": "Session 1: Introduction",
    "section": "Research questions about",
    "text": "Research questions about\n\nOpen-ended question\n\nHow does the frequency of cell phone changed over time?\nWhich region uses cell phone more frequently than mobile phone?\n\nClose-end deductive question\n\nDoes British use cell phone more often than mobile phone?"
  },
  {
    "objectID": "2025/slides/session-1.html#set-up-english-corpora.org-account",
    "href": "2025/slides/session-1.html#set-up-english-corpora.org-account",
    "title": "Session 1: Introduction",
    "section": "Set up English-Corpora.org account",
    "text": "Set up English-Corpora.org account\n\nVisit English-Corpora.org"
  },
  {
    "objectID": "2025/slides/session-1.html#how-many-times-does-cell-phone-occur-in-news-written-in-english",
    "href": "2025/slides/session-1.html#how-many-times-does-cell-phone-occur-in-news-written-in-english",
    "title": "Session 1: Introduction",
    "section": "how many times does cell phone occur in News written in English?",
    "text": "how many times does cell phone occur in News written in English?\n\ncell-phone"
  },
  {
    "objectID": "2025/slides/session-1.html#how-many-times-does-mobile-phone-occur-in-news-written-in-english",
    "href": "2025/slides/session-1.html#how-many-times-does-mobile-phone-occur-in-news-written-in-english",
    "title": "Session 1: Introduction",
    "section": "how many times does mobile phone occur in News written in English?",
    "text": "how many times does mobile phone occur in News written in English?\n\nmobile-phone"
  },
  {
    "objectID": "2025/slides/session-1.html#comparision",
    "href": "2025/slides/session-1.html#comparision",
    "title": "Session 1: Introduction",
    "section": "Comparision",
    "text": "Comparision"
  },
  {
    "objectID": "2025/slides/session-1.html#which-region-uses-cell-phone-more-frequenty",
    "href": "2025/slides/session-1.html#which-region-uses-cell-phone-more-frequenty",
    "title": "Session 1: Introduction",
    "section": "Which region uses cell phone more frequenty?",
    "text": "Which region uses cell phone more frequenty?\n\ncell-phone"
  },
  {
    "objectID": "2025/slides/session-1.html#how-about-mobile-phone",
    "href": "2025/slides/session-1.html#how-about-mobile-phone",
    "title": "Session 1: Introduction",
    "section": "How about mobile phone?",
    "text": "How about mobile phone?\n\nmobile-phone"
  },
  {
    "objectID": "2025/slides/session-1.html#answer-to-the-research-questions",
    "href": "2025/slides/session-1.html#answer-to-the-research-questions",
    "title": "Session 1: Introduction",
    "section": "Answer to the research questions",
    "text": "Answer to the research questions\n\nHow does the frequency of cell phone changed over time?\n\nThe use of cell phone tends to decline.\n\nWhich region uses cell phone more frequently than others?\n\ncell phone: US; Pakistan; Jamaica\nmobile phone: Philippines;\n\n\n‚Üí You can get quantitative insights into how certain language is used"
  },
  {
    "objectID": "2025/slides/session-1.html#research-questions-for-corpus-based-research",
    "href": "2025/slides/session-1.html#research-questions-for-corpus-based-research",
    "title": "Session 1: Introduction",
    "section": "Research Questions for corpus-based research",
    "text": "Research Questions for corpus-based research\nWith corpus we can ask questions:\n\nHow does X change over TIME?\nHow does X differ by REGION?\nHow does X vary by GENRE?\n\nWe can learn patterns of language use in relation to extra-linguistic factors."
  },
  {
    "objectID": "2025/slides/session-1.html#the-x-in-corpus-linguistics",
    "href": "2025/slides/session-1.html#the-x-in-corpus-linguistics",
    "title": "Session 1: Introduction",
    "section": "The X in corpus linguistics",
    "text": "The X in corpus linguistics\nIn corpus linguistics, we not only identify words but also:\n\nMorphemes (e.g., -ness; pre-; -ed)\nSyntactic constructions (e.g., by AGENT; relativa clauses)\nMultiword sequences (e.g., collocation significant role)\n\n\n\nPronunciation (segmental; prosodic features)\n\n\n‚Üí In this course, we focus on lexico-grammatical features"
  },
  {
    "objectID": "2025/slides/session-1.html#questions",
    "href": "2025/slides/session-1.html#questions",
    "title": "Session 1: Introduction",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "2025/slides/session-1.html#course-website",
    "href": "2025/slides/session-1.html#course-website",
    "title": "Session 1: Introduction",
    "section": "Course Website",
    "text": "Course Website\nWe will use the following two channels\n\n\n\nGithub Page\n\nto communicate course schedules, plans, and slides.\n\nGoogle classroom\n\nPlease do not hesitate to reach out\nfor assignment submission and Readings and Corpus data distribution.\n\n\n\n\n\n\nWebsite QR code"
  },
  {
    "objectID": "2025/slides/session-1.html#course-description",
    "href": "2025/slides/session-1.html#course-description",
    "title": "Session 1: Introduction",
    "section": "Course description",
    "text": "Course description\n\nThis 5-day introduction:\n\ncovers key concepts in corpus linguistics and learner corpus research\nteaches you how to conduct simple corpus searches using Concordance software\ngives you an overview of methods to investigate conditional distributions (e.g., frequency, co-occurrences) of vocabulary, multiword units, and grammatical items.\nintroduces foundational methods to identify linguisitic phenomena using corpus and how to know about their distribution\ndiscusses important applications of corpus methods in applied linguistic (second language) research"
  },
  {
    "objectID": "2025/slides/session-1.html#learning-objectives-1",
    "href": "2025/slides/session-1.html#learning-objectives-1",
    "title": "Session 1: Introduction",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nBy the end of this course, students will be able to:\n\nExplain what corpus linguistics is and how corpus linguistics can help learn linguistic phenomena\nSearch for and select available corpora relevant to their own research\nDiscuss design issues related to language corpora for specific research purposes\nApply introductory corpus linguistic analyses (e.g., frequency analysis, concordancing, collocation analysis, POS tagging, and dependency parsing) to preprocessed corpora\nEvaluate the benefits and drawbacks of a corpus linguistic approach to linguistic analysis"
  },
  {
    "objectID": "2025/slides/session-1.html#course-components",
    "href": "2025/slides/session-1.html#course-components",
    "title": "Session 1: Introduction",
    "section": "Course Components",
    "text": "Course Components\n\nLectures and tutorials\nDaily Hands-on activities\nMini-corpus labs\nFinal project (will talk more)"
  },
  {
    "objectID": "2025/slides/session-1.html#textbook",
    "href": "2025/slides/session-1.html#textbook",
    "title": "Session 1: Introduction",
    "section": "Textbook",
    "text": "Textbook\n\nDurrant, P. (2023). Corpus linguistics for writing development: A guide for research. Routledge. https://doi.org/10.4324/9781003152682\nStefanowitsch, A. (2020). Corpus linguistics: A guide to the methodology. Zenodo. https://doi.org/10.5281/ZENODO.3735822 (This is an open source textbook, so it‚Äôs freely available online)\n\nOther required/Optional readings are provided through Google Classroom."
  },
  {
    "objectID": "2025/slides/session-1.html#tools-and-softwares",
    "href": "2025/slides/session-1.html#tools-and-softwares",
    "title": "Session 1: Introduction",
    "section": "Tools and Softwares",
    "text": "Tools and Softwares\n\nSimple Text Analyzer: A web app created for you.\nAntConc: Corpus analysis toolkit for Concordancing\n\nPlease set this up by tomorrow set-up guide\n\nTagAnt:\n\nDownload and get ready to open\n\nGoogle Colaboratory: Follow the instruction here to enable the tool."
  },
  {
    "objectID": "2025/slides/session-1.html#session-overview",
    "href": "2025/slides/session-1.html#session-overview",
    "title": "Session 1: Introduction",
    "section": "Session overview",
    "text": "Session overview\n\n\n\n\n\n\n\n\n\nDay\nTheme\nSessions\n\n\n\n\nDay 1\nIntroduction & Corpus Basics\nSession 1-3\n\n\nDay 2\nAnalysis of Vocabulary & Multiword Units (1)\nSession 4-6\n\n\nDay 3\nAnalysis of Vocabulary & Multiword Units (2)\nSession 7-9\n\n\nDay 4\nAnalysis of Grammar\nSession 10-12\n\n\nDay 5\nAdvanced Topics & Projects\nSession 13-15"
  },
  {
    "objectID": "2025/slides/session-1.html#day-1-today",
    "href": "2025/slides/session-1.html#day-1-today",
    "title": "Session 1: Introduction",
    "section": "Day 1 Today",
    "text": "Day 1 Today\n\nBasic corpus search\nConstructing Research Questions and hypotheses\n\n\nsofa - chart"
  },
  {
    "objectID": "2025/slides/session-1.html#day-2-monday",
    "href": "2025/slides/session-1.html#day-2-monday",
    "title": "Session 1: Introduction",
    "section": "Day 2 (Monday)",
    "text": "Day 2 (Monday)\n\nCreating word frequency list\nCalculating lexical diversity measures\nCalculating lexical sophistication measures\n\n\nfreq-distribution"
  },
  {
    "objectID": "2025/slides/session-1.html#day-3-tuesday",
    "href": "2025/slides/session-1.html#day-3-tuesday",
    "title": "Session 1: Introduction",
    "section": "Day 3 (Tuesday)",
    "text": "Day 3 (Tuesday)\n\nIdentifying multiword units (ngrams and collocations)\nCalculating Strengths of Association Measures\n\n\ncollocation"
  },
  {
    "objectID": "2025/slides/session-1.html#day-4-wednesday",
    "href": "2025/slides/session-1.html#day-4-wednesday",
    "title": "Session 1: Introduction",
    "section": "Day 4 (Wednesday)",
    "text": "Day 4 (Wednesday)\n\nConducting POS tagging and dependency parsing\nWe will learn how to parse sentence using spaCy package in Python\n\n\ndependency"
  },
  {
    "objectID": "2025/slides/session-1.html#day-5-thursday",
    "href": "2025/slides/session-1.html#day-5-thursday",
    "title": "Session 1: Introduction",
    "section": "Day 5 (Thursday)",
    "text": "Day 5 (Thursday)\n\nSpecial session on linguistic annotation using Large-Language Models\n\nlinguistic accuracy\ndiscourse move (if time allowed)"
  },
  {
    "objectID": "2025/slides/session-1.html#daily-schedule",
    "href": "2025/slides/session-1.html#daily-schedule",
    "title": "Session 1: Introduction",
    "section": "Daily schedule",
    "text": "Daily schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 1\n\n\n12:00-13:00\nLunch break\n\n\n13:00-14:30\nSession 2\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 3\n\n\n16:15-17:00\nOffice hour"
  },
  {
    "objectID": "2025/slides/session-1.html#option-a---conducting-a-separate-mini-project",
    "href": "2025/slides/session-1.html#option-a---conducting-a-separate-mini-project",
    "title": "Session 1: Introduction",
    "section": "Option A - Conducting a separate mini-project",
    "text": "Option A - Conducting a separate mini-project\n\nOption A is more extensive in that you will be asked to conduct a new mini-project using the toolkit you have learned throughout the course.\nGiven the limited time, however, this plan requires a lot of commitment to the present course and may not be feasible, (but we can try if you‚Äôd like!).\nWe will discuss possible alternatives (like Option B below)"
  },
  {
    "objectID": "2025/slides/session-1.html#option-b---revisiting-one-of-the-completed-corpus-lab-assignments",
    "href": "2025/slides/session-1.html#option-b---revisiting-one-of-the-completed-corpus-lab-assignments",
    "title": "Session 1: Introduction",
    "section": "Option B - Revisiting one of the completed Corpus Lab assignments",
    "text": "Option B - Revisiting one of the completed Corpus Lab assignments\n\nAs this 5-day intensive course teaches you a lot of new techniques and approaches to analyze lingusitic data, it is important for us to revisit the already completed assignments and consolidate our skills.\nIn Option B, you will be asked to make a presentation on one of your previously completed Corpus Lab assignments, clearly articulating the thinking process as well as potential extension of your approach.\nMore details will be provided on the first day of the course."
  },
  {
    "objectID": "2025/slides/session-1.html#what-weve-covered-so-far",
    "href": "2025/slides/session-1.html#what-weve-covered-so-far",
    "title": "Session 1: Introduction",
    "section": "What We‚Äôve Covered So Far",
    "text": "What We‚Äôve Covered So Far\n\nCorpora = searchable digital collection of real-world language use\nWe can ask questions about patterns\nCorpus gives us evidence-based answers"
  },
  {
    "objectID": "2025/slides/session-1.html#assignment-1-preview",
    "href": "2025/slides/session-1.html#assignment-1-preview",
    "title": "Session 1: Introduction",
    "section": "Assignment 1 Preview",
    "text": "Assignment 1 Preview\nBy the end of today, you‚Äôll be able to:\n\nAsk your own research question\nSearch corpus to find relevant results\nInterpret what you find\nWrite up your discoveries\n\nStart thinking: Are there any English expression or construction you want to learn more?"
  },
  {
    "objectID": "2025/slides/session-1.html#time-to-set-up-the-tools",
    "href": "2025/slides/session-1.html#time-to-set-up-the-tools",
    "title": "Session 1: Introduction",
    "section": "Time to set-up the tools!",
    "text": "Time to set-up the tools!\n\nAntConc: Corpus analysis toolkit for Concordancing\n\nPlease set this up by tomorrow set-up guide\n\nTagAnt:\n\nDownload and get ready to open\n\nGoogle Colaboratory: Follow the instruction here to enable the tool."
  },
  {
    "objectID": "2025/slides/session-2.html#learning-objectives",
    "href": "2025/slides/session-2.html#learning-objectives",
    "title": "Session 2: Corpus as a scientific method",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\n\nBy the end of this session, students will be able to:\n\nDefine corpus linguistics as an empirical methodology\nExplain key limitations of introspection in linguistic research\nDescribe the role of frequency data and patterns in corpus analysis\nIdentify and explain the basic steps in corpus-based research\nReflect on their own stance toward data, intuition, and linguistic evidence"
  },
  {
    "objectID": "2025/slides/session-2.html#corpus-linguistics",
    "href": "2025/slides/session-2.html#corpus-linguistics",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Corpus linguistics",
    "text": "Corpus linguistics\n\nthe investigation of linguistic research question\nthat have been framed in terms of the conditional distribution of linguisitc phenomena\nin a linguistic corpus.\n\n(Stefanowitsch, 2020, p.¬†56)"
  },
  {
    "objectID": "2025/slides/session-2.html#a-linguistic-corpus",
    "href": "2025/slides/session-2.html#a-linguistic-corpus",
    "title": "Session 2: Corpus as a scientific method",
    "section": "A linguistic Corpus",
    "text": "A linguistic Corpus\nA linguistic corpus is:\n\nsearchable digital collection of\nreal-world language use\noften accompanied by meta-data describing the contextual parameters\n\n‚Üí A corpus documents ‚Äúrecorded observation of language behavior‚Äù"
  },
  {
    "objectID": "2025/slides/session-2.html#common-criticism-to-corpus-linguistics",
    "href": "2025/slides/session-2.html#common-criticism-to-corpus-linguistics",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Common criticism to corpus linguistics",
    "text": "Common criticism to corpus linguistics\n\nCorpus documents performance not competence\nCorpus is incomplete\nIt provides language forms but not meaning\nCorpus does not give negative evidence\n\n‚Üí First two points are worth elaborating."
  },
  {
    "objectID": "2025/slides/session-2.html#some-researchers-particulary-generative-linguists-say",
    "href": "2025/slides/session-2.html#some-researchers-particulary-generative-linguists-say",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Some researchers (particulary generative linguists) say‚Ä¶",
    "text": "Some researchers (particulary generative linguists) say‚Ä¶\n\nCorpus may not tell us about ‚Äúcompetence‚Äù\n\n\nThere is an assumption that ‚Äúcompetence‚Äù (Language Acquisition Device) exists\nPerformance data is too noisy to get to the actual competence"
  },
  {
    "objectID": "2025/slides/session-2.html#some-researchers-particulary-generative-linguists-say-1",
    "href": "2025/slides/session-2.html#some-researchers-particulary-generative-linguists-say-1",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Some researchers (particulary generative linguists) say‚Ä¶",
    "text": "Some researchers (particulary generative linguists) say‚Ä¶\n\nCorpus is imcomplete‚Ä¶\n\n\nbecause corpus size is finite we cannot really tap into what native speakers will be able to say about the language\n\n‚Üí Corpus linguists respectfully disagree with these statements."
  },
  {
    "objectID": "2025/slides/session-2.html#why-cant-we-trust-our-intuition",
    "href": "2025/slides/session-2.html#why-cant-we-trust-our-intuition",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Why Can‚Äôt We Trust Our Intuition?",
    "text": "Why Can‚Äôt We Trust Our Intuition?"
  },
  {
    "objectID": "2025/slides/session-2.html#guess-the-frequency-test-your-intuition",
    "href": "2025/slides/session-2.html#guess-the-frequency-test-your-intuition",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Guess the frequency! Test your intuition",
    "text": "Guess the frequency! Test your intuition\n\nRank the following words by frequency in iWeb (14 billion word Web Corpus):\n\nthe\nobvious\ndog\nabsolutely\nmeaningful\nchair\nand\n\nDon‚Äôt turn to next slide until you finish sorting!"
  },
  {
    "objectID": "2025/slides/session-2.html#the-frequency-rank-in-iweb-corpus",
    "href": "2025/slides/session-2.html#the-frequency-rank-in-iweb-corpus",
    "title": "Session 2: Corpus as a scientific method",
    "section": "The frequency rank in iWeb corpus",
    "text": "The frequency rank in iWeb corpus\n\n\n\n\nword\nfreq\nfreq per million\n\n\n\n\nthe\n746240010\n53,332.79\n\n\nand\n387613768\n27,666.94\n\n\ndog\n1447231\n103.30\n\n\nabsolutely\n1027853\n73.37\n\n\nobvious\n715011\n51.04\n\n\nchair\n621975\n44.40\n\n\nmeaningful\n297635\n21.24"
  },
  {
    "objectID": "2025/slides/session-2.html#test-your-intuition---2",
    "href": "2025/slides/session-2.html#test-your-intuition---2",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Test Your Intuition - 2",
    "text": "Test Your Intuition - 2\n\nIs the following expressions grammatical? Yes or No?\n\ndifferent from\ndifferent than\ndifferent to"
  },
  {
    "objectID": "2025/slides/session-2.html#what-corpus-has-to-say-about-them",
    "href": "2025/slides/session-2.html#what-corpus-has-to-say-about-them",
    "title": "Session 2: Corpus as a scientific method",
    "section": "What corpus has to say about them?",
    "text": "What corpus has to say about them?\n\nGiven this data, how would you describe the pattern?\n\n\ndifferent + from/to/than"
  },
  {
    "objectID": "2025/slides/session-2.html#your-intuition-may-have-failed-because",
    "href": "2025/slides/session-2.html#your-intuition-may-have-failed-because",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Your intuition may have failed because:",
    "text": "Your intuition may have failed because:\n\nWe notice unusual things, not common ones\n\nLanguage frequency AND our physical world are different\nOur intuition is also affected by our other sensory modes and cognitive mechanisms"
  },
  {
    "objectID": "2025/slides/session-2.html#reliability-of-intuited-data",
    "href": "2025/slides/session-2.html#reliability-of-intuited-data",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Reliability of intuited ‚Äúdata‚Äù",
    "text": "Reliability of intuited ‚Äúdata‚Äù\n\nIntuited data is vulnerable\n\nNo one experienced every linguistic varieties spoken in the world.\nThe cognitive process to judge grammaticality will be different from usual language processing interpreting the ‚Äúmeaning‚Äù."
  },
  {
    "objectID": "2025/slides/session-2.html#it-doesnt-matter-the-n",
    "href": "2025/slides/session-2.html#it-doesnt-matter-the-n",
    "title": "Session 2: Corpus as a scientific method",
    "section": "It doesn‚Äôt matter the N",
    "text": "It doesn‚Äôt matter the N\n\n\nI searched this construction over News on the web (NOW) corpus which is 22.5 billion+ corpus across 20 countries.\nIt is not actually exclusive to the British construction.\n\n See the result here"
  },
  {
    "objectID": "2025/slides/session-2.html#corpus-size-comparison",
    "href": "2025/slides/session-2.html#corpus-size-comparison",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Corpus Size Comparison",
    "text": "Corpus Size Comparison"
  },
  {
    "objectID": "2025/slides/session-2.html#limitations-of-corpus",
    "href": "2025/slides/session-2.html#limitations-of-corpus",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Limitations of Corpus",
    "text": "Limitations of Corpus\n\nCorpus is incomplete; impossible to reflect all the linguistic variations.\nprovides distributional information, but not interpretation\nlimited contextual information\ndoes not tell us about language processing\n\nCorpus has to be selected in relation to the purpose of the study"
  },
  {
    "objectID": "2025/slides/session-2.html#summary",
    "href": "2025/slides/session-2.html#summary",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Summary",
    "text": "Summary\n\nCorpus can provide information about how people are actually using the language\nMay not be complete; but big enough to tell us something more than a small group of individuals can say about the language use\n\nPrescriptive vs Descriptive\n\nCorpus also has limitation\n\nCorpus is finite\nCorpus data needs to be interpreted (but better than intuiting)."
  },
  {
    "objectID": "2025/slides/session-2.html#what-do-you-think-is-important-in-scientific-process",
    "href": "2025/slides/session-2.html#what-do-you-think-is-important-in-scientific-process",
    "title": "Session 2: Corpus as a scientific method",
    "section": "What do you think is important in scientific process?",
    "text": "What do you think is important in scientific process?\n\nSo far we have talked about how corpus can be useful as a method of inquiry.\nWe have not talked about how corpus can be integrated in the research process.\n\n‚Üí Let‚Äôs talk about general research cycle then we will go over with corpus examples."
  },
  {
    "objectID": "2025/slides/session-2.html#scientific-research-cycle",
    "href": "2025/slides/session-2.html#scientific-research-cycle",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Scientific research cycle",
    "text": "Scientific research cycle\n\nResearch Process"
  },
  {
    "objectID": "2025/slides/session-2.html#lets-do-it-together",
    "href": "2025/slides/session-2.html#lets-do-it-together",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Let‚Äôs do it together",
    "text": "Let‚Äôs do it together\n\nVerbalize your initial questions\nFormulate research questions\nFormulate research hypothesis\nOperationalize - defining the construct so we can measure\nTest it\nEvaluate the hypothesis"
  },
  {
    "objectID": "2025/slides/session-2.html#step-1-starting-from-question-or-curiosity",
    "href": "2025/slides/session-2.html#step-1-starting-from-question-or-curiosity",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Step 1: Starting from question or curiosity",
    "text": "Step 1: Starting from question or curiosity\nResearch is mostly driven by curiosity.\n\nIs ‚Äúsort of‚Äù used exclusively in spoken discourse? If not what written genre might include this?\nWhat adjectives are commonly used to modify ‚Äústair‚Äù?\nWhere is the contraction form ‚Äúgonna‚Äù more likely to be used in English?"
  },
  {
    "objectID": "2025/slides/session-2.html#your-turn",
    "href": "2025/slides/session-2.html#your-turn",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Your turn",
    "text": "Your turn\n\nIn pair, exchange what you‚Äôve wondered about the English language use in context.\n\nFocus on vocabulary, grammar, and possibly discourse\nWe cannot unfortunately look at phonetics/phonology in this class‚Ä¶\n\nAfter sharing your curiosity, we will then use that to frame the research question."
  },
  {
    "objectID": "2025/slides/session-2.html#step-2-turning-curiosity-into-research-question",
    "href": "2025/slides/session-2.html#step-2-turning-curiosity-into-research-question",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Step 2: Turning curiosity into research question",
    "text": "Step 2: Turning curiosity into research question\nA research question often involves relations between two or more constructs."
  },
  {
    "objectID": "2025/slides/session-2.html#what-are-constructs",
    "href": "2025/slides/session-2.html#what-are-constructs",
    "title": "Session 2: Corpus as a scientific method",
    "section": "What are constructs?",
    "text": "What are constructs?\n\nConstructs are simply theoretical concepts we are interested in researching.\nLinguistic constructs\n\nWords, Grammar, etc.\n\nExtra-linguistic constructs\n\nTIME\nREGION\nGENRE\nPROFICIENCY LEVEL"
  },
  {
    "objectID": "2025/slides/session-2.html#research-question-templates",
    "href": "2025/slides/session-2.html#research-question-templates",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Research question templates",
    "text": "Research question templates\n\nIn corpus-driven linguistic research, we tend to ask following questions:\n\nHow does the frequency of word X change over time?\nHow does the frequency of word X differ across text types Z?\nWhat is the frequency distribution of grammatical pattern X across different text types?\nWhat are the typical contexts in which expression X appears based on concordance analysis?\nHow do different varieties of language Y (e.g., British vs.¬†American English) differ in frequency and contexts of feature X?\nHow does register (formal/informal) affect both the frequency and typical contexts of linguistic feature X?"
  },
  {
    "objectID": "2025/slides/session-2.html#your-turn-1",
    "href": "2025/slides/session-2.html#your-turn-1",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Your turn:",
    "text": "Your turn:\n\nIn the same pair, try turning your curiosity into formalized research question.\nShare the research questions to class."
  },
  {
    "objectID": "2025/slides/session-2.html#step-3-turning-question-into-hypothesis",
    "href": "2025/slides/session-2.html#step-3-turning-question-into-hypothesis",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Step 3: Turning question into hypothesis",
    "text": "Step 3: Turning question into hypothesis\nResearch Q: ‚ÄúIs the expression ‚Äògonna‚Äô more frequent in informal genres?‚Äù\nHypothesis: ‚ÄúI expect ‚Äògonna‚Äô to occur more frequently in TV/Movie subtitles than in Magazine texts because‚Ä¶‚Äù"
  },
  {
    "objectID": "2025/slides/session-2.html#your-turn---writing-hypothesis",
    "href": "2025/slides/session-2.html#your-turn---writing-hypothesis",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Your turn - Writing hypothesis",
    "text": "Your turn - Writing hypothesis\nIn the same pair, you will be writing your hypothesis.\nTemplate: ‚ÄúI expect [specific pattern] because [reasoning based on experience/knowledge]‚Äù"
  },
  {
    "objectID": "2025/slides/session-2.html#step-4-choosing-a-corpus-based-on-your-research-question-and-hypothesis",
    "href": "2025/slides/session-2.html#step-4-choosing-a-corpus-based-on-your-research-question-and-hypothesis",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Step 4: Choosing a corpus based on your research question and hypothesis",
    "text": "Step 4: Choosing a corpus based on your research question and hypothesis\nHow can I select the one for my research?"
  },
  {
    "objectID": "2025/slides/session-2.html#three-key-properties-of-corpus",
    "href": "2025/slides/session-2.html#three-key-properties-of-corpus",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Three key properties of corpus",
    "text": "Three key properties of corpus\n\nAuthentic:\n\nCorpus reflects real-language use\n\nRepresentative:\n\nCorpus should be sampled to reflect some portion of language use.\n\nLarge:\n\nCorpus should be in sufficient size to allow investigation"
  },
  {
    "objectID": "2025/slides/session-2.html#authenticity",
    "href": "2025/slides/session-2.html#authenticity",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Authenticity",
    "text": "Authenticity\n\nIt has to be authentic language use (= language used for real communication)\n\nNo invented sentences\nNo fake data or simulation\n\nBUT‚Ä¶ Debate on authenticity\n\nHow about elicited data?\nParticularly relevant to learner corpora\n\nTimed essay writing for English exam"
  },
  {
    "objectID": "2025/slides/session-2.html#representativeness",
    "href": "2025/slides/session-2.html#representativeness",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Representativeness",
    "text": "Representativeness\n\nHow do researchers try to take representative sample of language use?\nWhat makes a corpus representative?"
  },
  {
    "objectID": "2025/slides/session-2.html#representative-in-relation-to-your-purpose",
    "href": "2025/slides/session-2.html#representative-in-relation-to-your-purpose",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Representative in relation to your purpose",
    "text": "Representative in relation to your purpose\n\nRepresentativeness is evaluated based on\n\ncoverage of important situational variables\nrelative size in the corpus (balances)\n\nThink of a stratified random sampling approach"
  },
  {
    "objectID": "2025/slides/session-2.html#describing-situational-variables",
    "href": "2025/slides/session-2.html#describing-situational-variables",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Describing situational variables",
    "text": "Describing situational variables\nDiscuss in pair/group:\nIf you were to create following corpus, what should you include in your corpus? Choose one and brainstorm the data sources you need to collect.\n\nhow English is currently used in the US\nhow English/Japanese is used in US/Japanese universities\nhow certain dialect changes overtime"
  },
  {
    "objectID": "2025/slides/session-2.html#corpus-of-contemporary-american-coca",
    "href": "2025/slides/session-2.html#corpus-of-contemporary-american-coca",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Corpus Of Contemporary American (COCA)",
    "text": "Corpus Of Contemporary American (COCA)\n\n\n\n\n\n\n\n\n\n\nGenre\n# texts\n# words\nExplanation\n\n\n\n\nSpoken\n44,803\n127,396,932\nTranscripts of unscripted conversation from more than 150 different TV and radio programs (examples: All Things Considered (NPR), Newshour (PBS), Good Morning America (ABC), Oprah)\n\n\nFiction\n25,992\n119,505,305\nShort stories and plays from literary magazines, children‚Äôs magazines, popular magazines, first chapters of first edition books 1990-present, and fan fiction.\n\n\nMagazines\n86,292\n127,352,030\nNearly 100 different magazines, with a good mix between specific domains like news, health, home and gardening, women, financial, religion, sports, etc.\n\n\nNewspapers\n90,243\n122,958,016\nNewspapers from across the US, including: USA Today, New York Times, Atlanta Journal Constitution, San Francisco Chronicle, etc. Good mix between different sections of the newspaper, such as local news, opinion, sports, financial, etc.\n\n\nAcademic\n26,137\n120,988,361\nMore than 200 different peer-reviewed journals. These cover the full range of academic disciplines, with a good balance among education, social sciences, history, humanities, law, medicine, philosophy/religion, science/technology, and business"
  },
  {
    "objectID": "2025/slides/session-2.html#corpus-of-contemporary-american-coca-1",
    "href": "2025/slides/session-2.html#corpus-of-contemporary-american-coca-1",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Corpus of Contemporary American (COCA)",
    "text": "Corpus of Contemporary American (COCA)\n\n\n\n\n\n\n\n\n\n\nGenre\n# texts\n# words\nExplanation\n\n\n\n\nWeb (Genl)\n88,989\n129,899,427\nClassified into the web genres of academic, argument, fiction, info, instruction, legal, news, personal, promotion, review web pages (by Serge Sharoff). Taken from the US portion of the GloWbE corpus.\n\n\nWeb (Blog)\n98,748\n125,496,216\nTexts that were classified by Google as being blogs. Further classified into the web genres of academic, argument, fiction, info, instruction, legal, news, personal, promotion, review web pages. Taken from the US portion of the GloWbE corpus.\n\n\nTV/Movies\n23,975\n129,293,467\nSubtitles from OpenSubtitles.org, and later the TV and Movies corpora. Studies have shown that the language from these shows and movies is even more colloquial / core than the data in actual ‚Äúspoken corpora‚Äù.\n\n\nTotal\n485,179\n1,002,889,754\n\n\n\n\n\nsee more details"
  },
  {
    "objectID": "2025/slides/session-2.html#british-academic-written-english-alsop-nesi-2009",
    "href": "2025/slides/session-2.html#british-academic-written-english-alsop-nesi-2009",
    "title": "Session 2: Corpus as a scientific method",
    "section": "British Academic Written English (Alsop & Nesi, 2009)",
    "text": "British Academic Written English (Alsop & Nesi, 2009)\nA excellent example of a specialized corpus.\n\nA corpus documenting about 2,700 university written assignments in the U.K.\n30 disiplines, 13 genre families\nFour course levels (freshman to MA level)\nBoth Distinction (A) and Merit (B)"
  },
  {
    "objectID": "2025/slides/session-2.html#bawe-disciplinary-group",
    "href": "2025/slides/session-2.html#bawe-disciplinary-group",
    "title": "Session 2: Corpus as a scientific method",
    "section": "BAWE disciplinary group",
    "text": "BAWE disciplinary group\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisciplinary group\nDisciplines\nLevel 1\nLevel 2\nLevel 3\nLevel 4\nSum\n\n\n\n\nArts and Humanities (AH)\nLinguistics, English, Philosophy, History, Classics, Archaeology, Comparative American Studies, Other\n231\n225\n160\n77\n693\n\n\nLife Sciences (LS)\nBiology, Agriculture, Food Sciences, Psychology, Health, Medicine\n172\n183\n106\n193\n654\n\n\nPhysical Sciences (PS)\nEngineering, Chemistry, Computer Science, Physics, Mathematics, Meteorology, Cybernetics & Electronics, Planning, Architecture\n181\n146\n156\n95\n578\n\n\nSocial Sciences (SS)\nBusiness, Law, Sociology, Politics, Economics, Hospitality Leisure & Tourism Management, Anthropology, Publishing\n203\n196\n159\n202\n760\n\n\nSum\n\n787\n750\n581\n567\n2,685"
  },
  {
    "objectID": "2025/slides/session-2.html#bawe-genre-family",
    "href": "2025/slides/session-2.html#bawe-genre-family",
    "title": "Session 2: Corpus as a scientific method",
    "section": "BAWE genre family",
    "text": "BAWE genre family\n\n\n\n\n\n\n\n\n\n\n\n\n\nSocial purpose\nGenre family\nLevel 1\nLevel 2\nLevel 3\nLevel 4\nSum\n\n\n\n\nDemonstrating knowledge and understanding\nexercise\n28\n20\n27\n27\n102\n\n\n\nexplanation\n72\n54\n33\n26\n185\n\n\nDeveloping powers of independent reasoning\ncritique\n75\n78\n67\n87\n307\n\n\n\nessay\n398\n357\n263\n184\n1,202\n\n\nBuilding research skills\nliterature survey\n10\n6\n7\n9\n32\n\n\n\nmethodology recount\n106\n114\n43\n60\n323\n\n\n\nresearch report\n7\n16\n22\n16\n61\n\n\nPreparing for professional practice\ncase study\n26\n30\n34\n98\n188\n\n\n\ndesign specification\n24\n19\n35\n11\n89\n\n\n\nproblem question\n12\n18\n5\n2\n37\n\n\n\nproposal\n10\n18\n8\n34\n70\n\n\nWriting for oneself and others\nempathy writing\n4\n2\n17\n5\n28\n\n\n\nnarrative recount\n15\n18\n20\n8\n61\n\n\n\nSum\n787\n750\n581\n567\n2,685"
  },
  {
    "objectID": "2025/slides/session-2.html#corpus-representativeness",
    "href": "2025/slides/session-2.html#corpus-representativeness",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Corpus Representativeness",
    "text": "Corpus Representativeness\n\nMultiple factors affecting linguistic variations can be used to balance corpus sampling:\n\nLinguistic variation = ‚Äúto‚ÄÄany‚ÄÄform‚ÄÄof‚ÄÄlanguage‚ÄÄdelineable‚ÄÄfrom‚ÄÄother‚ÄÄforms‚ÄÄalong‚ÄÄcultural,‚ÄÄlinguistic‚ÄÄor‚ÄÄdemographic criteria‚Äù (Stefanowitsch, 2020, p.¬†28)\n\ngenre\nregister\nstyle\n\n\nBut see Biber and Conrad (2019) for different definitions of each factor."
  },
  {
    "objectID": "2025/slides/session-2.html#genre",
    "href": "2025/slides/session-2.html#genre",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Genre",
    "text": "Genre\n\ncategory of speech event or text defined in terms of their communicative purpose in culture\n\nrecipe\nmanuals\nacademic paper\nnews report\netc."
  },
  {
    "objectID": "2025/slides/session-2.html#register",
    "href": "2025/slides/session-2.html#register",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Register",
    "text": "Register\n\nvariety in a set of linguistic features associated with a particular situation of language use.\n\nregister of:\n\nacademic paper\nrecipe"
  },
  {
    "objectID": "2025/slides/session-2.html#style",
    "href": "2025/slides/session-2.html#style",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Style",
    "text": "Style\n\nStefanowitsch defines: ‚Äúthe degrees of formality‚Äù (e.g.¬†formal, informal, colloquial, humorous)\nBiber & Conrad (2019) linguistic variety often associated within a register or genre\n\ndue to the author, specific speech community, etc."
  },
  {
    "objectID": "2025/slides/session-2.html#in-case-of-learner-corpora",
    "href": "2025/slides/session-2.html#in-case-of-learner-corpora",
    "title": "Session 2: Corpus as a scientific method",
    "section": "In case of learner corpora",
    "text": "In case of learner corpora\nAdditional variables such as:\n\nFirst language\nSecond-language proficiency levels\nTheir learning experiences"
  },
  {
    "objectID": "2025/slides/session-2.html#icnale",
    "href": "2025/slides/session-2.html#icnale",
    "title": "Session 2: Corpus as a scientific method",
    "section": "ICNALE",
    "text": "ICNALE\n\n\nInternational Corpus Network of Asian Learner of English (Ishikawa, 2013)\nOne of the largest learner corpora (EFL learners in Asia)\nSubcorpora for different modalities available\n\nMonologic speech\nDialogic speech\nArgumentative essay writing\n\nExtensive rating data is also available for subset of the data (GRA)"
  },
  {
    "objectID": "2025/slides/session-2.html#icnale-1",
    "href": "2025/slides/session-2.html#icnale-1",
    "title": "Session 2: Corpus as a scientific method",
    "section": "ICNALE",
    "text": "ICNALE\n\n\n\n\n\n\n\n\n\n\n\nModules\nContents\n# of participants\n# of samples\n# of words\n\n\n\n\nSpoken Monologues (SM)\n60-seconds monologues about two ICNALE common topics\n1,100\n4,400\n500,000\n\n\nSpoken Dialogues (SD)\n30-40-minutes oral interviews including picture descriptions and role plays, and 3-5-minute follow-up L1 reflections. Interview tasks are related to two ICNALE common topics\n425\n4,250\n1,600,000\n\n\nWritten Essays (WE)\n200-300-word essays about two ICNALE common topics\n2,800\n5,600\n1,300,000\n\n\nEdited Essays (EE)\nFully edited versions of learner essays about two ICNALE common topics. Rubric-based essay evaluation data is also included\n328\n1,312\n150,000\n\n\nWritten Essays Plus (WEP)\n200-300-word essays about two ICNALE common topics collected from new Asian countries\n948\n1,896\n470,000\n\n\nGlobal Rating Archives (GRA)\n+ Ratings of 140 speeches and 140 essays by 80 raters with varied L1 and occupational backgrounds.+ Plus fully edited versions of 140 essays\n280 samples √ó 80 raters\n22,400\n(Edited essays) 65,000"
  },
  {
    "objectID": "2025/slides/session-2.html#from-question-to-corpus-choice",
    "href": "2025/slides/session-2.html#from-question-to-corpus-choice",
    "title": "Session 2: Corpus as a scientific method",
    "section": "From Question to Corpus Choice",
    "text": "From Question to Corpus Choice\nLet‚Äôs say you want to investigate the following question:\n\n‚ÄúIs passive construction (be + past participle) more frequent in formal genres?‚Äù\n\nWhat information do you need in the corpus?\nTry giving some answers in pairs."
  },
  {
    "objectID": "2025/slides/session-2.html#typical-answer",
    "href": "2025/slides/session-2.html#typical-answer",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Typical answer",
    "text": "Typical answer\n\nHas multiple genres (formal AND informal)\nRecent enough to include passive construction (be + past participle)\nLarge enough to compare"
  },
  {
    "objectID": "2025/slides/session-2.html#corpora-in-english-corpora.org",
    "href": "2025/slides/session-2.html#corpora-in-english-corpora.org",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Corpora in English-Corpora.org",
    "text": "Corpora in English-Corpora.org\n\n\n\n\nCorpus\nSize\nRegions\nTime\nGenre\n\n\n\n\nIWEB\n13.9b\n6\n2017\nWeb\n\n\nNOW\n16.2b\n20\n2010-now\nWeb: News\n\n\nCOCA\n1.0b\nAm\n1990-2019\nBalanced\n\n\nCOHA\n400m\nAm\n1810-2009\nBalanced\n\n\nTIME\n100m\nAm\n1923-2006\nMagazine\n\n\nBNC\n100m\nBr\n1980s-1993\nBalanced\n\n\nCORE\n50m\n6\n2014\nWeb"
  },
  {
    "objectID": "2025/slides/session-2.html#corpus-size-comparison-1",
    "href": "2025/slides/session-2.html#corpus-size-comparison-1",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Corpus Size Comparison",
    "text": "Corpus Size Comparison"
  },
  {
    "objectID": "2025/slides/session-2.html#gnowing-size---good-or-bad",
    "href": "2025/slides/session-2.html#gnowing-size---good-or-bad",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Gnowing Size - Good or bad?",
    "text": "Gnowing Size - Good or bad?\nThe corpus is becoming larger and larger.\n\nIs this a good thing?\nAny drawbacks we might foresee?"
  },
  {
    "objectID": "2025/slides/session-2.html#steps-4-6-in-session-3",
    "href": "2025/slides/session-2.html#steps-4-6-in-session-3",
    "title": "Session 2: Corpus as a scientific method",
    "section": "Steps 4-6: In Session 3",
    "text": "Steps 4-6: In Session 3\n\nOperationalize = Define exactly what to search\nTest = Actually search the corpus\nEvaluate = Interpret your results\n\nWe‚Äôll practice these with real searches in Session 3!"
  },
  {
    "objectID": "2025/slides/session-12.html#learning-objectives",
    "href": "2025/slides/session-12.html#learning-objectives",
    "title": "Session 12: Hands-on Activity",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nDefine extraction rules to identify fine-grained grammatical features in language\nConduct analysis using a template Python code or web application provided by the instructor."
  },
  {
    "objectID": "2025/slides/session-12.html#colab-notebook",
    "href": "2025/slides/session-12.html#colab-notebook",
    "title": "Session 12: Hands-on Activity",
    "section": "Colab Notebook",
    "text": "Colab Notebook\n\nAccess the notebook here."
  },
  {
    "objectID": "2025/slides/session-12.html#introduction",
    "href": "2025/slides/session-12.html#introduction",
    "title": "Session 12: Hands-on Activity",
    "section": "Introduction",
    "text": "Introduction\n\nThe notebook is very basic version of what TAASSC will do\nThis is meant for educational use; for research more rigorous approach may be needed."
  },
  {
    "objectID": "2025/slides/session-12.html#algorithm-used-in-the-notebook",
    "href": "2025/slides/session-12.html#algorithm-used-in-the-notebook",
    "title": "Session 12: Hands-on Activity",
    "section": "Algorithm used in the notebook",
    "text": "Algorithm used in the notebook\nIn this notebook, the following analysis pipeline is implemented for you.\n\nYour input is file path to yout corpus files.\nThe current code loads the corpus files onto colab.\nIt then iterate through the corpus files one by one."
  },
  {
    "objectID": "2025/slides/session-12.html#algorithm-used-in-the-notebook-contd",
    "href": "2025/slides/session-12.html#algorithm-used-in-the-notebook-contd",
    "title": "Session 12: Hands-on Activity",
    "section": "Algorithm used in the notebook (cont‚Äôd)",
    "text": "Algorithm used in the notebook (cont‚Äôd)\n\nParse the sentence using spacy\nConduct basic analysis (such as calculating the number of tokens, sentences, etc.)\nCount the number of specific grammatical structures (MAIN FEATURE)\nStore the results into a Python dictionary\nAfter every corpus file is processed, it can create a dataset to export.\nYou can export the results for further analysis"
  },
  {
    "objectID": "2025/slides/session-12.html#spacy-token-information",
    "href": "2025/slides/session-12.html#spacy-token-information",
    "title": "Session 12: Hands-on Activity",
    "section": "spaCy token information",
    "text": "spaCy token information\nSome useful token information are following:\n\n\n\ncode\nwhat it does\nexample\n\n\n\n\ntoken.lemma_\nlemmatized form\nbe, child\n\n\ntoken.pos_\nsimple POS (Universal Dependency)\nNOUN, VERB\n\n\ntoken.tag_\nfine-grained POS (PennTag set)\nNN, JJ, VB, BBZ\n\n\ntoken.dep_\ndependency type\namod, advmd\n\n\ntoken.head\ntoken information of the head of the dependency"
  },
  {
    "objectID": "2025/slides/session-12.html#excercise",
    "href": "2025/slides/session-12.html#excercise",
    "title": "Session 12: Hands-on Activity",
    "section": "Excercise",
    "text": "Excercise\nFrom Table 5.1 in Durrant (2023, p.¬†102), pick one or two sentences.\nTry to identify that structure using the following extraction pipeline."
  },
  {
    "objectID": "2025/slides/session-12.html#extraction-pipelines.",
    "href": "2025/slides/session-12.html#extraction-pipelines.",
    "title": "Session 12: Hands-on Activity",
    "section": "Extraction pipelines.",
    "text": "Extraction pipelines."
  },
  {
    "objectID": "2025/slides/session-12.html#thinking-grammartically",
    "href": "2025/slides/session-12.html#thinking-grammartically",
    "title": "Session 12: Hands-on Activity",
    "section": "Thinking grammartically",
    "text": "Thinking grammartically\nIn pair, brainstorm 3 - 5 grammatical constructions you would like to identify in your corpus search.\n\nDescribe the grammatical feature\nGive some examples that fall under the grammatical construction.\nExplain why you are interested in."
  },
  {
    "objectID": "2025/slides/session-11.html#learning-objectives",
    "href": "2025/slides/session-11.html#learning-objectives",
    "title": "Session 11: Hands-on Activity",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nUnderstand NLP tasks such as POS tagging and dependency parsing\nUnderstand how automated parsing works\nConduct multi-lingual Part-Of-Speech (POS) tagging using TagAnt\nConduct POS tagging using spaCy library in Python (through Google Colab)\nConduct Dependency parsing using spaCy library in Python (through Google Colab)"
  },
  {
    "objectID": "2025/slides/session-11.html#hands-on-activity",
    "href": "2025/slides/session-11.html#hands-on-activity",
    "title": "Session 11: Hands-on Activity",
    "section": "Hands-on Activity",
    "text": "Hands-on Activity\nTask 1: POS tagging with TagAnt\nTask 2: POS-sensitive frequency list\nTask 3: Understanding dependency grammar through visualization"
  },
  {
    "objectID": "2025/slides/session-11.html#pos-tagging-with-tagant",
    "href": "2025/slides/session-11.html#pos-tagging-with-tagant",
    "title": "Session 11: Hands-on Activity",
    "section": "POS tagging with TagAnt",
    "text": "POS tagging with TagAnt"
  },
  {
    "objectID": "2025/slides/session-11.html#tagging-with-tagant",
    "href": "2025/slides/session-11.html#tagging-with-tagant",
    "title": "Session 11: Hands-on Activity",
    "section": "Tagging with TagAnt",
    "text": "Tagging with TagAnt\n\nOpen TagAnt\nSelect Input Files\nSelect Language\nSelect Display information (see next)"
  },
  {
    "objectID": "2025/slides/session-11.html#display-setting-info-in-tagant",
    "href": "2025/slides/session-11.html#display-setting-info-in-tagant",
    "title": "Session 11: Hands-on Activity",
    "section": "Display setting info in TagAnt",
    "text": "Display setting info in TagAnt\nFollowings are basic selection in TagAnt.\n\n\n\nMenu\nFunction\nExample\n\n\n\n\nword\ntokenization\ndogs, ran\n\n\npos\nPOS tag (simple)\nNOUN, VERB\n\n\npos_tag\nPOS tag (detailed)\nNNS, VBD\n\n\nlemma\nlemmatized word\ndog, run"
  },
  {
    "objectID": "2025/slides/session-11.html#other-diaplay-settings",
    "href": "2025/slides/session-11.html#other-diaplay-settings",
    "title": "Session 11: Hands-on Activity",
    "section": "Other Diaplay settings",
    "text": "Other Diaplay settings\n\n\n\nMenu\nFunction\nExample\n\n\n\n\nword+pos\ntokenization and POS\ndogs_NOUN, ran_VERB\n\n\nword+lemma +pos_tag\ntoken+lemma+POS\ndogs_dog_NN, ran_run_VERB"
  },
  {
    "objectID": "2025/slides/session-11.html#task-1-annotating-japanese-text-10-mins",
    "href": "2025/slides/session-11.html#task-1-annotating-japanese-text-10-mins",
    "title": "Session 11: Hands-on Activity",
    "section": "Task 1: Annotating Japanese text (10 mins)",
    "text": "Task 1: Annotating Japanese text (10 mins)\n\nAnnotate 50 Japanese text files with TagAnt.\nCreate frequency list for aozora_50"
  },
  {
    "objectID": "2025/slides/session-11.html#task-1-answer",
    "href": "2025/slides/session-11.html#task-1-answer",
    "title": "Session 11: Hands-on Activity",
    "section": "Task 1: Answer",
    "text": "Task 1: Answer\nBefore\n„ÄåÂ§ßÊ∫ù„Äç\n\n„ÄÄÂÉï„ÅØÊú¨ÊâÄÁïåÈöà„ÅÆ„Åì„Å®„Çí„Çπ„Ç±„ÉÑ„ÉÅ„Åó„Çç„Å®„ÅÑ„ÅµÁ§æÂëΩ„ÇíÂèó„Åë„ÄÅÂêå„ÅòÁ§æ„ÅÆÔºØÂêõ„Å®‰∏Ä„Åó„Çà„Å´‰πÖÊåØ„Çä„Å´Êú¨ÊâÄ„Å∏Âá∫„Åã„Åë„Å¶Ë°å„Å§„Åü„ÄÇ\nAfter\n„Äå_Ë£úÂä©Ë®òÂè∑-Êã¨ÂºßÈñã_„Äå Â§ßÊ∫ù_ÂêçË©û-Âõ∫ÊúâÂêçË©û-‰∫∫Âêç-Âßì_Â§ßÊ∫ù „Äç_Ë£úÂä©Ë®òÂè∑-Êã¨ÂºßÈñâ_„Äç\n\n„ÄÄ_SPACE_„ÄÄ ÂÉï_‰ª£ÂêçË©û_ÂÉï „ÅØ_Âä©Ë©û-‰øÇÂä©Ë©û_„ÅØ Êú¨ÊâÄ_ÂêçË©û-Âõ∫ÊúâÂêçË©û-Âú∞Âêç-‰∏ÄËà¨_Êú¨ÊâÄ ÁïåÈöà_ÂêçË©û-ÊôÆÈÄöÂêçË©û-‰∏ÄËà¨_ÁïåÈöà „ÅÆ_Âä©Ë©û-Ê†ºÂä©Ë©û_„ÅÆ „Åì„Å®_ÂêçË©û-ÊôÆÈÄöÂêçË©û-‰∏ÄËà¨_„Åì„Å® „Çí_Âä©Ë©û-Ê†ºÂä©Ë©û_„Çí „Çπ„Ç±„ÉÑ„ÉÅ_ÂêçË©û-ÊôÆÈÄöÂêçË©û-‰∏ÄËà¨_„Çπ„Ç±„ÉÑ„ÉÅ „Åó„Çç_ÂãïË©û-ÈùûËá™Á´ãÂèØËÉΩ_„Åô„Çã „Å®_Âä©Ë©û-Ê†ºÂä©Ë©û_„Å® „ÅÑ„Åµ_ÂãïË©û-‰∏ÄËà¨_„ÅÑ„Åµ Á§æÂëΩ_ÂêçË©û-ÊôÆÈÄöÂêçË©û-‰∏ÄËà¨_Á§æÂëΩ „Çí_Âä©Ë©û-Ê†ºÂä©Ë©û_„Çí Âèó„Åë_ÂãïË©û-‰∏ÄËà¨_Âèó„Åë„Çã „ÄÅ_Ë£úÂä©Ë®òÂè∑-Ë™≠ÁÇπ_„ÄÅ Âêå„Åò_ÈÄ£‰ΩìË©û_Âêå„Åò Á§æ_ÂêçË©û-ÊôÆÈÄöÂêçË©û-Âä©Êï∞Ë©ûÂèØËÉΩ_Á§æ „ÅÆ_Âä©Ë©û-Ê†ºÂä©Ë©û_„ÅÆ ÔºØ_ÂêçË©û-ÊôÆÈÄöÂêçË©û-‰∏ÄËà¨_o Âêõ_Êé•Â∞æËæû-ÂêçË©ûÁöÑ-‰∏ÄËà¨_Âêõ „Å®_Âä©Ë©û-Ê†ºÂä©Ë©û_„Å® ‰∏Ä„Åó„Çà_ÂêçË©û-ÊôÆÈÄöÂêçË©û-„ÇµÂ§âÂèØËÉΩ_‰∏Ä„Åó„Çà „Å´_Âä©Ë©û-Ê†ºÂä©Ë©û_„Å´ ‰πÖ_ÂΩ¢ÂÆπË©û-‰∏ÄËà¨_‰πÖ„ÅÑ ÊåØ„Çä_Êé•Â∞æËæû-ÂêçË©ûÁöÑ-‰∏ÄËà¨_ÊåØ„Çä „Å´_Âä©Ë©û-Ê†ºÂä©Ë©û_„Å´ Êú¨ÊâÄ_ÂêçË©û-Âõ∫ÊúâÂêçË©û-Âú∞Âêç-‰∏ÄËà¨_Êú¨ÊâÄ „Å∏_Âä©Ë©û-Ê†ºÂä©Ë©û_„Å∏ Âá∫_ÂãïË©û-‰∏ÄËà¨_Âá∫„Çã „Åã„Åë_ÂãïË©û-ÈùûËá™Á´ãÂèØËÉΩ_„Åã„Åë„Çã „Å¶_Âä©Ë©û-Êé•Á∂öÂä©Ë©û_„Å¶ Ë°å„Å§_ÂãïË©û-‰∏ÄËà¨_Ë°å„Åµ „Åü_Âä©ÂãïË©û_„Åü „ÄÇ_Ë£úÂä©Ë®òÂè∑-Âè•ÁÇπ_„ÄÇ"
  },
  {
    "objectID": "2025/slides/session-11.html#task-2-frequency-list-by-pos-tags-10-mins",
    "href": "2025/slides/session-11.html#task-2-frequency-list-by-pos-tags-10-mins",
    "title": "Session 11: Hands-on Activity",
    "section": "Task 2: Frequency-list by POS tags (10 mins)",
    "text": "Task 2: Frequency-list by POS tags (10 mins)\n\nUsing AntConc, create following frequency lists:\n\nCreate a frequency list of ÂãïË©û-ÈùûËá™Á´ãÂèØËÉΩ\n\nIf you are done, please create another frequency list with different search terms."
  },
  {
    "objectID": "2025/slides/session-11.html#task-2-key",
    "href": "2025/slides/session-11.html#task-2-key",
    "title": "Session 11: Hands-on Activity",
    "section": "Task 2: Key",
    "text": "Task 2: Key\n\nÈùûËá™Á´ãÂèØËÉΩÂãïË©û"
  },
  {
    "objectID": "2025/slides/session-11.html#advanced-options-in-tagant",
    "href": "2025/slides/session-11.html#advanced-options-in-tagant",
    "title": "Session 11: Hands-on Activity",
    "section": "Advanced options in TagAnt",
    "text": "Advanced options in TagAnt\n\nIn TagAnt, you can download models for other languages.\n\n\nloading other models"
  },
  {
    "objectID": "2025/slides/session-11.html#any-questions",
    "href": "2025/slides/session-11.html#any-questions",
    "title": "Session 11: Hands-on Activity",
    "section": "Any questions?",
    "text": "Any questions?\n\nNow you can parse multilingual text with TagAnt."
  },
  {
    "objectID": "2025/slides/session-11.html#goals",
    "href": "2025/slides/session-11.html#goals",
    "title": "Session 11: Hands-on Activity",
    "section": "Goals",
    "text": "Goals\n\nDescribe grammatical structure of a simple sentence using terminology such as ROOT,head, dependency type, and dependent."
  },
  {
    "objectID": "2025/slides/session-11.html#dependency-grammar-‰øÇÂèó„Åë",
    "href": "2025/slides/session-11.html#dependency-grammar-‰øÇÂèó„Åë",
    "title": "Session 11: Hands-on Activity",
    "section": "Dependency grammar (‰øÇÂèó„Åë)",
    "text": "Dependency grammar (‰øÇÂèó„Åë)\n\nDependency grammar is particular type of syntactic tree.\nforms a tree by defining binary relations between running tokens.\nEach token in the sentence is governed by one token (i.e., head)\nThe highest in the syntactic tree is termed as ROOT"
  },
  {
    "objectID": "2025/slides/session-11.html#dependency-grammar-‰øÇÂèó„Åë---2",
    "href": "2025/slides/session-11.html#dependency-grammar-‰øÇÂèó„Åë---2",
    "title": "Session 11: Hands-on Activity",
    "section": "Dependency grammar (‰øÇÂèó„Åë) - 2",
    "text": "Dependency grammar (‰øÇÂèó„Åë) - 2\n\nThere are a few different approaches to formalize dependency\n\nUniversal Dependency\nStanford Dependency\nClearNLP\netc."
  },
  {
    "objectID": "2025/slides/session-11.html#simple-example",
    "href": "2025/slides/session-11.html#simple-example",
    "title": "Session 11: Hands-on Activity",
    "section": "Simple example",
    "text": "Simple example\nThe following is a dependency for I play baseball.\n\nsimple-dependency"
  },
  {
    "objectID": "2025/slides/session-11.html#in-table-format",
    "href": "2025/slides/session-11.html#in-table-format",
    "title": "Session 11: Hands-on Activity",
    "section": "In table format",
    "text": "In table format\nThe same sentence, I play baseball can be expressed in the following format\n\n\n\ntid\ntoken\ndep\nhead\n\n\n\n\n1\nI\nnsubj\n2\n\n\n2\nplay\nROOT\n\n\n\n3\nbaseball\ndobj\n2\n\n\n4\n.\npunct\n2\n\n\n\nThis type of vertical format is often used to represent multi-layered token information."
  },
  {
    "objectID": "2025/slides/session-11.html#some-excercise---problem-1",
    "href": "2025/slides/session-11.html#some-excercise---problem-1",
    "title": "Session 11: Hands-on Activity",
    "section": "Some excercise - Problem 1",
    "text": "Some excercise - Problem 1\nTry filling in the gap in the following table.\n\n\n\ntid\ntoken\ndep\nhead\n\n\n\n\n1\nI\n\n\n\n\n2\nlove\nROOT\n\n\n\n3\nbeef\n\n\n\n\n4\ntongue\n\n\n\n\n5\n.\npunct\n2"
  },
  {
    "objectID": "2025/slides/session-11.html#some-excercise---problem-2",
    "href": "2025/slides/session-11.html#some-excercise---problem-2",
    "title": "Session 11: Hands-on Activity",
    "section": "Some excercise - Problem 2",
    "text": "Some excercise - Problem 2\nTry filling in the gap in the following table.\n\n\n\ntid\ntoken\ndep\nhead\n\n\n\n\n1\nThe\n\n\n\n\n2\ncat\n\n\n\n\n3\nsleeps\nROOT\n\n\n\n4\non\n\n\n\n\n5\nthe\n\n\n\n\n6\nmat\n\n\n\n\n7\n.\npunct\n3"
  },
  {
    "objectID": "2025/slides/session-11.html#some-excercise---problem-3",
    "href": "2025/slides/session-11.html#some-excercise---problem-3",
    "title": "Session 11: Hands-on Activity",
    "section": "Some excercise - Problem 3",
    "text": "Some excercise - Problem 3\nTry filling in the gap in the following table.\n\n\n\ntid\ntoken\ndep\nhead\n\n\n\n\n1\nShe\n\n\n\n\n2\nquickly\n\n\n\n\n3\nreads\nROOT\n\n\n\n4\ninteresting\n\n\n\n\n5\nbooks\n\n\n\n\n6\n.\npunct\n3"
  },
  {
    "objectID": "2025/slides/session-11.html#about-google-colab-5-mins",
    "href": "2025/slides/session-11.html#about-google-colab-5-mins",
    "title": "Session 11: Hands-on Activity",
    "section": "About Google Colab (5 mins)",
    "text": "About Google Colab (5 mins)"
  },
  {
    "objectID": "2025/slides/session-11.html#python-basics-15-mins",
    "href": "2025/slides/session-11.html#python-basics-15-mins",
    "title": "Session 11: Hands-on Activity",
    "section": "Python basics (15 mins)",
    "text": "Python basics (15 mins)"
  },
  {
    "objectID": "2025/slides/session-11.html#first-text-analysis",
    "href": "2025/slides/session-11.html#first-text-analysis",
    "title": "Session 11: Hands-on Activity",
    "section": "First text analysis",
    "text": "First text analysis\n\nFirst thing is to load the package.\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\nThen you will define a variable example_text\n\nexample_text = \"Hi. This is my first awesome sentence to analyze.\"\n\nAnalyze this using spacy\n\ndoc = nlp(example_text)"
  },
  {
    "objectID": "2025/slides/session-11.html#result-of-your-first-text-analysis",
    "href": "2025/slides/session-11.html#result-of-your-first-text-analysis",
    "title": "Session 11: Hands-on Activity",
    "section": "Result of your first text analysis",
    "text": "Result of your first text analysis\n\nLet‚Äôs print analysis results\n\nfor token in doc:\n    print(token.text, token.pos_, token.tag_, sep=\"\\t\")"
  },
  {
    "objectID": "2025/slides/session-11.html#lets-parse-the-sentence.",
    "href": "2025/slides/session-11.html#lets-parse-the-sentence.",
    "title": "Session 11: Hands-on Activity",
    "section": "Let‚Äôs parse the sentence.",
    "text": "Let‚Äôs parse the sentence.\n\nVisit our webapp\nTry the sentences above and analyze their dependencies"
  },
  {
    "objectID": "2025/slides/session-11.html#in-what-way-are-these-sentence-complex",
    "href": "2025/slides/session-11.html#in-what-way-are-these-sentence-complex",
    "title": "Session 11: Hands-on Activity",
    "section": "In what way are these sentence complex?",
    "text": "In what way are these sentence complex?\nDescribe complexification strategies:\n\nShe hopes to join an international research team after graduation.\nExperts agree that collaboration improves problem-solving efficiency.\nStudents often struggle because they lack sufficient guidance.\nHe succeeded in the most demanding and competitive program at the university.\nThe growing influence of social media on youth behavior is concerning.\nPolicies that encourage innovation are essential for economic growth."
  },
  {
    "objectID": "2025/slides/session-11.html#now-try-your-own-examples",
    "href": "2025/slides/session-11.html#now-try-your-own-examples",
    "title": "Session 11: Hands-on Activity",
    "section": "Now try your own examples",
    "text": "Now try your own examples"
  },
  {
    "objectID": "2025/slides/session-11.html#questions",
    "href": "2025/slides/session-11.html#questions",
    "title": "Session 11: Hands-on Activity",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "2025/slides/session-8.html#learning-objectives",
    "href": "2025/slides/session-8.html#learning-objectives",
    "title": "Session 8: Hands-on activity #4",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nSearch for window-based collocations and n-grams in AntConc\nCalculate commonly used strengths of association measures by hand using spreadsheet software\nDiscuss benefits and drawbacks of different strength of association measures"
  },
  {
    "objectID": "2025/slides/session-8.html#corpus-lab-2-submission",
    "href": "2025/slides/session-8.html#corpus-lab-2-submission",
    "title": "Session 8: Hands-on activity #4",
    "section": "Corpus Lab 2: Submission",
    "text": "Corpus Lab 2: Submission\n\n\nTask 1: Japanese Word Frequency List and small write-up (5 points)\nTask 2: Replication of Durrant‚Äôs analysis from Figure 4.19 (5 points)\n\nresearch question,\nhypothesis,\nplots, and\nresults\n\nTask 3: Comparison of two texts in terms of lexical sophistication (5 points)"
  },
  {
    "objectID": "2025/slides/session-8.html#corpus-lab-2-task-1",
    "href": "2025/slides/session-8.html#corpus-lab-2-task-1",
    "title": "Session 8: Hands-on activity #4",
    "section": "Corpus Lab 2: Task 1",
    "text": "Corpus Lab 2: Task 1\n\nInstruction"
  },
  {
    "objectID": "2025/slides/session-8.html#task-1-compile-a-japanese-word-frequency-list",
    "href": "2025/slides/session-8.html#task-1-compile-a-japanese-word-frequency-list",
    "title": "Session 8: Hands-on activity #4",
    "section": "Task 1: Compile a Japanese Word Frequency list",
    "text": "Task 1: Compile a Japanese Word Frequency list\n\nTask\nCompile a Japanese frequency list based on a corpus.\nResource\n\nDownload a Japanese text Aozora 500 from Google Drive.\nUse AntConc, TagAnt, and Simple Text Analyzer."
  },
  {
    "objectID": "2025/slides/session-8.html#submission-for-task-1",
    "href": "2025/slides/session-8.html#submission-for-task-1",
    "title": "Session 8: Hands-on activity #4",
    "section": "Submission for task 1",
    "text": "Submission for task 1\n\n\nSubmit a frequency list .tsv or .txt.\nA short description of word frequency pattern in Japanese.\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\nYour submission ‚Ä¶\n\nincludes a frequency list of Japanese words based on Aozora 500\nprovides a description of word-frequency patterns in Aozora 500 using example words from each frequency bins"
  },
  {
    "objectID": "2025/slides/session-8.html#corpus-lab-2-task-2",
    "href": "2025/slides/session-8.html#corpus-lab-2-task-2",
    "title": "Session 8: Hands-on activity #4",
    "section": "Corpus Lab 2: Task 2",
    "text": "Corpus Lab 2: Task 2"
  },
  {
    "objectID": "2025/slides/session-8.html#instruction",
    "href": "2025/slides/session-8.html#instruction",
    "title": "Session 8: Hands-on activity #4",
    "section": "Instruction",
    "text": "Instruction\nGoal: to replicate analysis on GiG.\nYou will need to have access to both metadata file.\nThe corpus data is here."
  },
  {
    "objectID": "2025/slides/session-8.html#about-gig-meta-data",
    "href": "2025/slides/session-8.html#about-gig-meta-data",
    "title": "Session 8: Hands-on activity #4",
    "section": "About GiG meta data",
    "text": "About GiG meta data\n\nGiG metadata documents the necessary data to use for plotting\n\nYear Group (X-axis in Figure 4.19)\nGenre (grouping variable in Figure 4.19)\n\n\n\nGiG Metadata"
  },
  {
    "objectID": "2025/slides/session-8.html#writing-up-research-question-hypothesis-and-results",
    "href": "2025/slides/session-8.html#writing-up-research-question-hypothesis-and-results",
    "title": "Session 8: Hands-on activity #4",
    "section": "Writing up research question, hypothesis, and results",
    "text": "Writing up research question, hypothesis, and results\n\n\nResearch question?\nHypothesis: Write your own.\nResults: Write your own.\n\n\n\n\n\n\n\nSuccess Criteria\n\n\nYour submission ‚Ä¶\n\nincludes a spreatsheet that contains lexical diversity (i.e., TTR, Root TTR, Log TTR, MAAS) scores for the sample texts\nprovides two sets of plots that describe trends of lexical diversity across year groups and genre.\nprovides 200-300 word replication report on lexical diversity trends in GiG corpus, presented as Figure 4.19 in Durrant (2023)."
  },
  {
    "objectID": "2025/slides/session-8.html#corpus-lab-2-task-3",
    "href": "2025/slides/session-8.html#corpus-lab-2-task-3",
    "title": "Session 8: Hands-on activity #4",
    "section": "Corpus Lab 2: Task 3",
    "text": "Corpus Lab 2: Task 3"
  },
  {
    "objectID": "2025/slides/session-8.html#comparing-lexical-characteristics-of-two-texts",
    "href": "2025/slides/session-8.html#comparing-lexical-characteristics-of-two-texts",
    "title": "Session 8: Hands-on activity #4",
    "section": "Comparing lexical characteristics of two texts",
    "text": "Comparing lexical characteristics of two texts\n\nGoals\n\nCompare and contrast two texts along with several lexical diversity metrics\nObserve differences in single-word and multiword sophistication\n\nData\n\nChoose two texts from the ICNALE GRA\nIf you are unsure, use choose from the following three files:\n\nGRA_PTJ0_124_ORIG.txt\nGRA_PTJ0_070_ORIG.txt\nGRA_PTJ0_112_ORIG.txt"
  },
  {
    "objectID": "2025/slides/session-8.html#step-1-qualitatively-compare-two-files",
    "href": "2025/slides/session-8.html#step-1-qualitatively-compare-two-files",
    "title": "Session 8: Hands-on activity #4",
    "section": "Step 1: Qualitatively compare two files",
    "text": "Step 1: Qualitatively compare two files\n\nBefore we actually obtain lexical sophistication measures, compare two texts in terms of their lexical use.\nIn pairs, describe the strengths and weakeness in vocabulary use."
  },
  {
    "objectID": "2025/slides/session-8.html#step-2-hypothesis",
    "href": "2025/slides/session-8.html#step-2-hypothesis",
    "title": "Session 8: Hands-on activity #4",
    "section": "Step 2: Hypothesis",
    "text": "Step 2: Hypothesis\n\nIn what way is one text more lexically sophisticated than the other?\n\nTry to come up with characteristics that describe the quality of word use in each text"
  },
  {
    "objectID": "2025/slides/session-8.html#step-3-pick-two-or-three-lexical-sophistication-variables",
    "href": "2025/slides/session-8.html#step-3-pick-two-or-three-lexical-sophistication-variables",
    "title": "Session 8: Hands-on activity #4",
    "section": "Step 3: Pick two or three lexical sophistication variables",
    "text": "Step 3: Pick two or three lexical sophistication variables\nEnter the text into analyzer\nWe can also compare two texts in simple text analyzer.\n\ntwo-text"
  },
  {
    "objectID": "2025/slides/session-8.html#step-4-run-analyses",
    "href": "2025/slides/session-8.html#step-4-run-analyses",
    "title": "Session 8: Hands-on activity #4",
    "section": "Step 4: Run analyses",
    "text": "Step 4: Run analyses\nPlots that compares two lists\n!"
  },
  {
    "objectID": "2025/slides/session-8.html#step-5-interpret-the-findings",
    "href": "2025/slides/session-8.html#step-5-interpret-the-findings",
    "title": "Session 8: Hands-on activity #4",
    "section": "Step 5: Interpret the findings",
    "text": "Step 5: Interpret the findings\n\nLet‚Äôs discuss how the two text differ in lexical use from one another.\nUse the tables with token information and visualization to (dis)confirm your hypothesis\n\n\n\n\n\n\n\nSuccess Criteria\n\n\nYour submission ‚Ä¶\n\nincludes plots from one frequency index and one other type of index\nprovides desciption of how two texts differ in terms of the selected lexical sophistication indices."
  },
  {
    "objectID": "2025/slides/session-8.html#expected-ocurrences-correction",
    "href": "2025/slides/session-8.html#expected-ocurrences-correction",
    "title": "Session 8: Hands-on activity #4",
    "section": "Expected Ocurrences (Correction)",
    "text": "Expected Ocurrences (Correction)\n\nExpected frequency tries to get ‚Äúnumber of times two words occur together if they were truly independent at chance level.‚Äù\nExpected frequency are usually calculated as follows: \\[E_{11} = {(\\text{freq of node word} * \\text{freq of collocate } ) \\over Corpus size}\\]\nIf word1 and word 2 occur 500 times each in a million word corpus‚Ä¶\n\n\n(500 * 500) / 1000000\n\n[1] 0.25"
  },
  {
    "objectID": "2025/slides/session-8.html#differences-between-expected-frequency-joint-probability",
    "href": "2025/slides/session-8.html#differences-between-expected-frequency-joint-probability",
    "title": "Session 8: Hands-on activity #4",
    "section": "Differences between expected frequency & Joint probability",
    "text": "Differences between expected frequency & Joint probability\nThey are mathematical conversion between the two.\n$ = {500 } $\nThis is probability, to convert back to COUNT over all corpus, you multiply the corpus size\n\\(\\text{Expected frequency} = {500 \\over 1000000} \\times {500 \\over 1000000} \\times 1000000\\)"
  },
  {
    "objectID": "2025/slides/session-8.html#generating-n-gram-list-with-antconc-10-mins",
    "href": "2025/slides/session-8.html#generating-n-gram-list-with-antconc-10-mins",
    "title": "Session 8: Hands-on activity #4",
    "section": "Generating N-gram list with AntConc (10 mins)",
    "text": "Generating N-gram list with AntConc (10 mins)\n\nLet‚Äôs generate a list of n-grams.\nYou can use either English or Japanese.\nFor Japanese you can use Aozora 500 data we used.\nFor English you can use Ame06 or B06 data in AntConc."
  },
  {
    "objectID": "2025/slides/session-8.html#three-word-sequences",
    "href": "2025/slides/session-8.html#three-word-sequences",
    "title": "Session 8: Hands-on activity #4",
    "section": "three-word sequences",
    "text": "three-word sequences\n\nYou can (a) load corpus, (b) go to N-gram, (c) apply settings, and (d) hit start.\n\n\nTrigram in BE06"
  },
  {
    "objectID": "2025/slides/session-8.html#five-word-sequences",
    "href": "2025/slides/session-8.html#five-word-sequences",
    "title": "Session 8: Hands-on activity #4",
    "section": "five-word sequences",
    "text": "five-word sequences\n\nNow let‚Äôs examine 5 word sequences\n\n\nQuintgram in BE06"
  },
  {
    "objectID": "2025/slides/session-8.html#jumping-to-kwic-view-from-the-list",
    "href": "2025/slides/session-8.html#jumping-to-kwic-view-from-the-list",
    "title": "Session 8: Hands-on activity #4",
    "section": "Jumping to KWIC view from the list",
    "text": "Jumping to KWIC view from the list\nIn AntConc, we can jump from the item in the list to show KWIC.\n\nKWIC"
  },
  {
    "objectID": "2025/slides/session-8.html#generating-p-frame-list-with-antconc",
    "href": "2025/slides/session-8.html#generating-p-frame-list-with-antconc",
    "title": "Session 8: Hands-on activity #4",
    "section": "Generating P-frame list with AntConc",
    "text": "Generating P-frame list with AntConc\n\nLet‚Äôs now generate p-frame.\nSet openslot to 1. Hit start. What do you see?\n\n\nP-frames"
  },
  {
    "objectID": "2025/slides/session-8.html#questions",
    "href": "2025/slides/session-8.html#questions",
    "title": "Session 8: Hands-on activity #4",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "2025/slides/session-8.html#recap-what-is-collocation",
    "href": "2025/slides/session-8.html#recap-what-is-collocation",
    "title": "Session 8: Hands-on activity #4",
    "section": "Recap: What is collocation?",
    "text": "Recap: What is collocation?\n\nCollocation\nnode word: play\ncollocates: role, game, sports, etc."
  },
  {
    "objectID": "2025/slides/session-8.html#collocation-in-antconc",
    "href": "2025/slides/session-8.html#collocation-in-antconc",
    "title": "Session 8: Hands-on activity #4",
    "section": "Collocation in AntConc",
    "text": "Collocation in AntConc\nYou can search collocation by entering node words\n\nOpen AntConc, load BROWN corpus.\nGo to Collocate\nEnter play in search window and hit Start"
  },
  {
    "objectID": "2025/slides/session-8.html#you-should-get-the-following.",
    "href": "2025/slides/session-8.html#you-should-get-the-following.",
    "title": "Session 8: Hands-on activity #4",
    "section": "You should get the following.",
    "text": "You should get the following.\n\nCollocation search in AntConc"
  },
  {
    "objectID": "2025/slides/session-8.html#try-a-few-different-search-terms.",
    "href": "2025/slides/session-8.html#try-a-few-different-search-terms.",
    "title": "Session 8: Hands-on activity #4",
    "section": "Try a few different search terms.",
    "text": "Try a few different search terms.\n\nAny node word you want to search?"
  },
  {
    "objectID": "2025/slides/session-8.html#some-options",
    "href": "2025/slides/session-8.html#some-options",
    "title": "Session 8: Hands-on activity #4",
    "section": "Some options",
    "text": "Some options\n\n\n\n\n\n\n\nOption name\nDescription\n\n\n\n\nWindow Span\nSpecifies how many words on the left or right do you consider as candidates.\n\n\nMin. freq\nhow many times the collocation must occur\n\n\nMin. Range\nhow many document must the collocation occur in"
  },
  {
    "objectID": "2025/slides/session-8.html#any-questions",
    "href": "2025/slides/session-8.html#any-questions",
    "title": "Session 8: Hands-on activity #4",
    "section": "Any questions?",
    "text": "Any questions?"
  },
  {
    "objectID": "2025/slides/session-8.html#calculating-soa-by-hand",
    "href": "2025/slides/session-8.html#calculating-soa-by-hand",
    "title": "Session 8: Hands-on activity #4",
    "section": "Calculating SOA by hand",
    "text": "Calculating SOA by hand\n\nOpen Google Sreadsheet.\nBuild function to calculate the following SOA measures\n\nT-score\nMutual Information\nMutual Information Squared (\\(MI^2\\))\nLogDice"
  },
  {
    "objectID": "2025/slides/session-8.html#preparation",
    "href": "2025/slides/session-8.html#preparation",
    "title": "Session 8: Hands-on activity #4",
    "section": "Preparation",
    "text": "Preparation\n\nCopy and paste word frequency list in frequency tab\n\n\nfrequency list"
  },
  {
    "objectID": "2025/slides/session-8.html#enter-the-node-and-collocates",
    "href": "2025/slides/session-8.html#enter-the-node-and-collocates",
    "title": "Session 8: Hands-on activity #4",
    "section": "Enter the node and collocates",
    "text": "Enter the node and collocates\n\nNow retrieve word frequencies from the frequency list, using function called vlookup.\n\n\nretrieve frequency"
  },
  {
    "objectID": "2025/slides/session-8.html#collocation-frequency-observed",
    "href": "2025/slides/session-8.html#collocation-frequency-observed",
    "title": "Session 8: Hands-on activity #4",
    "section": "Collocation frequency (observed)",
    "text": "Collocation frequency (observed)\n\nEnter node word in search window and look for FreqLR.\n\n\nObserved frequency"
  },
  {
    "objectID": "2025/slides/session-8.html#enter-observed-frequency-and-window-size",
    "href": "2025/slides/session-8.html#enter-observed-frequency-and-window-size",
    "title": "Session 8: Hands-on activity #4",
    "section": "Enter Observed frequency and window size",
    "text": "Enter Observed frequency and window size\n\nEnter O11"
  },
  {
    "objectID": "2025/slides/session-8.html#expected-frequency",
    "href": "2025/slides/session-8.html#expected-frequency",
    "title": "Session 8: Hands-on activity #4",
    "section": "Expected frequency",
    "text": "Expected frequency\nNow we will enter formula for the expected frequency.\n\\[E_{11} = {(freq_{node} * freq_{collocate} ) \\over Corpus size}\\]\n\nThis formula says:\n\nThe expected frequency of collocation would be joint probability of the two words."
  },
  {
    "objectID": "2025/slides/session-8.html#expected-frequency-1",
    "href": "2025/slides/session-8.html#expected-frequency-1",
    "title": "Session 8: Hands-on activity #4",
    "section": "Expected frequency",
    "text": "Expected frequency\n\n\\(E_{11} = {(freq_{node} * freq_{collocate} ) \\over Corpus size}\\)\n\n\nexpected-frequency"
  },
  {
    "objectID": "2025/slides/session-8.html#mutual-information",
    "href": "2025/slides/session-8.html#mutual-information",
    "title": "Session 8: Hands-on activity #4",
    "section": "Mutual Information",
    "text": "Mutual Information\nFinally, we will enter the following formula.\n\\[MI = {log_2{ Observed freq \\over Expected frequency }}\\]\n\nCalculating MI"
  },
  {
    "objectID": "2025/slides/session-8.html#wait",
    "href": "2025/slides/session-8.html#wait",
    "title": "Session 8: Hands-on activity #4",
    "section": "Wait ‚Ä¶",
    "text": "Wait ‚Ä¶\n\nOur calculation shows that MI = 8.69\nAntConc says 5.377 ‚Ä¶\n\n\nAntConc MI"
  },
  {
    "objectID": "2025/slides/session-8.html#why-are-the-scores-different",
    "href": "2025/slides/session-8.html#why-are-the-scores-different",
    "title": "Session 8: Hands-on activity #4",
    "section": "Why are the scores different?",
    "text": "Why are the scores different?\n\nI was also confused‚Ä¶\nIt appears based on this conversation in google group that corpus tools are adjusting the observed frequency by multiplying window size.\nThis means that Expected frequency becomes\n\n\\(E_{11} = {(freq_{node} * freq_{collocate} * \\color{red}{window size}) \\over Corpus size}\\)"
  },
  {
    "objectID": "2025/slides/session-8.html#okay..-now-what",
    "href": "2025/slides/session-8.html#okay..-now-what",
    "title": "Session 8: Hands-on activity #4",
    "section": "Okay.. now what?",
    "text": "Okay.. now what?\nLet‚Äôs fix the expected frequency count.\n\nFixed expected frequency\nOkay close enough!"
  },
  {
    "objectID": "2025/slides/session-8.html#confirming-with-casual-conc",
    "href": "2025/slides/session-8.html#confirming-with-casual-conc",
    "title": "Session 8: Hands-on activity #4",
    "section": "Confirming with Casual Conc",
    "text": "Confirming with Casual Conc\n\nOur score actually is also close to that of Casual Conc\n\n\nCollocation"
  },
  {
    "objectID": "2025/slides/session-8.html#take-away",
    "href": "2025/slides/session-8.html#take-away",
    "title": "Session 8: Hands-on activity #4",
    "section": "Take-away‚Ä¶",
    "text": "Take-away‚Ä¶\n\nEven widely used corpus software may give you slightly different SOA scores, because of possible differences in counting strategies, tokenization, etc..\nAntConc and Casual Conc likely uses:\n\n\\(E_{11} = {(freq_{node} * freq_{collocate} * \\color{red}{window size}) \\over Corpus size}\\)\n\nIt is very important to triple-check your work and‚Ä¶"
  },
  {
    "objectID": "2025/slides/session-8.html#report-exactly-what-you-did-for-replication",
    "href": "2025/slides/session-8.html#report-exactly-what-you-did-for-replication",
    "title": "Session 8: Hands-on activity #4",
    "section": "Report exactly what you did (for replication)",
    "text": "Report exactly what you did (for replication)\nReplicability is key to science.\nFor example, you might say.\n\ne.g., I used AntCont version 4.2.x\ne.g., MI was calculated using the following formula\n\n\\(MI = {log_2{ Observed freq \\over Expected frequency }}\\)\nwhere the expected frequency is calculated as:\n\\(E_{11} = {(freq_{node} * freq_{collocate} * \\color{red}{window size}) \\over Corpus size}\\)"
  },
  {
    "objectID": "2025/slides/session-8.html#questions-1",
    "href": "2025/slides/session-8.html#questions-1",
    "title": "Session 8: Hands-on activity #4",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "2025/slides/session-8.html#lets-finish-it-up",
    "href": "2025/slides/session-8.html#lets-finish-it-up",
    "title": "Session 8: Hands-on activity #4",
    "section": "Let‚Äôs finish it up",
    "text": "Let‚Äôs finish it up\n\nR1 = Frequency of node word\nC1 = Frequency of collocate\n\\(MI = {log_2{ Observed freq \\over Expected frequency }}\\)\n\\(\\text{T-score} = {\\text{Observed}  - \\text{Expected}  \\over \\sqrt{Observed}}\\)\n\\(\\text{log Dice} = 14 + \\log_2( {{2 \\times Observed} \\over {R_1 + C_1}})\\)"
  },
  {
    "objectID": "2025/slides/session-8.html#results-should-look-like-something",
    "href": "2025/slides/session-8.html#results-should-look-like-something",
    "title": "Session 8: Hands-on activity #4",
    "section": "Results should look like something",
    "text": "Results should look like something\n\nLogDice:\n\nOurs: 6.548\nAntConc: 6.551 Likely multiplying window size in the denominator.\nCasualConc: 11.14 ???\n\nT-score:\n\nOurs: 2.927\nAntConc: 2.928 (Almost identical)\nCasualConc: 2.99 (close)"
  },
  {
    "objectID": "2025/slides/session-8.html#if-time-allowed-how-can-i-generate-a-collocation-list",
    "href": "2025/slides/session-8.html#if-time-allowed-how-can-i-generate-a-collocation-list",
    "title": "Session 8: Hands-on activity #4",
    "section": "(If time allowed) how can I generate a collocation list?",
    "text": "(If time allowed) how can I generate a collocation list?\n\nYou can take a look at the following notebook"
  },
  {
    "objectID": "2025/slides/session-8.html#reflection",
    "href": "2025/slides/session-8.html#reflection",
    "title": "Session 8: Hands-on activity #4",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now do the followings:\n\nGenerate lists of n-grams and p-frames using AntConc.\nSearch for collocates with AntConc.\nCalculate major Strengths of Association (SOA) measures by hand."
  },
  {
    "objectID": "2025/assignments/hands-on-4/draft.html",
    "href": "2025/assignments/hands-on-4/draft.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/hands-on-3/draft.html",
    "href": "2025/assignments/hands-on-3/draft.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/hands-on-2/draft.html",
    "href": "2025/assignments/hands-on-2/draft.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html",
    "href": "2025/assignments/hands-on-1/index.html",
    "title": "Corpus Lab 1",
    "section": "",
    "text": "This assignment aims to help you practice the following skills:\n\nArticulating linguistic research questions for corpus-driven research\nExplaining corpus choices and analysis approach for basic corpus search\nInterpreting and describing the basic concordance search results",
    "crumbs": [
      "Assignments",
      "Corpus Lab 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html#task-1-constructing-corpus-search-research-question-and-hypothesis",
    "href": "2025/assignments/hands-on-1/index.html#task-1-constructing-corpus-search-research-question-and-hypothesis",
    "title": "Corpus Lab 1",
    "section": "Task 1: Constructing corpus-search research question and hypothesis",
    "text": "Task 1: Constructing corpus-search research question and hypothesis\nIn this task, you are asked to articulate your research question and hypothesis for your first corpus assignment. Use the following information as guideline.\n\nResearch question:\n\nResearch questions should be answerable. Follow the class material to construct a intruiging research question that you can answer with basic corpus search skills you learned.\nWrite one or two research questions you would like to answer through this assignment.\n\n\n\nHypothesis:\n\nOnce you decided on research question, state your hypothesis.\nUse the course material to help you articulate research hypothesis.\nWrite a short paragraph stating your hypothesis and why you think your research hypotheses may be true.",
    "crumbs": [
      "Assignments",
      "Corpus Lab 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html#task-2-methods",
    "href": "2025/assignments/hands-on-1/index.html#task-2-methods",
    "title": "Corpus Lab 1",
    "section": "Task 2: Methods",
    "text": "Task 2: Methods\nIn this task, you are asked to describe the methods choice of your corpus search. This should include justifications for the corpora used, type of corpus search used.\n\nCorpora\n\nWhich corpora or sub-section of a corpus would you conduct a search on? Why? Justify your choice in a paragraph.\n\n\n\nSearch Methods and Plan\n\nWhat corpus search methods would you choose and why? In this assignment, search methods mainly include methods available through English-Corpora.org, including LIST, Chart, Collocates, Compare and KWIC.\nIn what way are you planning to conduct the search and what kind of information are you expecting from it?",
    "crumbs": [
      "Assignments",
      "Corpus Lab 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/index.html#task-3-result",
    "href": "2025/assignments/hands-on-1/index.html#task-3-result",
    "title": "Corpus Lab 1",
    "section": "Task 3: Result",
    "text": "Task 3: Result\nIn this part of the assignment, you are asked to describe the search results and provide interpretations on the findings.\n\nCorpus search results: Provide some numbers or discourse samples based on your corpus search. This can be frequency list, table with frequency counts, or a copy of KWIC results.\nInterpretation: Write a paragraph, providing some interpretations of your findings.\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission ‚Ä¶\n\nincludes one or two research questions\narticulates hypotheses\ninclude a list of corpora used\njustifies the selection of corpus\noutlines the search plan (key phrase, regular expression, sorting, filtering, etc.)\nincludes results of your search\nprovides interpretation of the findings.",
    "crumbs": [
      "Assignments",
      "Corpus Lab 1"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html",
    "href": "2025/assignments/hands-on-1/draft.html",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "",
    "text": "How does the frequency of word X change over time, and what do the concordance lines reveal about changes in usage?\nHow does the frequency of word X differ across text types Z, and what patterns appear in the KWIC display?\nWhat is the frequency distribution of grammatical pattern X across different text types, and what contexts does KWIC reveal?\nHow has the usage of word X evolved in the past Y years based on frequency trends and concordance evidence?\nWhat are the typical contexts in which expression X appears based on concordance analysis?\nHow do different varieties of language Y (e.g., British vs.¬†American English) differ in frequency and contexts of feature X?\nWhat patterns emerge when comparing the frequency and KWIC results for synonyms X and Y?\nHow does register (formal/informal) affect both the frequency and typical contexts of linguistic feature X?\nWhich words frequently appear near word X based on manual analysis of concordance lines?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-change-over-time-and-what-do-the-concordance-lines-reveal-about-changes-in-usage",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-change-over-time-and-what-do-the-concordance-lines-reveal-about-changes-in-usage",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How does the frequency of word X change over time, and what do the concordance lines reveal about changes in usage?",
    "text": "Template: How does the frequency of word X change over time, and what do the concordance lines reveal about changes in usage?\n\nExample Questions:\n\nHow does the frequency of ‚Äúemail‚Äù change from 1990-2020, and what do concordance lines reveal about its usage evolution?\nHow has the frequency of ‚Äúglobal warming‚Äù vs ‚Äúclimate change‚Äù shifted over decades, and what contexts show this shift?\nHow does the frequency of ‚Äúshall‚Äù change over time in American English, and what do KWIC results show about its declining contexts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-differ-across-text-types-z-and-what-patterns-appear-in-the-kwic-display",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-does-the-frequency-of-word-x-differ-across-text-types-z-and-what-patterns-appear-in-the-kwic-display",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How does the frequency of word X differ across text types Z, and what patterns appear in the KWIC display?",
    "text": "Template: How does the frequency of word X differ across text types Z, and what patterns appear in the KWIC display?\n\nExample Questions:\n\nHow does the frequency of ‚Äútherefore‚Äù differ between academic and newspaper texts, and what sentence positions does it occupy in each?\nHow does the frequency of contractions (e.g., ‚Äúdon‚Äôt‚Äù) vary between spoken and written corpora, and what patterns emerge in KWIC?\nHow does the frequency of ‚Äúget‚Äù differ across fiction vs.¬†academic writing, and what meanings predominate in each genre?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-what-is-the-frequency-distribution-of-grammatical-pattern-x-across-different-text-types-and-what-contexts-does-kwic-reveal",
    "href": "2025/assignments/hands-on-1/draft.html#template-what-is-the-frequency-distribution-of-grammatical-pattern-x-across-different-text-types-and-what-contexts-does-kwic-reveal",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: What is the frequency distribution of grammatical pattern X across different text types, and what contexts does KWIC reveal?",
    "text": "Template: What is the frequency distribution of grammatical pattern X across different text types, and what contexts does KWIC reveal?\n\nExample Questions:\n\nWhat is the frequency of ‚Äúthere is/are‚Äù constructions in spoken vs.¬†written English, and what follows this pattern in each?\nHow frequent is the ‚Äúnot only‚Ä¶but also‚Äù construction across different registers, and what types of elements does it connect?\nWhat is the distribution of passive voice (‚Äúwas/were + past participle‚Äù) in news vs.¬†academic texts, and what verbs commonly appear?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-has-the-usage-of-word-x-evolved-in-the-past-y-years-based-on-frequency-trends-and-concordance-evidence",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-has-the-usage-of-word-x-evolved-in-the-past-y-years-based-on-frequency-trends-and-concordance-evidence",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How has the usage of word X evolved in the past Y years based on frequency trends and concordance evidence?",
    "text": "Template: How has the usage of word X evolved in the past Y years based on frequency trends and concordance evidence?\n\nExample Questions:\n\nHow has the usage of ‚Äúgay‚Äù evolved from 1950-2020 based on frequency and changing contexts in concordance lines?\nHow has ‚Äúliterally‚Äù changed in frequency and usage patterns over the past 30 years?\nHow has the word ‚Äúviral‚Äù evolved in meaning from 1990-2020 based on concordance evidence?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-what-are-the-typical-contexts-in-which-expression-x-appears-based-on-concordance-analysis",
    "href": "2025/assignments/hands-on-1/draft.html#template-what-are-the-typical-contexts-in-which-expression-x-appears-based-on-concordance-analysis",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: What are the typical contexts in which expression X appears based on concordance analysis?",
    "text": "Template: What are the typical contexts in which expression X appears based on concordance analysis?\n\nExample Questions:\n\nIn what contexts does the phrase ‚Äúat the end of the day‚Äù typically appear, and is it more common in spoken or written English?\nWhat are the typical contexts for ‚Äúon the other hand‚Äù and what usually precedes it?\nIn what contexts does ‚Äúfrankly speaking‚Äù appear, and what types of statements follow it?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-do-different-varieties-of-language-y-differ-in-frequency-and-contexts-of-feature-x",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-do-different-varieties-of-language-y-differ-in-frequency-and-contexts-of-feature-x",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How do different varieties of language Y differ in frequency and contexts of feature X?",
    "text": "Template: How do different varieties of language Y differ in frequency and contexts of feature X?\n\nExample Questions:\n\nHow do British and American English differ in the frequency and contexts of ‚Äúquite‚Äù?\nWhat is the frequency difference of ‚Äúshall‚Äù between British and American English, and in what contexts does each variety use it?\nHow do British and American English differ in using ‚Äúat the weekend‚Äù vs.¬†‚Äúon the weekend‚Äù?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-what-patterns-emerge-when-comparing-the-frequency-and-kwic-results-for-synonyms-x-and-y",
    "href": "2025/assignments/hands-on-1/draft.html#template-what-patterns-emerge-when-comparing-the-frequency-and-kwic-results-for-synonyms-x-and-y",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: What patterns emerge when comparing the frequency and KWIC results for synonyms X and Y?",
    "text": "Template: What patterns emerge when comparing the frequency and KWIC results for synonyms X and Y?\n\nExample Questions:\n\nWhat patterns emerge when comparing ‚Äúbig‚Äù vs.¬†‚Äúlarge‚Äù in terms of frequency and the nouns they modify?\nHow do ‚Äúbegin‚Äù and ‚Äústart‚Äù differ in frequency and grammatical patterns (begin to/begin -ing)?\nWhat differences appear between ‚Äúbuy‚Äù and ‚Äúpurchase‚Äù in frequency across registers and typical objects?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-how-does-register-affect-both-the-frequency-and-typical-contexts-of-linguistic-feature-x",
    "href": "2025/assignments/hands-on-1/draft.html#template-how-does-register-affect-both-the-frequency-and-typical-contexts-of-linguistic-feature-x",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: How does register affect both the frequency and typical contexts of linguistic feature X?",
    "text": "Template: How does register affect both the frequency and typical contexts of linguistic feature X?\n\nExample Questions:\n\nHow does register (academic vs.¬†conversational) affect the frequency and contexts of ‚Äúhowever‚Äù?\nHow do formal and informal registers differ in the frequency and usage of phrasal verbs like ‚Äúfind out‚Äù vs.¬†‚Äúdiscover‚Äù?\nHow does register influence the frequency and positioning of ‚Äúmoreover‚Äù and ‚Äúbesides‚Äù?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#template-which-words-frequently-appear-near-word-x-based-on-manual-analysis-of-concordance-lines",
    "href": "2025/assignments/hands-on-1/draft.html#template-which-words-frequently-appear-near-word-x-based-on-manual-analysis-of-concordance-lines",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Template: Which words frequently appear near word X based on manual analysis of concordance lines?",
    "text": "Template: Which words frequently appear near word X based on manual analysis of concordance lines?\n\nExample Questions:\n\nWhich words frequently appear immediately before and after ‚Äúdecision‚Äù in business English?\nWhat words commonly appear within 3 words of ‚Äúabsolutely‚Äù in spoken English?\nWhich adjectives most frequently appear before ‚Äúconsequences‚Äù in news texts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#social-media-and-internet-language",
    "href": "2025/assignments/hands-on-1/draft.html#social-media-and-internet-language",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Social Media and Internet Language",
    "text": "Social Media and Internet Language\n\nExample Questions:\n\nHow has the frequency of ‚Äúlol‚Äù changed from 2000-2020, and in what contexts does it appear beyond informal communication?\nWhat is the frequency difference of ‚Äúselfie‚Äù before and after 2010, and what verbs commonly appear with it?\nHow do ‚Äúemoji‚Äù and ‚Äúemoticon‚Äù differ in frequency over time, and what contexts show their usage patterns?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#pop-culture-and-entertainment",
    "href": "2025/assignments/hands-on-1/draft.html#pop-culture-and-entertainment",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Pop Culture and Entertainment",
    "text": "Pop Culture and Entertainment\n\nExample Questions:\n\nHow does the frequency of ‚Äúbinge-watch‚Äù compare to ‚Äúmarathon‚Äù (in TV context) and when did each term become popular?\nWhat patterns emerge when comparing ‚ÄúK-pop‚Äù mentions across different text types and time periods?\nHow has ‚Äúanime‚Äù increased in frequency in English corpora, and what words commonly co-occur with it?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#technology-and-gaming",
    "href": "2025/assignments/hands-on-1/draft.html#technology-and-gaming",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Technology and Gaming",
    "text": "Technology and Gaming\n\nExample Questions:\n\nHow do ‚Äúapp‚Äù and ‚Äúapplication‚Äù differ in frequency across registers, and which one dominates in informal contexts?\nWhat is the frequency evolution of ‚ÄúGoogle‚Äù as a verb from 2000-2020, and what objects follow it?\nHow has ‚Äústream/streaming‚Äù changed in meaning from 2000-2020 based on concordance contexts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#language-learning-and-education",
    "href": "2025/assignments/hands-on-1/draft.html#language-learning-and-education",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Language Learning and Education",
    "text": "Language Learning and Education\n\nExample Questions:\n\nHow do native speakers use ‚Äúactually‚Äù vs how it appears in academic writing by non-native speakers?\nWhat is the frequency of ‚ÄúI think‚Äù vs ‚ÄúIn my opinion‚Äù across spoken and written English?\nHow do apologetic expressions like ‚Äúsorry‚Äù and ‚Äúexcuse me‚Äù differ in frequency and contexts?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#food-and-lifestyle",
    "href": "2025/assignments/hands-on-1/draft.html#food-and-lifestyle",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Food and Lifestyle",
    "text": "Food and Lifestyle\n\nExample Questions:\n\nHow has ‚Äúbubble tea‚Äù entered English usage, and what verbs are associated with it?\nWhat is the frequency difference between ‚Äúsushi‚Äù in the 1990s vs 2010s, and how have its contexts changed?\nHow do ‚Äúramen‚Äù and ‚Äúnoodles‚Äù compare in frequency and what adjectives modify each?"
  },
  {
    "objectID": "2025/assignments/hands-on-1/draft.html#common-efl-learner-interests",
    "href": "2025/assignments/hands-on-1/draft.html#common-efl-learner-interests",
    "title": "Hands-on Assignment 1 - Answer Key",
    "section": "Common EFL Learner Interests",
    "text": "Common EFL Learner Interests\n\nExample Questions:\n\nHow does ‚Äúdeadline‚Äù appear in different registers, and what verbs commonly precede it (meet/miss/extend)?\nWhat patterns show the difference between ‚Äútake a test‚Äù vs ‚Äútake an exam‚Äù in various English varieties?\nHow do ‚Äúpart-time job‚Äù and ‚Äúinternship‚Äù differ in frequency and contexts across text types?"
  },
  {
    "objectID": "2025/assignments/final-project/index.html",
    "href": "2025/assignments/final-project/index.html",
    "title": "Final Project",
    "section": "",
    "text": "For the final assignment of the current 5-day intensive course, I have two possible plans we can discuss and decide together.\nLet‚Äôs decide by the end of Day 2.",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/final-project/index.html#project-guidelines",
    "href": "2025/assignments/final-project/index.html#project-guidelines",
    "title": "Final Project",
    "section": "",
    "text": "For the final assignment of the current 5-day intensive course, I have two possible plans we can discuss and decide together.\nLet‚Äôs decide by the end of Day 2.",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/final-project/index.html#option-a---conducting-a-separate-mini-project",
    "href": "2025/assignments/final-project/index.html#option-a---conducting-a-separate-mini-project",
    "title": "Final Project",
    "section": "Option A - Conducting a separate mini-project",
    "text": "Option A - Conducting a separate mini-project\n\nOption A is more extensive in that you will be asked to conduct a new mini-project using the toolkit you have learned throughout the course.\nGiven the limited time, however, this plan requires a lot of commitment to the present course and may not be feasible, (but we can try if you‚Äôd like!).\nWe will discuss possible alternatives (like Option B below)",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/final-project/index.html#option-b---revisiting-one-of-the-completed-corpus-lab-assignments",
    "href": "2025/assignments/final-project/index.html#option-b---revisiting-one-of-the-completed-corpus-lab-assignments",
    "title": "Final Project",
    "section": "Option B - Revisiting one of the completed Corpus Lab assignments",
    "text": "Option B - Revisiting one of the completed Corpus Lab assignments\n\nAs this 5-day intensive course teaches you a lot of new techniques and approaches to analyze lingusitic data, it is important for us to revisit the already completed assignments and consolidate our skills.\nIn Option B, you will be asked to make a presentation on one of your previously completed Corpus Lab assignments, clearly articulating the thinking process as well as potential extension of your approach.\nMore details will be provided on the first day of the course.",
    "crumbs": [
      "Assignments",
      "Final Project"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-1/answer-key.html",
    "href": "2025/assignments/hands-on-1/answer-key.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2025/assignments/index.html",
    "href": "2025/assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "This section contains all course assignments and the final project.",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#overview",
    "href": "2025/assignments/index.html#overview",
    "title": "Assignments",
    "section": "",
    "text": "This section contains all course assignments and the final project.",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#assignments",
    "href": "2025/assignments/index.html#assignments",
    "title": "Assignments",
    "section": "Assignments",
    "text": "Assignments\n\nCorpus Lab 1\nCorpus Lab 2\nCorpus Lab 3\nCorpus Lab 4\nFinal Project",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#corpus-lab",
    "href": "2025/assignments/index.html#corpus-lab",
    "title": "Assignments",
    "section": "Corpus Lab",
    "text": "Corpus Lab\nEach of the four Corpus Lab assignments are evaluated by two criteria: Submission and Quality.\n\n\n\n\n\n\n\n\nGrading Criteria\nDetails\nScore\n\n\n\n\nSubmission on time\nThe first submission is recieved in time.\n3 points\n\n\nQuality\nThe submission meets criteria. You can resubmit your work by resubmission deadlines.\n4 points √ó 3 questions",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/index.html#corpus-lab-activity-grading-rubrics",
    "href": "2025/assignments/index.html#corpus-lab-activity-grading-rubrics",
    "title": "Assignments",
    "section": "Corpus Lab activity grading rubrics",
    "text": "Corpus Lab activity grading rubrics\n\n\n\n\n\n\n\nScore\nDescriptor\n\n\n\n\n4\n- All the required/essential components are addressed;  - The write-up is complete and easy to follow.\n\n\n3\n- One or two required/essential components are missing;  - The write-up lacks elaboration.\n\n\n2\n- Half of the required/essential components are missing;  - The write-up lacks critical information.\n\n\n1\n- Most of the required/essential components are missing;  - The write-up is outline based",
    "crumbs": [
      "Assignments",
      "Assignments"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html",
    "href": "2025/assignments/hands-on-2/index.html",
    "title": "Corpus Lab 2",
    "section": "",
    "text": "This assignment aims to help you practice the following skills:\n\nCreating a word frequency list based on a corpus of Japanese (as an example of non-English language)\nComputing and interpreting lexical diversity scores for English text samples\nComputing and interpreting lexical sophistication indices for English text samples",
    "crumbs": [
      "Assignments",
      "Corpus Lab 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html#task-1-a-japanese-word-frequency-list",
    "href": "2025/assignments/hands-on-2/index.html#task-1-a-japanese-word-frequency-list",
    "title": "Corpus Lab 2",
    "section": "Task 1: A Japanese word frequency List",
    "text": "Task 1: A Japanese word frequency List\n\nGoals\nThe goal of this task is to:\n\nConstruct a Japanese word frequency list.\n\n\n\nInstruction\n\nUse Aozora 500 corpus.\nCreate frequency list using TagAnt and AntConc\nUnderstand frequency distributions using simple text analyzer.\n\n\n\n\n\n\n\nSubmission:\n\n\n\n\nA Japanese word frequency list (.txt or .tsv format)\nDescriptive paragraphs explaining the frequency distributions of Japanese language.\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission ‚Ä¶\n\nincludes a frequency list of Japanese words based on Aozora 500\nprovides a description of word-frequency patterns in Aozora 500 using example words from each frequency bins",
    "crumbs": [
      "Assignments",
      "Corpus Lab 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html#task-2-replication-of-figure-4.19-from-durrant-with-two-more-recent-lexical-diversity-indices",
    "href": "2025/assignments/hands-on-2/index.html#task-2-replication-of-figure-4.19-from-durrant-with-two-more-recent-lexical-diversity-indices",
    "title": "Corpus Lab 2",
    "section": "Task 2: Replication of Figure 4.19 from Durrant with two more recent lexical diversity indices",
    "text": "Task 2: Replication of Figure 4.19 from Durrant with two more recent lexical diversity indices\n\nGoals\nThe goals of this task are:\n\nto compute more recent, robust alternatives to classical indices using TAALED\nto replicate Durrant‚Äôs analysis with two more recent lexical diversity indices\n\n\n\nInstructions\n\nComplete the hand calculation of lexical diversity indices on the spreadsheet\nCompute recommended lexical diversity indices ‚Äî MATTR and MTLD Original ‚Äî using TAALED.\nReplicate Figure 4.19 in Durrant (2023, p.¬†72) with the two indices (i.e., MATTR and MTLD Original)\nDiscuss implication of the findings.\n\n\n\n\n\n\n\nSubmission:\n\n\n\n\nSpreadsheet file containing hand-calculated lexical diversity scores.\nDescriptive paragraphs explaining the replication of Durrant‚Äôs analysis (300 words).\n\nResearch question\nYour hypothesis regarding the replication\nPlots (one for MTLD; the other for MATTR)\nResults and interpretation\n\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission ‚Ä¶\n\nincludes a spreatsheet that contains lexical diversity (i.e., TTR, Root TTR, Log TTR, MAAS) scores for the sample texts\nincludes a csv file\nprovides two sets of plots that describe trends of lexical diversity across year groups and genre.\nprovides 200-300 word replication report on lexical diversity trends in GiG corpus, presented as Figure 4.19 in Durrant (2023).",
    "crumbs": [
      "Assignments",
      "Corpus Lab 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-2/index.html#task-3-qualitative-analysis-of-lexical-sophistication",
    "href": "2025/assignments/hands-on-2/index.html#task-3-qualitative-analysis-of-lexical-sophistication",
    "title": "Corpus Lab 2",
    "section": "Task 3: Qualitative analysis of lexical sophistication",
    "text": "Task 3: Qualitative analysis of lexical sophistication\n\nGoal\nThe goals of this task are to:\n\ncompute several important lexical sophistication indices\ncompare and contrast two texts using the selected indices\ndescribe the use of vocabulary in the two text based on the quantitative and qualitative information\n\n\n\nInstructions\n\nTwo texts from the example used in the classroom\nUsing the simple text analyzer, compare two texts based on two indices that you select.\nSelect one frequency-based index and another type of index.\nInterpret the results of the analysis and describe the difference in a (few) paragraph(s).\n\n\n\n\n\n\n\nSubmission:\n\n\n\n\nPlots that contains results of the lexical sophistication analysis.\nDescriptive paragraph(s) contrasting two texts based on lexical sophistication (200-300 words).\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission ‚Ä¶\n\nincludes plots from one frequency index and one other type of index\nprovides desciption of how two texts differ in terms of the selected lexical sophistication indices.",
    "crumbs": [
      "Assignments",
      "Corpus Lab 2"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-3/index.html",
    "href": "2025/assignments/hands-on-3/index.html",
    "title": "Corpus Lab 3",
    "section": "",
    "text": "This assignment aims to help you practice the following skills:\n\nConstructing lists of formulaic language through concordance software\nPlanning and conducting a small-scale corpus study using single- and multi-word indices",
    "crumbs": [
      "Assignments",
      "Corpus Lab 3"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-3/index.html#task-1-describing-statistical-characteristics-of-collocations-4-points",
    "href": "2025/assignments/hands-on-3/index.html#task-1-describing-statistical-characteristics-of-collocations-4-points",
    "title": "Corpus Lab 3",
    "section": "Task 1: Describing statistical characteristics of collocations (4 points)",
    "text": "Task 1: Describing statistical characteristics of collocations (4 points)\nIn the first task, I would like you to calculate major Strengths Of Association (SOA) measures to quantify the association between two words (node words and their collocates.)\nThe frequency of node words, their collocates and entire corpus size will be given to you.\nYour task is to calculate T-score, MI, MI^2, and LogDice.\n\n\n\n\n\n\nSubmission\n\n\n\n\nA spreadsheet file with SOA values.\nA word file (.docx) for plots and prose descriptions.\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission ‚Ä¶\n\ncontains accurate T-score, MI, MI^2 and LogDice scores\nprovides visualization of the relations between SOA indices\ndescribe the relationships among SOA indices and typical collocations",
    "crumbs": [
      "Assignments",
      "Corpus Lab 3"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-3/index.html#task-2-3-mini-research-project-8-points-altogether",
    "href": "2025/assignments/hands-on-3/index.html#task-2-3-mini-research-project-8-points-altogether",
    "title": "Corpus Lab 3",
    "section": "Task 2 & 3: Mini-research project (8 points altogether)",
    "text": "Task 2 & 3: Mini-research project (8 points altogether)\nThe task 2 and 3 are related to the mini-research project.\nIn this part of the assignment, you will conduct a mini-research project to describe uses of single- and multi-word units in a corpus you choose.\nSpecifically, you will:\n\nselect lexical richness or phraseological sophistication indices to answer a set of research questions\nanalyze the chosen corpus with the selected indices\npresent the results and interpretation in a written prose\n\n\n\n\n\n\n\nSubmission\n\n\n\nThe final report are one-to-two page lengths report.\n\nShort background and Research Questions (one paragraph)\nMethod section\n\nCorpus descriptions (one paragraph)\nIndex descriptions (one paragraph)\nAnalysis (one paragraph)\nResearch hypothesis (one paragraph)\n\nResults (data interpretation and commentary)\n\nFigures or statistical report\n\nConclusion\n\n\n\n\nAssignment Guideline\n\n\nStep 1: Construct research questions\nIn this type of research, researchers typically set RQs about the relationships between lexical characteristncs and variables that defines subsection of the corpus (e.g., grade, genre, or proficieincy score).\nThe following information is available through the GiG corpus:\nThe following information is available through the ICNALE corpus:\n\nRatings performed by external raters\n\n\n\nStep 2: Understand and choose the corpus\nIn this assignment, please choose one of the following corpora:\n\nGrowth in Grammar (GiG) corpus (Durrant, 2023)\nICNALE corpus (Edited Essay OR GRA)\nSome Japanese corpus here (Ask Masaki about availability).\n\n\nStep 3: Construct hypothesis\nBased on what you‚Äôve learned about the vocabulary use of learner, state several hypotheses that you expect as the findings for the research question.\nIn other words, what do you expect as the relationship between lexical characteristics X and external variable Y?\n\n\nStep 4: Select index\nBased on the RQs and hypotheses, you will select indices that can capture the lexical characteristics X in your corpus.\n\n\nStep 5: Compute the index\nYou will now use the tools we have covered in this course to derive lexical richness scores for the text.\n\n\nStep 6: Conduct analysis\nTo answer the research questions, you may want to do the followings: - Obtain descriptive statistics of the lexical richness indices - Visualize the relationship between variables - Optionally run statistical analyses\n\n\nStep 7: Interpret and write-up the results\nYou will write-up what you found in your mini research in a one-to-two page short report.\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission ‚Ä¶\n\noutlines research questions and hypotheses\nprovide description of lexical richness measures that you used and how you calculated the measures\nprovides analysis results and their interpretations in relation to the research questions",
    "crumbs": [
      "Assignments",
      "Corpus Lab 3"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html",
    "href": "2025/assignments/hands-on-4/index.html",
    "title": "Corpus Lab 4",
    "section": "",
    "text": "This assignment aims to help you practice the following skills:\n\nextracting fine-grained grammatical features from either a Japanese or an English corpus.\nwriting a short report describing the results and interpretation of the analysis results.",
    "crumbs": [
      "Assignments",
      "Corpus Lab 4"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html#task-1-research-questions-hypotheses-and-methods",
    "href": "2025/assignments/hands-on-4/index.html#task-1-research-questions-hypotheses-and-methods",
    "title": "Corpus Lab 4",
    "section": "Task 1: Research questions, Hypotheses and Methods",
    "text": "Task 1: Research questions, Hypotheses and Methods\nIn this task you will describe research questions, hypothesis, and methods.\n\nResearch questions\n\nResearch questions should include:\n\ntype of features you are looking at (e.g., adverbial clauses)\nsituational variables that defines your sub-corpora (e.g., grade, genre, proficiency)\n\n\n\n\nHypothesis\n\nYour research hypothesis should:\n\ndescribe your predictions in terms of:\n\nquantitative trends of the feature in relation to the factor you are interested in.\n\n\n\n\n\nDefinitions and operationalization of grammatical features to extract\n\nYou must describe the specific grammatical features that you plan to extract.\nFor example, for clausal features you need to specify if you are interested in :\n\nsubordinate clauses or embedded clauses\nparticular type of clauses\n\nDescription of rules to identify desirable linguistic feature.\n\nFor example, you will need to specify amod for dependency label to extract adjective + noun phrase.",
    "crumbs": [
      "Assignments",
      "Corpus Lab 4"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html#task-2-fine-grained-descriptive-grammatical-features",
    "href": "2025/assignments/hands-on-4/index.html#task-2-fine-grained-descriptive-grammatical-features",
    "title": "Corpus Lab 4",
    "section": "Task 2: Fine-grained Descriptive grammatical features",
    "text": "Task 2: Fine-grained Descriptive grammatical features\n\nOnce you articulated the information above, you will now conduct a search over the corpus.\nYou should use either simple text analyzer or your own Colab Notebook.\n\nI will specify which option should be used by the time we start working on this assignment (that is, this depends on your progress as a group.)",
    "crumbs": [
      "Assignments",
      "Corpus Lab 4"
    ]
  },
  {
    "objectID": "2025/assignments/hands-on-4/index.html#task-3-results-and-interpretation",
    "href": "2025/assignments/hands-on-4/index.html#task-3-results-and-interpretation",
    "title": "Corpus Lab 4",
    "section": "Task 3: Results and interpretation",
    "text": "Task 3: Results and interpretation\n\nProvide the results of your corpus analysis in a way you think is most effective to address your research questions. Make effective use of tables, plots, or other data presentation technique as you think.\nProvide descriptive paragraphs to walk the reader through the results and how to interprete that results.\n\n\n\n\n\n\n\nSubmission\n\n\n\n\nA word file (.docx file) that addresses requirements in a written format (one or two pages depending on your analysis results.).\n\nScreenshots of your search settings on the simple text analyzer tool.\n\nIF you use colab, Google Colab notebook (.ipynb file) with extraction code and results.\n\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\n\nYour submission ‚Ä¶\n\noutlines research questions and hypotheses\nprovide description of your approach (algorithms and rules) to identify the desired linguistic structure.\nprovides analysis results and their interpretations in relation to the research questions",
    "crumbs": [
      "Assignments",
      "Corpus Lab 4"
    ]
  },
  {
    "objectID": "2025/slides/session-9.html#learning-objectives",
    "href": "2025/slides/session-9.html#learning-objectives",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nAs a wrap-up of the vocabulary and multiword sessions, you will be able to:\n\n\n\nConduct mini learner corpus research.\n\nConstruct several learner corpus research questions with structured support.\nChoose a learner corpus that suit your research needs\nJustify choices of lexical richness measures to investigate a research questions\nCompute lexical richness indices and analyze the results to answer research questions\nPresent the results and interpretation of the findings in a written prose"
  },
  {
    "objectID": "2025/slides/session-9.html#mini-research-outline",
    "href": "2025/slides/session-9.html#mini-research-outline",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Mini-research outline",
    "text": "Mini-research outline\n\nConstruct research questions\nUnderstand and choose the learner corpus\nConstruct hypothesis\nSelect index\nCompute the index\nConduct analysis\nInterpret and write-up the results"
  },
  {
    "objectID": "2025/slides/session-9.html#step-1-construct-research-questions",
    "href": "2025/slides/session-9.html#step-1-construct-research-questions",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Step 1: Construct research questions",
    "text": "Step 1: Construct research questions\nResearchers typically set RQs about the relationships between lexical characteristncs and variables that defines subsection of the corpus (e.g., grade, genre, or proficieincy score)."
  },
  {
    "objectID": "2025/slides/session-9.html#step-2-understand-and-choose-the-corpus",
    "href": "2025/slides/session-9.html#step-2-understand-and-choose-the-corpus",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Step 2: Understand and choose the corpus",
    "text": "Step 2: Understand and choose the corpus\nIn this assignment, please choose one of the following corpora:\n\nGrowth in Grammar (GiG) corpus (Durrant, 2023)\nICNALE corpus (Edited Essay OR GRA)\nIf you have a corpus you want to analyze, let Masaki know!"
  },
  {
    "objectID": "2025/slides/session-9.html#understanding-the-corpus-contents",
    "href": "2025/slides/session-9.html#understanding-the-corpus-contents",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Understanding the corpus contents",
    "text": "Understanding the corpus contents\n\nYou can look at followings:\n\nmeta-data (the additional information about the corpus files)\nseveral text files"
  },
  {
    "objectID": "2025/slides/session-9.html#gig-corpus",
    "href": "2025/slides/session-9.html#gig-corpus",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "GiG corpus",
    "text": "GiG corpus\n\nFor detailed information, see Table 2.1 in Durrant (2023)\n\n\nGiG metadata"
  },
  {
    "objectID": "2025/slides/session-9.html#icnale-gra",
    "href": "2025/slides/session-9.html#icnale-gra",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "ICNALE GRA",
    "text": "ICNALE GRA\nICNALE GRA documents 120 essays evaluated by 80 people. - These 80 people are from different backgrounds. - You can basically use average essay ratings.\n\nICNALE metadata"
  },
  {
    "objectID": "2025/slides/session-9.html#step-1-2-formluating-research-questions",
    "href": "2025/slides/session-9.html#step-1-2-formluating-research-questions",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Step 1 & 2: Formluating research questions",
    "text": "Step 1 & 2: Formluating research questions\n\nWhat would you like to know?\n\nHow learners develop their language ability across time?\nWhat defines ‚Äúmore proficient‚Äù language use?\nHow situational variables of writing/speaking impact the language production?\nHow does production of X change across time/proficiency?\nAnything else?\nDo not forget to specify constructs!"
  },
  {
    "objectID": "2025/slides/session-9.html#step-2-choosing-a-right-corpus",
    "href": "2025/slides/session-9.html#step-2-choosing-a-right-corpus",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Step 2: Choosing a right corpus",
    "text": "Step 2: Choosing a right corpus\nBased on the research question, which corpus should you choose?\n\nGrowth in Grammar (GiG) corpus (Durrant, 2023)\nICNALE corpus (Edited Essay OR GRA)\n\n\nUnderstand what it offers and doesn‚Äôt.\nPick one corpus to move forward."
  },
  {
    "objectID": "2025/slides/session-9.html#step-3-construct-hypothesis",
    "href": "2025/slides/session-9.html#step-3-construct-hypothesis",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Step 3: Construct hypothesis",
    "text": "Step 3: Construct hypothesis\n\nOnce you‚Äôve articulated broad research questions and chose a corpus, speculate on possible results.\nUse what you‚Äôve learned so far about lexical richness.\n\nLexical diversity\nLexical sophistication\nPhraseological complexity\n\nWhich ones are relevant to your questions?"
  },
  {
    "objectID": "2025/slides/session-9.html#step-4-select-index",
    "href": "2025/slides/session-9.html#step-4-select-index",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Step 4: Select index",
    "text": "Step 4: Select index\nLet‚Äôs keep going!\nSpecifically, which index may capture the change in the vocabulary use in your context?\n\nList posssible indices to capture the differences in vocabulary use.\nWhy do you think those are good choices?"
  },
  {
    "objectID": "2025/slides/session-9.html#step-5-compute-the-index",
    "href": "2025/slides/session-9.html#step-5-compute-the-index",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Step 5: Compute the index",
    "text": "Step 5: Compute the index\nOkay now it‚Äôs time to do some analysis!\n\nWhich ones are you going to use:\n\nTAALED\nTAALES\nLexical profiler\nCustom list in Simple Text Analyzer?"
  },
  {
    "objectID": "2025/slides/session-9.html#step-6-conduct-analysis",
    "href": "2025/slides/session-9.html#step-6-conduct-analysis",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Step 6: Conduct analysis",
    "text": "Step 6: Conduct analysis\nOnce you obtained the indices, it‚Äôs time to understand the pattern by plotting.\n\nUse simple analyzer to plot data from:\n\nGiG corpus\nICNALE\n\nThe simple text analyzer is made for that purpose."
  },
  {
    "objectID": "2025/slides/session-9.html#step-7-interpret-and-write-up-the-results",
    "href": "2025/slides/session-9.html#step-7-interpret-and-write-up-the-results",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Step 7: Interpret and write-up the results",
    "text": "Step 7: Interpret and write-up the results\nNow you understand what is happening in your corpus, you can write that up."
  },
  {
    "objectID": "2025/slides/session-9.html#mini-research-outline-1",
    "href": "2025/slides/session-9.html#mini-research-outline-1",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Mini-research outline",
    "text": "Mini-research outline\n\nConstruct research questions\nUnderstand and choose the learner corpus\nConstruct hypothesis\nSelect index\nCompute the index\nConduct analysis\nInterpret and write-up the results"
  },
  {
    "objectID": "2025/slides/session-9.html#reflection",
    "href": "2025/slides/session-9.html#reflection",
    "title": "Session 9: Learner Corpus Mini-Research",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now:\n\nProvide reasons for your choice of lexical richness measures.\nConduct preliminary analysis to understand how text differ from one aother in relation to learner‚Äôs proficiency, learner groups, etc."
  },
  {
    "objectID": "2025/slides/session-10.html#learning-objectives",
    "href": "2025/slides/session-10.html#learning-objectives",
    "title": "Session 10: Analyzing Grammar",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nProvide historical overview of the syntactic complexity research\nDescribe different approaches to grammatical features:\n\nGrammatical complexity strand\nFine-grained grammatical complexity strand\nDescriptive (register-based analysis) strand\nVerb Argument Construction (VAC) strand\n\nUnderstand current trends of syntactic complexity research"
  },
  {
    "objectID": "2025/slides/session-10.html#grammatical-complexity",
    "href": "2025/slides/session-10.html#grammatical-complexity",
    "title": "Session 10: Analyzing Grammar",
    "section": "Grammatical complexity",
    "text": "Grammatical complexity\n\nIndicators of\n\nproficiency\ndevelopment"
  },
  {
    "objectID": "2025/slides/session-10.html#grammatical-complexity-strand",
    "href": "2025/slides/session-10.html#grammatical-complexity-strand",
    "title": "Session 10: Analyzing Grammar",
    "section": "Grammatical complexity strand",
    "text": "Grammatical complexity strand\n\nLength-based indices\n\nNumber of words per T-unit\n\nSubordination"
  },
  {
    "objectID": "2025/slides/session-10.html#t-unit",
    "href": "2025/slides/session-10.html#t-unit",
    "title": "Session 10: Analyzing Grammar",
    "section": "T-unit",
    "text": "T-unit\n\nWhen it comes to measuring grammatical complexity, one important consideration is how to define linguistic unit.\nWriting researchers use T-Unit (Hunt, 1966)\n\nT-unit: ‚ÄúIndependent clause plus any subordinate clauses attached to it‚Äù"
  },
  {
    "objectID": "2025/slides/session-10.html#typical-grammatical-complexity-measures",
    "href": "2025/slides/session-10.html#typical-grammatical-complexity-measures",
    "title": "Session 10: Analyzing Grammar",
    "section": "Typical grammatical complexity measures",
    "text": "Typical grammatical complexity measures\n\ncomplexity measures reported in Kyle & Crossley (2018)"
  },
  {
    "objectID": "2025/slides/session-10.html#findings",
    "href": "2025/slides/session-10.html#findings",
    "title": "Session 10: Analyzing Grammar",
    "section": "Findings",
    "text": "Findings"
  },
  {
    "objectID": "2025/slides/session-10.html#organic-approach-norris-ortega-2009",
    "href": "2025/slides/session-10.html#organic-approach-norris-ortega-2009",
    "title": "Session 10: Analyzing Grammar",
    "section": "Organic approach (Norris & Ortega, 2009)",
    "text": "Organic approach (Norris & Ortega, 2009)"
  },
  {
    "objectID": "2025/slides/session-10.html#fine-grained-grammatical-complexity-strand",
    "href": "2025/slides/session-10.html#fine-grained-grammatical-complexity-strand",
    "title": "Session 10: Analyzing Grammar",
    "section": "Fine-grained grammatical complexity strand",
    "text": "Fine-grained grammatical complexity strand\nCriticism on the (largely) length-based grammatical complexity:\n\nDoes not tell us about how sentence structures are elaborated"
  },
  {
    "objectID": "2025/slides/session-10.html#length-based-indices-does-not-tell-elaboration-strategy",
    "href": "2025/slides/session-10.html#length-based-indices-does-not-tell-elaboration-strategy",
    "title": "Session 10: Analyzing Grammar",
    "section": "Length-based indices does not tell elaboration strategy",
    "text": "Length-based indices does not tell elaboration strategy\n\n\nThe athletic man in the jersey kicked the ball over the fence.\n\n\nElaborated by phrases (adjectival modification; prepositional phrases)\n\n\nBecause he wanted to score a goal, the man kicked the ball.\n\n\nElaborated by subordinate clause.\n\n\nExample from Kyle & Crossley (2018)"
  },
  {
    "objectID": "2025/slides/session-10.html#kyle-crossley-2018",
    "href": "2025/slides/session-10.html#kyle-crossley-2018",
    "title": "Session 10: Analyzing Grammar",
    "section": "Kyle & Crossley (2018)",
    "text": "Kyle & Crossley (2018)\n\nKyle & Crossley (2018) proposed fine-grained clausal & phrasal complexity indices\nThey used dependency parsing to identify fine-grained features of grammar."
  },
  {
    "objectID": "2025/slides/session-10.html#clausal-indices",
    "href": "2025/slides/session-10.html#clausal-indices",
    "title": "Session 10: Analyzing Grammar",
    "section": "Clausal indices",
    "text": "Clausal indices\nThe followings are example:\n\n\n\n\nStructure\nDependency tag\nExample of Structure\n\n\n\n\nNominal subject\nnsubj\nThe athlete ran quickly.\n\n\nDirect object\ndobj\nHe plays soccer.\n\n\nIndirect object\niobj\nHe teaches me soccer.\n\n\nClausal complement\nccomp\nI am certain that he did it.\n\n\nAdjectival complement\nacomp\nHe looks fine.\n\n\nNominal complement\nncomp\nShe is a teacher."
  },
  {
    "objectID": "2025/slides/session-10.html#clausal-indices-oblique",
    "href": "2025/slides/session-10.html#clausal-indices-oblique",
    "title": "Session 10: Analyzing Grammar",
    "section": "Clausal indices (Oblique)",
    "text": "Clausal indices (Oblique)\nThe followings are example:\n\n\n\n\nStructure\nAbbreviation\nExample of Structure\n\n\n\n\nAdverbial modifier\nadvmod\nAccordingly, I ate pizza.\n\n\nPrepositional modifier\nprep\nThey went into the score.\n\n\nTemporal modifier\ntmod\nLast night, we had fun.\n\n\nAdverbial clause\nadvcl\nThe accident happened as night fell.\n\n\nOpen clausal complement\nxcomp\nI am ready to leave."
  },
  {
    "objectID": "2025/slides/session-10.html#phrasal-indices",
    "href": "2025/slides/session-10.html#phrasal-indices",
    "title": "Session 10: Analyzing Grammar",
    "section": "Phrasal indices",
    "text": "Phrasal indices\n\nPhrasal indices counts how many dependents there are for each of the following structure: nsubj, nsubj_pass, agent, ncomp, dobj, iobj, pobj.\n\n\n\n\n\nStructure\nAbbreviation\nExample of Structure\n\n\n\n\nDeterminers\ndet\nThe man went into the store.\n\n\nPrepositional phrases\nprep\nthe man in the red hat.\n\n\nAdjective modifier\namod\nThe man in the red hat\n\n\nPossessives\nposs\nTom‚Äôs store; his intention\n\n\nRelative clause modifiers\nrecmod\nthe plan I thought\n\n\nAdverbial modifiers\nadvmod\nIt‚Äôs a really good idea."
  },
  {
    "objectID": "2025/slides/session-10.html#kyle-crossley-2018-1",
    "href": "2025/slides/session-10.html#kyle-crossley-2018-1",
    "title": "Session 10: Analyzing Grammar",
    "section": "Kyle & Crossley (2018)",
    "text": "Kyle & Crossley (2018)\n\nThey counted fine-grained clausal and phrasal indices.\n# dependents per clause/phrase\nThey examined correlations between TOEFL score and the fine-grained indices"
  },
  {
    "objectID": "2025/slides/session-10.html#results",
    "href": "2025/slides/session-10.html#results",
    "title": "Session 10: Analyzing Grammar",
    "section": "Results",
    "text": "Results\n\nFinal regression model"
  },
  {
    "objectID": "2025/slides/session-10.html#example-sentences",
    "href": "2025/slides/session-10.html#example-sentences",
    "title": "Session 10: Analyzing Grammar",
    "section": "Example sentences",
    "text": "Example sentences\n\n\n\nExample from score 1 essay\n\n\n\n\n\n\n\nExample from score 5 essay\n\n\n\n\n\n\nExample from score 5 essay"
  },
  {
    "objectID": "2025/slides/session-10.html#kyle-crossley-2018-2",
    "href": "2025/slides/session-10.html#kyle-crossley-2018-2",
    "title": "Session 10: Analyzing Grammar",
    "section": "Kyle & Crossley (2018)",
    "text": "Kyle & Crossley (2018)\n\nThe fine-grained indices:\n\nexplained more variances (~ 20%) than traditional complexity measures (~ 5%)\nprovides more insights into what structure the learners tend to use\n\nIn hands-on activity, we will talk more about how to identify fine-grained grammatical features."
  },
  {
    "objectID": "2025/slides/session-10.html#syntactic-sophistication-or-vac-strand",
    "href": "2025/slides/session-10.html#syntactic-sophistication-or-vac-strand",
    "title": "Session 10: Analyzing Grammar",
    "section": "Syntactic sophistication (or VAC) strand",
    "text": "Syntactic sophistication (or VAC) strand\nAccording to construction grammar (Goldberg, 1995, 2006), grammatical construction (structure) convey abstract linguistic meaning.\nIn turn,\n\n\n\nConstruction\nStructure\nExample\n\n\n\n\n\n\nIntransitive:\nTransitive:"
  },
  {
    "objectID": "2025/slides/session-10.html#descriptive-register-based-feature-strand",
    "href": "2025/slides/session-10.html#descriptive-register-based-feature-strand",
    "title": "Session 10: Analyzing Grammar",
    "section": "Descriptive (register-based feature) strand",
    "text": "Descriptive (register-based feature) strand"
  },
  {
    "objectID": "2025/slides/session-13.html#learning-objectives",
    "href": "2025/slides/session-13.html#learning-objectives",
    "title": "Session 12: Hands-on Activity",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nDescribe how LLMs are trained generally and what LLMs do to produce language.\nExplain the benefits and drawbacks of using LLMs for linguistic annotation.\nDemonstrate/discuss potential impacts of prompts on the LLMs performance on linguistic annotation.\nDesign an experiment to investigate LLMs output accuracy on a given annotation task."
  },
  {
    "objectID": "2025/slides/session-3.html#goals-and-overview",
    "href": "2025/slides/session-3.html#goals-and-overview",
    "title": "Session 3: Hands-on #1",
    "section": "Goals and Overview",
    "text": "Goals and Overview\nThis assignment aims to help you practice the following skills:\n\nArticulating linguistic research questions for corpus-driven research\nExplaining corpus choices and analysis approach for basic corpus search\nInterpreting and describing the basic concordance search results"
  },
  {
    "objectID": "2025/slides/session-3.html#goals-and-overview-1",
    "href": "2025/slides/session-3.html#goals-and-overview-1",
    "title": "Session 3: Hands-on #1",
    "section": "Goals and overview",
    "text": "Goals and overview\n\nTask 1: Constructing corpus-search research question and hypothesis\nTask 2: Methods\nTask 3: Result"
  },
  {
    "objectID": "2025/slides/session-3.html#task-1-research-question-and-hypothesis",
    "href": "2025/slides/session-3.html#task-1-research-question-and-hypothesis",
    "title": "Session 3: Hands-on #1",
    "section": "Task 1: Research question and hypothesis",
    "text": "Task 1: Research question and hypothesis\nIn this task, you are asked to articulate your research question and hypothesis for your first corpus assignment.\nResearch question:\n\nResearch questions should be answerable.\nWrite one (or two related) research questions you would like to answer through this assignment."
  },
  {
    "objectID": "2025/slides/session-3.html#task-1-research-question-and-hypothesis-1",
    "href": "2025/slides/session-3.html#task-1-research-question-and-hypothesis-1",
    "title": "Session 3: Hands-on #1",
    "section": "Task 1: Research question and hypothesis",
    "text": "Task 1: Research question and hypothesis\nHypothesis:\n\nOnce you decided on research question, state your hypothesis.\nWrite a short paragraph stating your hypothesis and why you think your research hypotheses may be true."
  },
  {
    "objectID": "2025/slides/session-3.html#task-2-methods",
    "href": "2025/slides/session-3.html#task-2-methods",
    "title": "Session 3: Hands-on #1",
    "section": "Task 2: Methods",
    "text": "Task 2: Methods\nIn this task, you are asked to describe the methods choice of your corpus search. This should include justifications for the corpora used, type of corpus search used.\nCorpora\n\nWhich corpora or sub-section of a corpus would you conduct a search on? Why? Justify your choice in a paragraph."
  },
  {
    "objectID": "2025/slides/session-3.html#task-2-methods-contd",
    "href": "2025/slides/session-3.html#task-2-methods-contd",
    "title": "Session 3: Hands-on #1",
    "section": "Task 2: Methods (cont‚Äôd)",
    "text": "Task 2: Methods (cont‚Äôd)\nSearch Methods and Plan\n\nWhat corpus search methods would you choose and why? In this assignment, search methods mainly include methods available through English-Corpora.org, including LIST, Chart, Collocates, Compare and KWIC.\nIn what way are you planning to conduct the search and what kind of information are you expecting from it?"
  },
  {
    "objectID": "2025/slides/session-3.html#task-3-result",
    "href": "2025/slides/session-3.html#task-3-result",
    "title": "Session 3: Hands-on #1",
    "section": "Task 3: Result",
    "text": "Task 3: Result\nIn this part of the assignment, you are asked to describe the search results and provide interpretations on the findings.\n\nCorpus search results: Provide some numbers or discourse samples based on your corpus search. This can be frequency list, table with frequency counts, or a copy of KWIC results.\nInterpretation: Write a paragraph, providing some interpretations of your findings."
  },
  {
    "objectID": "2025/slides/session-3.html#submission",
    "href": "2025/slides/session-3.html#submission",
    "title": "Session 3: Hands-on #1",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nSuccess Criteria\n\n\nYour submission ‚Ä¶\n\nincludes one or two research questions\narticulates hypotheses\ninclude a list of corpora used\njustifies the selection of corpus\noutlines the search plan (key phrase, regular expression, sorting, filtering, etc.)\nincludes results of your search\nprovides interpretation of the findings."
  },
  {
    "objectID": "2025/slides/session-3.html#recap-the-scientific-research-cycle",
    "href": "2025/slides/session-3.html#recap-the-scientific-research-cycle",
    "title": "Session 3: Hands-on #1",
    "section": "Recap: The Scientific Research Cycle",
    "text": "Recap: The Scientific Research Cycle\nRemember from Session 2:\n\nVerbalize your question\nFormulate research questions\n\nFormulate hypothesis\nChoose appropriate corpus\n‚Üí Operationalize\n‚Üí Test & Evaluate"
  },
  {
    "objectID": "2025/slides/session-3.html#what-does-operationalize-mean",
    "href": "2025/slides/session-3.html#what-does-operationalize-mean",
    "title": "Session 3: Hands-on #1",
    "section": "What does ‚Äúoperationalize‚Äù mean?",
    "text": "What does ‚Äúoperationalize‚Äù mean?\n\n\n\n\n\n\nOperationalization\n\n\n‚Äúan explicit and unambiguous description of a set of operations that are performed to identify and measure that construct.‚Äù (Stefanowitsch, 2020, p.¬†77)\n\n\n\n\nDefining exactly WHAT to search\nDeciding HOW to search it\nBeing explicit about your choices\n\n\n\n\n\n\n\nMeasure of height\n\n\n\nHow to operationalize ‚Äúheight‚Äù?\n\nMeter? Feet?"
  },
  {
    "objectID": "2025/slides/session-3.html#first-corpus-search",
    "href": "2025/slides/session-3.html#first-corpus-search",
    "title": "Session 3: Hands-on #1",
    "section": "First corpus search",
    "text": "First corpus search\n\nLet‚Äôs operationalize a foundational concept in linguistic research, word.\nConcept to operationalize: The word ‚Äúrun‚Äù\n\nWhat is possible operationalization?\nThat is, what search term would you like to enter below?\n\nSearch field"
  },
  {
    "objectID": "2025/slides/session-3.html#result-of-the-search-with-run",
    "href": "2025/slides/session-3.html#result-of-the-search-with-run",
    "title": "Session 3: Hands-on #1",
    "section": "Result of the search with run",
    "text": "Result of the search with run\n\nYes, you now get the frequency result (= 264014).\nYou might get slightly different results.\n\n\nSearch - run"
  },
  {
    "objectID": "2025/slides/session-3.html#lets-look-at-the-context",
    "href": "2025/slides/session-3.html#lets-look-at-the-context",
    "title": "Session 3: Hands-on #1",
    "section": "Let‚Äôs look at the context",
    "text": "Let‚Äôs look at the context\n\nWait a minute‚Ä¶ where is runs, ran, and running?"
  },
  {
    "objectID": "2025/slides/session-3.html#thinking-about-the-operationalization",
    "href": "2025/slides/session-3.html#thinking-about-the-operationalization",
    "title": "Session 3: Hands-on #1",
    "section": "Thinking about the operationalization",
    "text": "Thinking about the operationalization\n\nNow we recognized that the word run was just one of the possible forms of the headword run.\nIf we were thinking to retrieve all the occurence of the word ‚Äúrun‚Äù then we were imprecise.\nWhat should we do?"
  },
  {
    "objectID": "2025/slides/session-3.html#lemma",
    "href": "2025/slides/session-3.html#lemma",
    "title": "Session 3: Hands-on #1",
    "section": "Lemma",
    "text": "Lemma\n\nLemma: A headword and all the inflected form derived from it.\nRUN:\n\nrun, ran, runs, running‚Ä¶ etc."
  },
  {
    "objectID": "2025/slides/session-3.html#refining-our-operationalization",
    "href": "2025/slides/session-3.html#refining-our-operationalization",
    "title": "Session 3: Hands-on #1",
    "section": "Refining our operationalization",
    "text": "Refining our operationalization\n\nConcept: All forms of ‚Äúrun‚Äù\nBetter operationalization: Search for LEMMA"
  },
  {
    "objectID": "2025/slides/session-3.html#lemma-search",
    "href": "2025/slides/session-3.html#lemma-search",
    "title": "Session 3: Hands-on #1",
    "section": "LEMMA search",
    "text": "LEMMA search\n\nYou can search LEMMA in English-Corpora.org through Capital letters.\nThe search methods will depend on the corpus tool you use."
  },
  {
    "objectID": "2025/slides/session-3.html#lemma-search-1",
    "href": "2025/slides/session-3.html#lemma-search-1",
    "title": "Session 3: Hands-on #1",
    "section": "LEMMA search",
    "text": "LEMMA search\n\nSearching lemma - run"
  },
  {
    "objectID": "2025/slides/session-3.html#lemma-search-result---run",
    "href": "2025/slides/session-3.html#lemma-search-result---run",
    "title": "Session 3: Hands-on #1",
    "section": "LEMMA search result - run",
    "text": "LEMMA search result - run\n\nSearch result"
  },
  {
    "objectID": "2025/slides/session-3.html#task-a.2---pos-search",
    "href": "2025/slides/session-3.html#task-a.2---pos-search",
    "title": "Session 3: Hands-on #1",
    "section": "Task A.2 - POS search",
    "text": "Task A.2 - POS search\nNow you might wonder:\n\nResearch question: ‚ÄúHow often is ‚Äòrun‚Äô used as a noun vs.¬†verb?‚Äù\n\n‚Üí This is even more precise operationalization of word by its grammatical category."
  },
  {
    "objectID": "2025/slides/session-3.html#specifying-part-of-speech-pos",
    "href": "2025/slides/session-3.html#specifying-part-of-speech-pos",
    "title": "Session 3: Hands-on #1",
    "section": "Specifying Part Of Speech (POS)",
    "text": "Specifying Part Of Speech (POS)\nYou can use POS tag like the following .\n\n\n\n\nCategory\nSimple tag\nSymbol\nExample\n\n\n\n\nCommon noun\nNOUN\nN\nrun_N\n\n\nProper Nouns\nNAME\nNP\nSendai_N\n\n\nAll nouns\nNOUN+\nN+\nsun_N+, Sendai_N+\n\n\nLexical verbs\nVERB\nV\nrun_V\n\n\nAll verbs\nVERB+\nV\nrun_V+, do_V+"
  },
  {
    "objectID": "2025/slides/session-3.html#specifying-part-of-speech-pos-1",
    "href": "2025/slides/session-3.html#specifying-part-of-speech-pos-1",
    "title": "Session 3: Hands-on #1",
    "section": "Specifying Part Of Speech (POS)",
    "text": "Specifying Part Of Speech (POS)\n\n\n\n\nCategory\nSimple tag\nSymbol\nExample\n\n\n\n\nAdjectives\nADJ\nJ\nsimple_J\n\n\nAdverbs\nADV\nR\nclear_R\n\n\n\n\nSee this page for more information of POS in English-Corpora.org"
  },
  {
    "objectID": "2025/slides/session-3.html#the-word-run-used-as-noun",
    "href": "2025/slides/session-3.html#the-word-run-used-as-noun",
    "title": "Session 3: Hands-on #1",
    "section": "the word run used as noun",
    "text": "the word run used as noun\n\nSearch occurrences of run that are used as nouns."
  },
  {
    "objectID": "2025/slides/session-3.html#the-word-run-used-as-noun-1",
    "href": "2025/slides/session-3.html#the-word-run-used-as-noun-1",
    "title": "Session 3: Hands-on #1",
    "section": "the word run used as noun",
    "text": "the word run used as noun\nSearch: run_N\n\nrun - as noun"
  },
  {
    "objectID": "2025/slides/session-3.html#your-turn-operationalize-these-concepts",
    "href": "2025/slides/session-3.html#your-turn-operationalize-these-concepts",
    "title": "Session 3: Hands-on #1",
    "section": "Your turn: Operationalize these concepts",
    "text": "Your turn: Operationalize these concepts\n\nThe word ‚Äúchair‚Äù as a verb only"
  },
  {
    "objectID": "2025/slides/session-3.html#concordances-or-kwic",
    "href": "2025/slides/session-3.html#concordances-or-kwic",
    "title": "Session 3: Hands-on #1",
    "section": "Concordances (or KWIC)",
    "text": "Concordances (or KWIC)\n\nNow we know frequency of X in COCA, but what about context?\n\nKWIC = Key Word In Context\nKWIC will give us insights into how each word is used in context."
  },
  {
    "objectID": "2025/slides/session-3.html#kwic-view",
    "href": "2025/slides/session-3.html#kwic-view",
    "title": "Session 3: Hands-on #1",
    "section": "KWIC view",
    "text": "KWIC view\n\nGo back to SEARCH window.\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#kwic-view-1",
    "href": "2025/slides/session-3.html#kwic-view-1",
    "title": "Session 3: Hands-on #1",
    "section": "KWIC view",
    "text": "KWIC view\n\nClick on the + button in the search menu and enter your search word.\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#kwic-view-2",
    "href": "2025/slides/session-3.html#kwic-view-2",
    "title": "Session 3: Hands-on #1",
    "section": "KWIC view",
    "text": "KWIC view\n\nThe result is displayed. Default sort : R1 &gt; R2 &gt; R3\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#sorting-kwic-window",
    "href": "2025/slides/session-3.html#sorting-kwic-window",
    "title": "Session 3: Hands-on #1",
    "section": "Sorting KWIC window",
    "text": "Sorting KWIC window\n\nNow let‚Äôs sort the results according to the position in context.\n\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#sorting-words",
    "href": "2025/slides/session-3.html#sorting-words",
    "title": "Session 3: Hands-on #1",
    "section": "Sorting words",
    "text": "Sorting words\nYou can sort the words.\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#sorting-to-find-patterns",
    "href": "2025/slides/session-3.html#sorting-to-find-patterns",
    "title": "Session 3: Hands-on #1",
    "section": "Sorting to find patterns",
    "text": "Sorting to find patterns\nSorting helps identify: - Common phrases - Grammatical patterns\n- Semantic preferences"
  },
  {
    "objectID": "2025/slides/session-3.html#lets-try-kwic",
    "href": "2025/slides/session-3.html#lets-try-kwic",
    "title": "Session 3: Hands-on #1",
    "section": "Let‚Äôs Try: KWIC",
    "text": "Let‚Äôs Try: KWIC\n\nChoose a word that you want to see the context for.\nSearch the word with KWIC.\nSort the word in the following way.\n\nDefault: R1 &gt; R2 &gt; R3\nCustom 1: L1 &gt; L2 &gt; L3\nCustom 2: L1 &gt; R1 &gt; R2"
  },
  {
    "objectID": "2025/slides/session-3.html#how-to-get-custom-1-and-2",
    "href": "2025/slides/session-3.html#how-to-get-custom-1-and-2",
    "title": "Session 3: Hands-on #1",
    "section": "How to get Custom 1 and 2",
    "text": "How to get Custom 1 and 2\n\nKWIC search"
  },
  {
    "objectID": "2025/slides/session-3.html#practice-finding-patterns",
    "href": "2025/slides/session-3.html#practice-finding-patterns",
    "title": "Session 3: Hands-on #1",
    "section": "Practice: Finding patterns",
    "text": "Practice: Finding patterns\nSearch ‚Äúprove‚Äù (lemma) and sort by: 1. R1 (what follows ‚Äúprove‚Äù?) 2. L1 (what precedes ‚Äúprove‚Äù?)\nWhat patterns do you notice?"
  },
  {
    "objectID": "2025/slides/session-3.html#chart-function",
    "href": "2025/slides/session-3.html#chart-function",
    "title": "Session 3: Hands-on #1",
    "section": "Chart function",
    "text": "Chart function\nCHART shows frequency across:\n\nGenres\nTime periods\nText types\nThis allows you to return frequency by sections of of COCA (= conditional frequency)."
  },
  {
    "objectID": "2025/slides/session-3.html#search-lol-using-chart",
    "href": "2025/slides/session-3.html#search-lol-using-chart",
    "title": "Session 3: Hands-on #1",
    "section": "Search lol using CHART",
    "text": "Search lol using CHART\n\nGo to Search Tab, select, CHART and enter lol"
  },
  {
    "objectID": "2025/slides/session-3.html#search-lol-using-chart-1",
    "href": "2025/slides/session-3.html#search-lol-using-chart-1",
    "title": "Session 3: Hands-on #1",
    "section": "Search lol using CHART",
    "text": "Search lol using CHART\n\nYou will get the following:\nWhat do you notice here?\n\n\nSearch - lol"
  },
  {
    "objectID": "2025/slides/session-3.html#interpreting-the-distribution",
    "href": "2025/slides/session-3.html#interpreting-the-distribution",
    "title": "Session 3: Hands-on #1",
    "section": "Interpreting the distribution",
    "text": "Interpreting the distribution\nWhat does this tell us about ‚Äúlol‚Äù?\n\nGenre preferences?\nFormality levels?\nChange over time?\nDescribe the frequency pattern looking at PER MIL row."
  },
  {
    "objectID": "2025/slides/session-3.html#summary",
    "href": "2025/slides/session-3.html#summary",
    "title": "Session 3: Hands-on #1",
    "section": "Summary",
    "text": "Summary\nCHART function can be used to get frequency across:\n\nGenres\nTime periods\nText types"
  },
  {
    "objectID": "2025/slides/session-3.html#regular-expressions",
    "href": "2025/slides/session-3.html#regular-expressions",
    "title": "Session 3: Hands-on #1",
    "section": "Regular expressions",
    "text": "Regular expressions\nRegular expression (Ê≠£Ë¶èË°®Áèæ) allows you to search corpus through ‚Äúpattern matching‚Äù.\n\nHave you ever used (*) asterisk in your web search?\nThis is one of the regular expression (= wild card)"
  },
  {
    "objectID": "2025/slides/session-3.html#using-regular-expression-to-get-words-with-specific-morpheme",
    "href": "2025/slides/session-3.html#using-regular-expression-to-get-words-with-specific-morpheme",
    "title": "Session 3: Hands-on #1",
    "section": "Using regular expression to get words with specific morpheme",
    "text": "Using regular expression to get words with specific morpheme\nAny idea for operationalizing derivational morphemes?\n\nOperationalization: *ness"
  },
  {
    "objectID": "2025/slides/session-3.html#results-of-ness",
    "href": "2025/slides/session-3.html#results-of-ness",
    "title": "Session 3: Hands-on #1",
    "section": "Results of *ness",
    "text": "Results of *ness\n\nSearch results *ness"
  },
  {
    "objectID": "2025/slides/session-3.html#using-wild-card-for-lexicalized-pattern",
    "href": "2025/slides/session-3.html#using-wild-card-for-lexicalized-pattern",
    "title": "Session 3: Hands-on #1",
    "section": "Using wild card for lexicalized pattern",
    "text": "Using wild card for lexicalized pattern\n\nLet‚Äôs go back to List search.\nEnter ‚Äúa * of the‚Äù\nWhat result do you expect with this search?\nDon‚Äôt turn to the next page YET!!!"
  },
  {
    "objectID": "2025/slides/session-3.html#result-with-a-of-the",
    "href": "2025/slides/session-3.html#result-with-a-of-the",
    "title": "Session 3: Hands-on #1",
    "section": "Result with a * of the",
    "text": "Result with a * of the\n\n\n\n\na * of the\n\n\n\nThis helps find:\n\nFixed phrases with variable slots\nGrammatical frames\nIdiomatic expressions"
  },
  {
    "objectID": "2025/slides/session-3.html#more-search-methods",
    "href": "2025/slides/session-3.html#more-search-methods",
    "title": "Session 3: Hands-on #1",
    "section": "More search methods",
    "text": "More search methods\n\nYou can check how to use other search methods in English-Corpora.org here"
  },
  {
    "objectID": "2025/slides/session-3.html#more-complex-searches",
    "href": "2025/slides/session-3.html#more-complex-searches",
    "title": "Session 3: Hands-on #1",
    "section": "More complex searches",
    "text": "More complex searches\nWhich is more frequent?\n\nexciting or excited?\nRQ: Is there any reletitive phrase with three adjectives?"
  },
  {
    "objectID": "2025/slides/session-3.html#adj-x-3",
    "href": "2025/slides/session-3.html#adj-x-3",
    "title": "Session 3: Hands-on #1",
    "section": "ADJ x 3",
    "text": "ADJ x 3\n\nthree adjectives\nNote: Sometimes the search results will contain parsing errors. (We will come back to this topic on Day 4.)"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search",
    "href": "2025/slides/session-3.html#collocation-search",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nThis is main topic for Day 3.\nBriefly, collocates search allows us to search for co-occurring words within specified window."
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-1",
    "href": "2025/slides/session-3.html#collocation-search-1",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nSelect collocates\n\n\ncollocation"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-2",
    "href": "2025/slides/session-3.html#collocation-search-2",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nEnter search word\n\n\nentering words"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-3",
    "href": "2025/slides/session-3.html#collocation-search-3",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nOptional enter the word to look for\n\n\nenter collocates"
  },
  {
    "objectID": "2025/slides/session-3.html#collocation-search-4",
    "href": "2025/slides/session-3.html#collocation-search-4",
    "title": "Session 3: Hands-on #1",
    "section": "Collocation search",
    "text": "Collocation search\n\nSpecify window\n\n\nHow distant do you search for the collocates\n\n\nwindow"
  },
  {
    "objectID": "2025/slides/session-3.html#search-methods-summary",
    "href": "2025/slides/session-3.html#search-methods-summary",
    "title": "Session 3: Hands-on #1",
    "section": "Search Methods Summary",
    "text": "Search Methods Summary\n\n\n\n\n\n\n\n\n\n\nMethod\nPurpose\nExample\nKey Features\n\n\n\n\nSimple Word\nFind exact word forms\nrun ‚Üí finds only ‚Äúrun‚Äù\n- Case sensitive- Single form only\n\n\nLEMMA\nFind all forms of a word\nRUN ‚Üí finds ‚Äúrun, runs, ran, running‚Äù\n- Use CAPITAL letters- Includes all inflections\n\n\nPOS Tag\nFind words by grammatical category\nrun_N (noun)run_V (verb)\n- Disambiguates word classes- Use underscore + tag"
  },
  {
    "objectID": "2025/slides/session-3.html#search-methods-summary-1",
    "href": "2025/slides/session-3.html#search-methods-summary-1",
    "title": "Session 3: Hands-on #1",
    "section": "Search Methods Summary",
    "text": "Search Methods Summary\n\n\n\n\n\n\n\n\n\n\nMethod\nPurpose\nExample\nKey Features\n\n\n\n\nKWIC\nView words in context\nAny search term\n- Sort by L1, R1, etc.- Find patterns in usage\n\n\nCHART\nTrack frequency across categories\nlol across time/genres\n- Shows distribution- Genre/time comparisons\n\n\nWildcards\nPattern matching\n*ness ‚Üí ‚Äúhappiness‚Äùa * of the ‚Üí ‚Äúa lot of the‚Äù\n- * = any characters- Find lexical patterns\n\n\nCollocates\nFind co-occurring words\nWords near target\n- Specify window size- Statistical associations"
  },
  {
    "objectID": "2025/slides/session-3.html#your-assignment-structure",
    "href": "2025/slides/session-3.html#your-assignment-structure",
    "title": "Session 3: Hands-on #1",
    "section": "Your Assignment Structure",
    "text": "Your Assignment Structure\n\nResearch Question & Hypothesis (Sessions 1-2)\nMethods (Session 2-3)\n\nChoose corpus\nOperationalize search methods and terms\n\nResults (Session 3)\n\nConduct searches\nInterpret findings"
  },
  {
    "objectID": "2025/slides/session-3.html#research-question-1",
    "href": "2025/slides/session-3.html#research-question-1",
    "title": "Session 3: Hands-on #1",
    "section": "Research question:",
    "text": "Research question:\n\nShould be answerable with corpus data\nFocus on distribution/frequency/context"
  },
  {
    "objectID": "2025/slides/session-3.html#hypothesis-1",
    "href": "2025/slides/session-3.html#hypothesis-1",
    "title": "Session 3: Hands-on #1",
    "section": "Hypothesis:",
    "text": "Hypothesis:\n\nMake a prediction\nExplain your reasoning using the template we have used"
  },
  {
    "objectID": "2025/slides/session-3.html#task-2-methods-1",
    "href": "2025/slides/session-3.html#task-2-methods-1",
    "title": "Session 3: Hands-on #1",
    "section": "Task 2: Methods",
    "text": "Task 2: Methods\nCorpus Selection\n\nWhich corpus/corpora?\nWhy did you choose it/them?\nWhat does it allow you to test?\n\nSearch Plan\n\nHow will you operationalize your concepts?\nLIST, CHART, KWIC, or combination?\nAny special search syntax needed?"
  },
  {
    "objectID": "2025/slides/session-3.html#task-3-results",
    "href": "2025/slides/session-3.html#task-3-results",
    "title": "Session 3: Hands-on #1",
    "section": "Task 3: Results",
    "text": "Task 3: Results\nPresent your findings:\n\nFrequency tables\nKWIC samples\nDistribution charts\n\nInterpret the patterns:\n\nWhat do the numbers mean?\nWhat do the contexts reveal?\nDoes this support your hypothesis?"
  },
  {
    "objectID": "2025/slides/session-3.html#submission-1",
    "href": "2025/slides/session-3.html#submission-1",
    "title": "Session 3: Hands-on #1",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nSuccess Criteria\n\n\nYour submission ‚Ä¶\n\nincludes one or two research questions\narticulates hypotheses\ninclude a list of corpora used\njustifies the selection of corpus\noutlines the search plan (key phrase, regular expression, sorting, filtering, etc.)\nincludes results of your search\nprovides interpretation of the findings."
  },
  {
    "objectID": "2025/slides/session-3.html#start-thinking-now",
    "href": "2025/slides/session-3.html#start-thinking-now",
    "title": "Session 3: Hands-on #1",
    "section": "Start thinking now!",
    "text": "Start thinking now!\nWhat patterns in English interest you?\nConsider:\n\nWords that seem to be changing\nRegional differences\nFormal vs.¬†informal patterns\nNew expressions\nGrammar variations"
  },
  {
    "objectID": "2025/slides/session-3.html#todays-achievement",
    "href": "2025/slides/session-3.html#todays-achievement",
    "title": "Session 3: Hands-on #1",
    "section": "Today‚Äôs achievement",
    "text": "Today‚Äôs achievement\n\nOperationalize linguistic concepts for corpus search\nUse different search types (word, LEMMA, POS)\nAnalyze patterns through KWIC\nEvaluate distributions with CHART\nApply the full research cycle"
  },
  {
    "objectID": "2025/slides/session-4.html#learner-corpus-research",
    "href": "2025/slides/session-4.html#learner-corpus-research",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Learner Corpus Research",
    "text": "Learner Corpus Research\nA strand of research investigating:\n\nWhat language varieties are present among non-native speaker\nHow their native language varieties influence their production\nWhat constitutes second-language proficiency\nHow learners develop their language skills\n\nIn this course, we will mostly focus on question 3 and 4."
  },
  {
    "objectID": "2025/slides/session-4.html#goals-of-sla-research",
    "href": "2025/slides/session-4.html#goals-of-sla-research",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Goals of SLA Research",
    "text": "Goals of SLA Research\nTo make a somewhat bold statement, many SLA researchers are interested in describing:\n\npatterns and rates of L2 development (Ortega & Iberri-Shea, 2005)\nfeatures that distinguishes more or less successful communication (Isaacs & Trofimovich, 2012; Revesz et al., 2014)\n\n\n\nIsaacs, T., & Trofimovich, P. (2012). DECONSTRUCTING COMPREHENSIBILITY: Identifying the Linguistic Influences on Listeners‚Äô L2 Comprehensibility Ratings. Studies in Second Language Acquisition, 34(3), 475‚Äì505. https://doi.org/10.1017/s0272263112000150\nOrtega, L., & Iberri-Shea, G. (2005). LONGITUDINAL RESEARCH IN SECOND LANGUAGE ACQUISITION: RECENT TRENDS AND FUTURE DIRECTIONS. Annual Review of Applied Linguistics, 25, 26‚Äì45. https://doi.org/10.1017/S0267190505000024\nRevesz, A., Ekiert, M., & Torgersen, E. N. (2014). The Effects of Complexity, Accuracy, and Fluency on Communicative Adequacy in Oral Task Performance. Applied Linguistics, amu069. https://doi.org/10.1093/applin/amu069"
  },
  {
    "objectID": "2025/slides/session-4.html#the-interface-between-sla-and-corpus-methods",
    "href": "2025/slides/session-4.html#the-interface-between-sla-and-corpus-methods",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "The interface between SLA and corpus methods",
    "text": "The interface between SLA and corpus methods\n\nGoal: Revealing development paces and patterns\nWe can use corpus linguistic methods to help us identify the features of language use to understand constructs of our interest.\n\n‚Üí More and more SLA researchers rely on corpus methods."
  },
  {
    "objectID": "2025/slides/session-4.html#benefits-and-caveats",
    "href": "2025/slides/session-4.html#benefits-and-caveats",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Benefits and Caveats",
    "text": "Benefits and Caveats\nBenefits:\n\nWe can calculate measures more easily.\n\nCaveats:\n\nResearchers tend to use ‚Äúsimple measures to calculate‚Äù"
  },
  {
    "objectID": "2025/slides/session-4.html#learning-objectives",
    "href": "2025/slides/session-4.html#learning-objectives",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nDiscuss measurement issues in SLA\nExplain the purposes of linguistic measures\nList commonly used lexical measures in second language acquisition research\nExplain sub-constructs of lexical richness measures\n\nLexical Diversity\nLexical Sophistication"
  },
  {
    "objectID": "2025/slides/session-4.html#measurement---what-is-it",
    "href": "2025/slides/session-4.html#measurement---what-is-it",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Measurement - What is it?",
    "text": "Measurement - What is it?\n\nWe talked about: scientific research cycle.\n\n\n\n\nresearch cycle\n\n\n\nAs expansion, we add one more term, measurement."
  },
  {
    "objectID": "2025/slides/session-4.html#the-measurement-process",
    "href": "2025/slides/session-4.html#the-measurement-process",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "The measurement Process",
    "text": "The measurement Process\n\nThe Measurement process"
  },
  {
    "objectID": "2025/slides/session-4.html#construct-definition-and-measurement",
    "href": "2025/slides/session-4.html#construct-definition-and-measurement",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Construct definition and measurement",
    "text": "Construct definition and measurement\n\nConceptual work\n\nConstruct definition: delineates the theoretical interpretation that can be attached to the observation data (or a measure).\n\nDo you want to know about motivation or engagement?\n\nBehavior identification: What kind of behavior do you need to observe?\nTask specification: In what condition do you need to set up to observe the intended behavior?"
  },
  {
    "objectID": "2025/slides/session-4.html#construct-definition-and-measurement-1",
    "href": "2025/slides/session-4.html#construct-definition-and-measurement-1",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Construct definition and measurement",
    "text": "Construct definition and measurement\n\nProcedual work\n\nBehavior elicitation: Target behavior is elicited, observed and recorded.\nObservation scoring: Classifying or coding observed behavior into categories or values to link them to theoretically meaningful interpretation.\nData analysis: Scores are summarized and patterns are described to provide probablistic evaluation of the data."
  },
  {
    "objectID": "2025/slides/session-4.html#compare-and-contrast-the-two-framework",
    "href": "2025/slides/session-4.html#compare-and-contrast-the-two-framework",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Compare and contrast the two framework",
    "text": "Compare and contrast the two framework\n\n\nWhat are the relationships between the two?\nWhat do you think are the key points extracted from the two approaches?\n\n\n\n\n\n\n\nResearch process (Stefanowtisch, 2020)\n\n\n\n\n\n\nThe Measurement process (Norris & Ortega, 2003, p.¬†720)"
  },
  {
    "objectID": "2025/slides/session-4.html#interrim-summary",
    "href": "2025/slides/session-4.html#interrim-summary",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Interrim Summary",
    "text": "Interrim Summary\n\nIn this course, we emphasize the measurement process of the corpus-based linguistic measures.\n\nCritical evaluation of measures/indices\nJustified choice of measures/indices\ncautionary interpretation of measures/indices"
  },
  {
    "objectID": "2025/slides/session-4.html#overview",
    "href": "2025/slides/session-4.html#overview",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Overview",
    "text": "Overview\n\nVocabulary knowledge is usually conceptualized as:\n\nBreadth,\nDepth, and\nFluency\n\nCan anyone explain what these are?"
  },
  {
    "objectID": "2025/slides/session-4.html#compare-the-two-texts.",
    "href": "2025/slides/session-4.html#compare-the-two-texts.",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Compare the two texts.",
    "text": "Compare the two texts.\n\nWhich one do you think reflect ‚Äúbetter‚Äù vocabulary use?\n\n\nIs important for college students to have a part-time job? I think that has much opinion to answer it. The part-time job is a job that can do in partial time. So the college student can do part-time job when they has spare time (if they want). There are many reasons why the college student do part-time job (if they do).\n\nI find it hard to make a generalisation on whether it‚Äôs important or not for college students to have a part-time job, because this seems like something very individual and highly dependent on the individual student and their circumstances. Jobs serve a few main functions: to earn money, to gain experience, to get a head-start in a career, and to have something to do."
  },
  {
    "objectID": "2025/slides/session-4.html#rich-vocabulary-use",
    "href": "2025/slides/session-4.html#rich-vocabulary-use",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Rich vocabulary use?",
    "text": "Rich vocabulary use?\n\n\n\n\ncollege students\nhave a part-time job?\nmuch opinion to answer\npartial time.\nspare time\n\nmany reasons\n\n\n\nmake a generalisation on\nthis seems like\nsomething very individual\nhighly dependent on\ncircumstances.\nserve functions"
  },
  {
    "objectID": "2025/slides/session-4.html#lexical-richness",
    "href": "2025/slides/session-4.html#lexical-richness",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Lexical Richness",
    "text": "Lexical Richness\n\nVocabulary use is usually conceptualized as:\n\nlexical richness (Read, 2000)\n\nlexical diversity (LD) and\nlexical sophistication (LS)"
  },
  {
    "objectID": "2025/slides/session-4.html#lexical-diversity",
    "href": "2025/slides/session-4.html#lexical-diversity",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Lexical diversity",
    "text": "Lexical diversity"
  },
  {
    "objectID": "2025/slides/session-4.html#definition-of-lexical-diversity-ld",
    "href": "2025/slides/session-4.html#definition-of-lexical-diversity-ld",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Definition of lexical diversity (LD)",
    "text": "Definition of lexical diversity (LD)\n\nLD: ‚Äúthe range of different words used in a text‚Äù (Durrant, 2023, p.¬†37)\nThe more different words a learner use, the more lexically diverse.\n\nAssumption\n\nThe more vocabulary they know, the more diverse their vocabulary use is."
  },
  {
    "objectID": "2025/slides/session-4.html#complicating-factor",
    "href": "2025/slides/session-4.html#complicating-factor",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Complicating factor",
    "text": "Complicating factor\n\nLexical diversity can be affected not only by proficiency or vocabulary breadth but also:\n\ntext genre they write in\nlocal cohesion"
  },
  {
    "objectID": "2025/slides/session-4.html#lexical-sophistication",
    "href": "2025/slides/session-4.html#lexical-sophistication",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Lexical sophistication",
    "text": "Lexical sophistication"
  },
  {
    "objectID": "2025/slides/session-4.html#definition-of-lexical-sophistication-ls",
    "href": "2025/slides/session-4.html#definition-of-lexical-sophistication-ls",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Definition of lexical sophistication (LS)",
    "text": "Definition of lexical sophistication (LS)\n\nLS is a bit more difficult to define:\n\nIt attempts to describe qualities of items being used\nSometimes defined as ‚Äúrelative difficulty‚Äù (Kyle, 2020)\n‚Äúsemantic specificity‚Äù or ‚Äúpragmatic appropriateness‚Äù"
  },
  {
    "objectID": "2025/slides/session-4.html#text-internal-vs-external-measures",
    "href": "2025/slides/session-4.html#text-internal-vs-external-measures",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Text-internal vs external measures",
    "text": "Text-internal vs external measures\nSkehan (2009) distinguished:\n\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nText-internal\nThe (learner-produced) text is sufficient for calculation\nLexical Diversity, Lexical Density\n\n\nText-external\nReference corpus is needed to derive score\nLexical Sophistication, Phraseological Sophistication\n\n\n\n\n\nSkehan, P. (2009). Modelling Second Language Performance: Integrating Complexity, Accuracy, Fluency, and Lexis. Applied Linguistics, 30(4), 510‚Äì532. https://doi.org/10.1093/applin/amp047"
  },
  {
    "objectID": "2025/slides/session-4.html#operationalizing-lexical-diversity",
    "href": "2025/slides/session-4.html#operationalizing-lexical-diversity",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Operationalizing lexical diversity",
    "text": "Operationalizing lexical diversity"
  },
  {
    "objectID": "2025/slides/session-4.html#operationalizing-ld",
    "href": "2025/slides/session-4.html#operationalizing-ld",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Operationalizing LD",
    "text": "Operationalizing LD\n\nTypically LD is operationalized by observing the proportion of unique words in a text (Jarvis, 2013, p.¬†88)\n\n\nJarvis, S. (2013). Capturing the Diversity in Lexical Diversity. Language Learning, 63(s1), 87‚Äì106. https://doi.org/10.1111/j.1467-9922.2012.00739.x"
  },
  {
    "objectID": "2025/slides/session-4.html#lexical-diversity-indices",
    "href": "2025/slides/session-4.html#lexical-diversity-indices",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Lexical diversity indices",
    "text": "Lexical diversity indices\nOld LD measures include:\n\n\\(TTR = {nType \\over nToken}\\)\n\\(RootTTR (Guiraud Index) = {nType \\over \\sqrt{nToken}}\\)\n\\(LogTTR = {\\log(nType) \\over \\log(nToken)}\\)\n\\(Maas = {\\log(nTokens) - \\log(nTypes) \\over \\log(nToken)^2}\\)\n‚Üí Never use TTR, RootTTR, LogTTR"
  },
  {
    "objectID": "2025/slides/session-4.html#a-major-problem-with-old-lexical-diversity-measures",
    "href": "2025/slides/session-4.html#a-major-problem-with-old-lexical-diversity-measures",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "A major problem with old lexical diversity measures",
    "text": "A major problem with old lexical diversity measures\n\nThe old LD indices is inherently related to text-lengths (Zenker & Kyle, 2020; Kyle et al., 2024).\n\n\nRelationship between text-lengths and LD measures\nKyle, K., Sung, H., Eguchi, M., & Zenker, F. (2024). Evaluating evidence for the reliability and validity of lexical diversity indices in L2 oral task responses. Studies in Second Language Acquisition, 46(1), 278‚Äì299. https://doi.org/10.1017/S0272263123000402\nZenker & Kyle"
  },
  {
    "objectID": "2025/slides/session-4.html#recent-ld-indices-that-overcame-text-lengths-issue",
    "href": "2025/slides/session-4.html#recent-ld-indices-that-overcame-text-lengths-issue",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Recent LD indices that overcame text-lengths issue",
    "text": "Recent LD indices that overcame text-lengths issue\n\nMean-Average Type Token Ratio (MATTR)\nThe measure of textual lexical diversity (MTLD)"
  },
  {
    "objectID": "2025/slides/session-4.html#mattr",
    "href": "2025/slides/session-4.html#mattr",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "MATTR",
    "text": "MATTR\n\nCalculating TTRs multiple times with sliding windows\n\nProcess:\n\nCalculate TTR for words 1-50\nCalculate TTR for words 2-51\nCalculate TTR for words 3-52\nContinue to end of text\nAverage all TTR values"
  },
  {
    "objectID": "2025/slides/session-4.html#mattr-example",
    "href": "2025/slides/session-4.html#mattr-example",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "MATTR example",
    "text": "MATTR example\nText with 100 words:\nWindow 1: [1-50] ‚Üí TTR = 0.68\nWindow 2: [2-51] ‚Üí TTR = 0.69\nWindow 3: [3-52] ‚Üí TTR = 0.67\n...\nWindow 51: [51-100] ‚Üí TTR = 0.70\nThen by taking average of all windows = 0.685"
  },
  {
    "objectID": "2025/slides/session-4.html#mtld",
    "href": "2025/slides/session-4.html#mtld",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "MTLD",
    "text": "MTLD\n\nMeasures how many words it takes for lexical diversity to ‚Äústabilize‚Äù\nProcess:\n\nReads through text sequentially\nCalculates TTR as it goes\nWhen TTR drops to threshold (traditionally 0.72), marks a ‚Äúfactor‚Äù\nStarts counting again for next factor\n\nScore: Average factor length (higher = more diverse)"
  },
  {
    "objectID": "2025/slides/session-4.html#mtld-example",
    "href": "2025/slides/session-4.html#mtld-example",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "MTLD Example",
    "text": "MTLD Example\nIn the following MTLD was calculated as:\n\n(32 + 45) / 2 = 38\n\nText: \"The cat sat on the mat... The cat was happy. ... The bird flew by.\"\n       |--- Factor 1: 32 words ----------||---- Factor 2: 44 words-----|\n        (TTR drops to 0.72)                (TTR drops to 0.72)\n\nMTLD does this process both forward and backward."
  },
  {
    "objectID": "2025/slides/session-4.html#any-questions",
    "href": "2025/slides/session-4.html#any-questions",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Any questions?",
    "text": "Any questions?"
  },
  {
    "objectID": "2025/slides/session-4.html#operationalizing-lexical-sophistication",
    "href": "2025/slides/session-4.html#operationalizing-lexical-sophistication",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Operationalizing Lexical Sophistication",
    "text": "Operationalizing Lexical Sophistication"
  },
  {
    "objectID": "2025/slides/session-4.html#operationalizing-ls",
    "href": "2025/slides/session-4.html#operationalizing-ls",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Operationalizing LS",
    "text": "Operationalizing LS\n\nHistorically, word frequency information in reference corpora has been used to operationalize LS\n\nExample 1: The big company bought the small business.\nExample 2: The major corporation acquired the diminutive enterprise."
  },
  {
    "objectID": "2025/slides/session-4.html#lexical-frequency-profile-lfp",
    "href": "2025/slides/session-4.html#lexical-frequency-profile-lfp",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Lexical Frequency Profile (LFP)",
    "text": "Lexical Frequency Profile (LFP)\nIn LFP, words in learner text are categorized into different frequency bands in the reference corpus.\n\n\n\n\nBand\nProportion\n\n\n\n\nFirst 1000\n75%\n\n\nSecond 2000\n10%\n\n\nAcademic Word List\n10%\n\n\nOthers\n5%\n\n\n\n\nThis gives you four scores: 75%, 10%, 10%, 5%.\nA simpler version Beyond 2000 (Laufer, 1995) is used more often\n\n\\(\\text{Beyond 2000} = \\frac{\\text{Number of beyond-2000 word types}}{\\text{Total number of word types}}\\)\n\n\n\n\n\nLaufer, B. (1995). Beyond 2000: A measure of productive lexicon in a second language. In L. Eubank, L. Selinker, & M. Sharwood Smith (Eds.), The Current State of Interlanguage: Studies in honor of William E. Rutherford (p.¬†265). John Benjamins Publishing Company. https://doi.org/10.1075/z.73.21lau\nLaufer, B., & Nation, P. (1995). Vocabulary Size and Use: Lexical Richness in L2 Written Production.\nLu, X. (2012). The Relationship of Lexical Richness to the Quality of ESL Learners‚Äô Oral Narratives. The Modern Language Journal, 96(2), 190‚Äì208. https://doi.org/10.1111/j.1540-4781.2011.01232_1.x"
  },
  {
    "objectID": "2025/slides/session-4.html#other-frequency-based-measures",
    "href": "2025/slides/session-4.html#other-frequency-based-measures",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Other frequency-based measures",
    "text": "Other frequency-based measures\n\nP_lex (Meara & Bell, 2000)\nS (Kojima & Yamashita, 2014)\nCount-based measures (Crossley et al., 2014)\n\n\n\nCrossley, S. A., Cobb, T., & McNamara, D. S. (2013). Comparing count-based and band-based indices of word frequency: Implications for active vocabulary research and pedagogical applications. System, 41(4), 965‚Äì981. https://doi.org/10.1016/j.system.2013.08.002\nKojima, M., & Yamashita, J. (2014). Reliability of lexical richness measures based on word lists in short second language productions. System, 42, 23‚Äì33. https://doi.org/10.1016/j.system.2013.10.019\nMeara, P., & Bell, H. (2001). P-Lex: A Simple and Effective Way of Describing the lexical Characteristics of Short L2 Tests. Prospect, 16(3), 5‚Äì19."
  },
  {
    "objectID": "2025/slides/session-4.html#towards-multidimensional-lexical-sophistication",
    "href": "2025/slides/session-4.html#towards-multidimensional-lexical-sophistication",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Towards multidimensional lexical sophistication",
    "text": "Towards multidimensional lexical sophistication\n\nMore recent research recognizes multidimensionality in conceptualize and operationalize LS.\nKyle & Crossley (2015) proposed the Tool of Automated Analysis of Lexical Sophistication (TAALES).\nKim, Crossley & Kyle (2018): proposed multidimensional LS for writing.\nEguchi & Kyle (2020): followed up the concept in L2 speaking."
  },
  {
    "objectID": "2025/slides/session-4.html#categories-of-lexical-sophistication",
    "href": "2025/slides/session-4.html#categories-of-lexical-sophistication",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Categories of lexical sophistication",
    "text": "Categories of lexical sophistication\nDurrant (2023) highlighted several categories:\n\nWord lengths\nWord frequency\nRegister-based measures\nContextual distinctiveness\nSemantic Measures\nPsycholinguistic measures"
  },
  {
    "objectID": "2025/slides/session-4.html#register-based-measures",
    "href": "2025/slides/session-4.html#register-based-measures",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Register-based measures",
    "text": "Register-based measures\nLS should also tap into the extent to which the text uses items that are characteristics of the target register\n\nProportion of words in Academic Word List (Coxhead, 2000)\n\nlow score: I saw that ‚Ä¶\nhigh score: I observed that ‚Ä¶"
  },
  {
    "objectID": "2025/slides/session-4.html#contextual-distinctiveness",
    "href": "2025/slides/session-4.html#contextual-distinctiveness",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Contextual distinctiveness",
    "text": "Contextual distinctiveness\nA word used in a narrower context (specific situation) should be considered more sophisticated.\n\nSemantic Diversity (SemD; Hoffman et al., 2013) \n\n\n\nHoffman, P., Lambon Ralph, M. A., & Rogers, T. T. (2013). Semantic diversity: A measure of semantic ambiguity based on variability in the contextual usage of words. Behavior Research Methods, 45(3), 718‚Äì730. https://doi.org/10.3758/s13428-012-0278-x"
  },
  {
    "objectID": "2025/slides/session-4.html#semantic-measures---concrete-vs-abstract-referents",
    "href": "2025/slides/session-4.html#semantic-measures---concrete-vs-abstract-referents",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Semantic Measures - Concrete vs Abstract referents",
    "text": "Semantic Measures - Concrete vs Abstract referents\nA word that involke more concrete concepts recieve higher score in concreteness.\n\n\n\n\nConcrete\nAbstract\n\n\n\n\nFrequent\ndog, car\nlove, idea\n\n\nInfrequent\ncharger, helmet\nempathy, hypothesis"
  },
  {
    "objectID": "2025/slides/session-4.html#semantic-measures---hypernymy",
    "href": "2025/slides/session-4.html#semantic-measures---hypernymy",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Semantic Measures - Hypernymy",
    "text": "Semantic Measures - Hypernymy\n\n\n\nA word that have a larger set of superordinate terms are high in hypernymy value.\n\n\n\n\n\nWord Net"
  },
  {
    "objectID": "2025/slides/session-4.html#hypernymy",
    "href": "2025/slides/session-4.html#hypernymy",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Hypernymy",
    "text": "Hypernymy\n\n\nResources: WordNet\nOperationalization: The distance (how many layers) to traceback to ‚Äúentity‚Äù.\n\n\n\n\nHypernymy value\nExample words\n\n\n\n\n3 ‚Äì 4\npart, group\n\n\n5 ‚Äì 6\nway, thing\n\n\n7 ‚Äì 8\nhouse, site\n\n\n9 ‚Äì 11\ncar, city\n\n\n12 ‚Äì 14\ndog, cancer\n\n\n15 ‚Äì 17\nbuffalo, stallion\n\n\n18 ‚Äì 19\nbulls, gaur"
  },
  {
    "objectID": "2025/slides/session-4.html#questions",
    "href": "2025/slides/session-4.html#questions",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Questions",
    "text": "Questions\n\nHow useful is Hypernymy is?\nWhich lexical sophistication measures are you interested in?"
  },
  {
    "objectID": "2025/slides/session-4.html#questions-1",
    "href": "2025/slides/session-4.html#questions-1",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "2025/slides/session-4.html#eguchi-kyle-2020",
    "href": "2025/slides/session-4.html#eguchi-kyle-2020",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Eguchi & Kyle (2020)",
    "text": "Eguchi & Kyle (2020)\n\nGoal: Investigate multidimensional nature of lexical sophistication in L2 oral proficiency interviews (OPIs)\nCorpus: NICT JLE corpus (1,281 Japanese L2 English OPIs)\nMethod: Exploratory Factor Analysis (EFA) + regression analysis"
  },
  {
    "objectID": "2025/slides/session-4.html#ls-measures",
    "href": "2025/slides/session-4.html#ls-measures",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "LS Measures",
    "text": "LS Measures\n\n\n\n\nFrequency (e.g., COCA-Spoken, BNC-Spoken)\nRange (e.g., COCA)\nPsycholinguistic norms (concreteness, imageability)\nAge of acquisition/exposure\nSemantic networks (hypernymy, polysemy)\nWord recognition (lexical decision times)\n\n\n\nContextual distinctiveness (semantic diversity)\nWord neighborhood (orthographic/phonological)\nAcademic language (AWL membership)\nN-gram frequency\nN-gram range\nN-gram association (MI, MI2, T-score)"
  },
  {
    "objectID": "2025/slides/session-4.html#sample-dimensions",
    "href": "2025/slides/session-4.html#sample-dimensions",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Sample dimensions",
    "text": "Sample dimensions\n\nFactor 1"
  },
  {
    "objectID": "2025/slides/session-4.html#sample-dimensions-1",
    "href": "2025/slides/session-4.html#sample-dimensions-1",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Sample dimensions",
    "text": "Sample dimensions\n\nFactor 4"
  },
  {
    "objectID": "2025/slides/session-4.html#final-regression-models",
    "href": "2025/slides/session-4.html#final-regression-models",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Final regression models",
    "text": "Final regression models\n\nfinal regression to predict OPI"
  },
  {
    "objectID": "2025/slides/session-4.html#summary",
    "href": "2025/slides/session-4.html#summary",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Summary",
    "text": "Summary\n\nThe final regression model explained 57% of scores in OPI.\nFactor analysis revealed 10 different dimensions: CW, FW, and N-grams."
  },
  {
    "objectID": "2025/slides/session-4.html#the-measurement-process-in-sla",
    "href": "2025/slides/session-4.html#the-measurement-process-in-sla",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "The Measurement Process in SLA",
    "text": "The Measurement Process in SLA\n\nThe measurement process"
  },
  {
    "objectID": "2025/slides/session-4.html#lexical-richness-two-main-components",
    "href": "2025/slides/session-4.html#lexical-richness-two-main-components",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Lexical Richness: Two Main Components",
    "text": "Lexical Richness: Two Main Components\n\n\n\nLexical Diversity (LD)\n\nRange of different words used\nText-internal measure\nAvoid TTR; use MTLD/MATTR\nAffected by text length\n\n\nLexical Sophistication (LS)\n\nQuality/difficulty of words used\nRequires reference corpus\nMultidimensional construct"
  },
  {
    "objectID": "2025/slides/session-4.html#download-tagant-and-taaled",
    "href": "2025/slides/session-4.html#download-tagant-and-taaled",
    "title": "Session 4: Analyzing Vocabulary",
    "section": "Download TagAnt and TAALED",
    "text": "Download TagAnt and TAALED\n\nDownload TagAnt\nDownload Tool for the Automatic Analysis of Lexical Diversity (TAALED)\n\n\nTAALED"
  },
  {
    "objectID": "2025/slides/session-7.html#corpus-lab-2-submission",
    "href": "2025/slides/session-7.html#corpus-lab-2-submission",
    "title": "Session 7: Multiword Units",
    "section": "Corpus Lab 2: Submission",
    "text": "Corpus Lab 2: Submission\n\n\nTask 1: Japanese Word Frequency List and small write-up (5 points)\nTask 2: Replication of Durrant‚Äôs analysis from Figure 4.19 (5 points)\n\nresearch question,\nhypothesis,\nplots, and\nresults\n\nTask 3: Comparison of two texts in terms of lexical sophistication (5 points)"
  },
  {
    "objectID": "2025/slides/session-7.html#corpus-lab-2-task-1",
    "href": "2025/slides/session-7.html#corpus-lab-2-task-1",
    "title": "Session 7: Multiword Units",
    "section": "Corpus Lab 2: Task 1",
    "text": "Corpus Lab 2: Task 1\n\nInstruction"
  },
  {
    "objectID": "2025/slides/session-7.html#task-1-compile-a-japanese-word-frequency-list",
    "href": "2025/slides/session-7.html#task-1-compile-a-japanese-word-frequency-list",
    "title": "Session 7: Multiword Units",
    "section": "Task 1: Compile a Japanese Word Frequency list",
    "text": "Task 1: Compile a Japanese Word Frequency list\n\nTask\nCompile a Japanese frequency list based on a corpus.\nResource\n\nDownload a Japanese text Aozora 500 from Google Drive.\nUse AntConc, TagAnt, and Simple Text Analyzer."
  },
  {
    "objectID": "2025/slides/session-7.html#submission-for-task-1",
    "href": "2025/slides/session-7.html#submission-for-task-1",
    "title": "Session 7: Multiword Units",
    "section": "Submission for task 1",
    "text": "Submission for task 1\n\n\nSubmit a frequency list .tsv or .txt.\nA short description of word frequency pattern in Japanese.\n\n\n\n\n\n\n\n\nSuccess Criteria\n\n\nYour submission ‚Ä¶\n\nincludes a frequency list of Japanese words based on Aozora 500\nprovides a description of word-frequency patterns in Aozora 500 using example words from each frequency bins"
  },
  {
    "objectID": "2025/slides/session-7.html#corpus-lab-2-task-2",
    "href": "2025/slides/session-7.html#corpus-lab-2-task-2",
    "title": "Session 7: Multiword Units",
    "section": "Corpus Lab 2: Task 2",
    "text": "Corpus Lab 2: Task 2"
  },
  {
    "objectID": "2025/slides/session-7.html#instruction",
    "href": "2025/slides/session-7.html#instruction",
    "title": "Session 7: Multiword Units",
    "section": "Instruction",
    "text": "Instruction\nGoal: to replicate analysis on GiG.\nYou will need to have access to both metadata file.\nThe corpus data is here."
  },
  {
    "objectID": "2025/slides/session-7.html#about-gig-meta-data",
    "href": "2025/slides/session-7.html#about-gig-meta-data",
    "title": "Session 7: Multiword Units",
    "section": "About GiG meta data",
    "text": "About GiG meta data\n\nGiG metadata documents the necessary data to use for plotting\n\nYear Group (X-axis in Figure 4.19)\nGenre (grouping variable in Figure 4.19)\n\n\n\nGiG Metadata"
  },
  {
    "objectID": "2025/slides/session-7.html#writing-up-research-question-hypothesis-and-results",
    "href": "2025/slides/session-7.html#writing-up-research-question-hypothesis-and-results",
    "title": "Session 7: Multiword Units",
    "section": "Writing up research question, hypothesis, and results",
    "text": "Writing up research question, hypothesis, and results\n\n\nResearch question?\nHypothesis: Write your own.\nResults: Write your own.\n\n\n\n\n\n\n\nSuccess Criteria\n\n\nYour submission ‚Ä¶\n\nincludes a spreatsheet that contains lexical diversity (i.e., TTR, Root TTR, Log TTR, MAAS) scores for the sample texts\nprovides two sets of plots that describe trends of lexical diversity across year groups and genre.\nprovides 200-300 word replication report on lexical diversity trends in GiG corpus, presented as Figure 4.19 in Durrant (2023)."
  },
  {
    "objectID": "2025/slides/session-7.html#corpus-lab-2-task-3",
    "href": "2025/slides/session-7.html#corpus-lab-2-task-3",
    "title": "Session 7: Multiword Units",
    "section": "Corpus Lab 2: Task 3",
    "text": "Corpus Lab 2: Task 3"
  },
  {
    "objectID": "2025/slides/session-7.html#comparing-lexical-characteristics-of-two-texts",
    "href": "2025/slides/session-7.html#comparing-lexical-characteristics-of-two-texts",
    "title": "Session 7: Multiword Units",
    "section": "Comparing lexical characteristics of two texts",
    "text": "Comparing lexical characteristics of two texts\n\nGoals\n\nCompare and contrast two texts along with several lexical diversity metrics\nObserve differences in single-word and multiword sophistication\n\nData\n\nChoose two texts from the ICNALE GRA\nIf you are unsure, use choose from the following three files:\n\nGRA_PTJ0_124_ORIG.txt\nGRA_PTJ0_070_ORIG.txt\nGRA_PTJ0_112_ORIG.txt"
  },
  {
    "objectID": "2025/slides/session-7.html#step-1-qualitatively-compare-two-files",
    "href": "2025/slides/session-7.html#step-1-qualitatively-compare-two-files",
    "title": "Session 7: Multiword Units",
    "section": "Step 1: Qualitatively compare two files",
    "text": "Step 1: Qualitatively compare two files\n\nBefore we actually obtain lexical sophistication measures, compare two texts in terms of their lexical use.\nIn pairs, describe the strengths and weakeness in vocabulary use."
  },
  {
    "objectID": "2025/slides/session-7.html#step-2-hypothesis",
    "href": "2025/slides/session-7.html#step-2-hypothesis",
    "title": "Session 7: Multiword Units",
    "section": "Step 2: Hypothesis",
    "text": "Step 2: Hypothesis\n\nIn what way is one text more lexically sophisticated than the other?\n\nTry to come up with characteristics that describe the quality of word use in each text"
  },
  {
    "objectID": "2025/slides/session-7.html#step-3-pick-two-or-three-lexical-sophistication-variables",
    "href": "2025/slides/session-7.html#step-3-pick-two-or-three-lexical-sophistication-variables",
    "title": "Session 7: Multiword Units",
    "section": "Step 3: Pick two or three lexical sophistication variables",
    "text": "Step 3: Pick two or three lexical sophistication variables\nEnter the text into analyzer\nWe can also compare two texts in simple text analyzer.\n\ntwo-text"
  },
  {
    "objectID": "2025/slides/session-7.html#step-4-run-analyses",
    "href": "2025/slides/session-7.html#step-4-run-analyses",
    "title": "Session 7: Multiword Units",
    "section": "Step 4: Run analyses",
    "text": "Step 4: Run analyses\nPlots that compares two lists\n!"
  },
  {
    "objectID": "2025/slides/session-7.html#step-5-interpret-the-findings",
    "href": "2025/slides/session-7.html#step-5-interpret-the-findings",
    "title": "Session 7: Multiword Units",
    "section": "Step 5: Interpret the findings",
    "text": "Step 5: Interpret the findings\n\nLet‚Äôs discuss how the two text differ in lexical use from one another.\nUse the tables with token information and visualization to (dis)confirm your hypothesis\n\n\n\n\n\n\n\nSuccess Criteria\n\n\nYour submission ‚Ä¶\n\nincludes plots from one frequency index and one other type of index\nprovides desciption of how two texts differ in terms of the selected lexical sophistication indices."
  },
  {
    "objectID": "2025/slides/session-7.html#learning-objectives",
    "href": "2025/slides/session-7.html#learning-objectives",
    "title": "Session 7: Multiword Units",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\nBy the end of this session, students will be able to:\n\n\n\nExplain different types of multiword units: collocation, n-grams, lexical bundles\nDemonstrate how major association strengths measures (t-score, Mutual Information, and LogDice) are calculated using examples"
  },
  {
    "objectID": "2025/slides/session-7.html#lexical-richness",
    "href": "2025/slides/session-7.html#lexical-richness",
    "title": "Session 7: Multiword Units",
    "section": "Lexical Richness",
    "text": "Lexical Richness\n\nLexical Diversity\nLexical Sophistication\n\n‚Üí These are not sufficient for complete analysis of learner language."
  },
  {
    "objectID": "2025/slides/session-7.html#remember-the-comparisons",
    "href": "2025/slides/session-7.html#remember-the-comparisons",
    "title": "Session 7: Multiword Units",
    "section": "Remember the comparisons?",
    "text": "Remember the comparisons?\n\nWhich one do you think reflect ‚Äúbetter‚Äù vocabulary use?\n\n\nIs important for college students to have a part-time job? I think that has much opinion to answer it. The part-time job is a job that can do in partial time. So the college student can do part-time job when they has spare time (if they want). There are many reasons why the college student do part-time job (if they do).\n\nI find it hard to make a generalisation on whether it‚Äôs important or not for college students to have a part-time job, because this seems like something very individual and highly dependent on the individual student and their circumstances. Jobs serve a few main functions: to earn money, to gain experience, to get a head-start in a career, and to have something to do."
  },
  {
    "objectID": "2025/slides/session-7.html#rich-vocabulary-use",
    "href": "2025/slides/session-7.html#rich-vocabulary-use",
    "title": "Session 7: Multiword Units",
    "section": "Rich vocabulary use?",
    "text": "Rich vocabulary use?\n\n\n\n\ncollege students\nhave a part-time job\nmuch opinion to answer\npartial time\nspare time\n\nmany reasons\n\n\n\nmake a generalisation on\nthis seems like\nsomething very individual\nhighly dependent on\ncircumstances.\nserve functions"
  },
  {
    "objectID": "2025/slides/session-7.html#phraseology-is-an-important-addition",
    "href": "2025/slides/session-7.html#phraseology-is-an-important-addition",
    "title": "Session 7: Multiword Units",
    "section": "Phraseology is an important addition",
    "text": "Phraseology is an important addition\n\nResearch suggests that learners need to know ‚Äúhow to combine words‚Äù.\nPhraseological complexity (Paquot, 2019)\n\nWe will talk about it more later‚Ä¶\n\n\n\n\nPaquot, M. (2018). Phraseological Competence: A Missing Component in University Entrance Language Tests? Insights From a Study of EFL Learners‚Äô Use of Statistical Collocations. Language Assessment Quarterly, 15(1), 29‚Äì43. https://doi.org/10.1080/15434303.2017.1405421\nPaquot, M. (2019). The phraseological dimension in interlanguage complexity research. Second Language Research, 35(1), 121‚Äì145. https://doi.org/10.1177/0267658317694221"
  },
  {
    "objectID": "2025/slides/session-7.html#usage-based-learning",
    "href": "2025/slides/session-7.html#usage-based-learning",
    "title": "Session 7: Multiword Units",
    "section": "Usage-based learning",
    "text": "Usage-based learning\n\nThe idea that item-based learning facilitate acquisition of systematic aspects of language.\n\nRepeated exposure leads to abstraction of the schematic language unit.\n\nSo usage/exposure in communicative contexts allows the learner to extract grammatical rules/learn how to express a concept."
  },
  {
    "objectID": "2025/slides/session-7.html#definitions",
    "href": "2025/slides/session-7.html#definitions",
    "title": "Session 7: Multiword Units",
    "section": "Definitions",
    "text": "Definitions\n\nMultiword Unit/Sequence is a cover term for different things.\n\n\n\n\n\n\n\n\nFormula type\nDescription\nExamples\n\n\n\n\nPhrasal verbs\nverbs followed by an adverbial particle, where the phrase as a whole is used with a non-literal meaning\nblow up, shut down\n\n\nIdioms\na relatively fixed sequence of words with a non-literal, typically metaphorical meaning\nkick the bucket\n\n\nBinomials\nrecurrent conventional phrase of two words from the same POS, connected by a conjunction\nblack and white\n\n\nPragmatic formulas\ncontext-bound phrases that are characteristic of a particular speech community\nthank you, bless you"
  },
  {
    "objectID": "2025/slides/session-7.html#definitions-1",
    "href": "2025/slides/session-7.html#definitions-1",
    "title": "Session 7: Multiword Units",
    "section": "Definitions",
    "text": "Definitions\n\n\n\n\n\n\n\n\n\nFormula type\nDescription\nExamples\n\n\n\n\nCollocations\nPairs of words that are syntagmatically associated\ntake time, young people\n\n\nLexical bundles\nContiguous word combinations which occur very frequently in language\nin other words\n\n\nLexicalized sentence stems\nA conventional expression in whih some elements are fixed and others allow for a range of possibilities\nwhat I want you to do is"
  },
  {
    "objectID": "2025/slides/session-7.html#approaches-to-define-and-operationalize-mwus",
    "href": "2025/slides/session-7.html#approaches-to-define-and-operationalize-mwus",
    "title": "Session 7: Multiword Units",
    "section": "Approaches to define and operationalize MWUs",
    "text": "Approaches to define and operationalize MWUs\nWe have two major approaches to define MWUs:\n\n\n\n\n\n\n\n\nApproach\nDescription\n\n\n\n\nFrequency-based\nQuantifies the degree of associations between component words\n\n\nPhraseological\nInvolves human judgement to focus on semantic transparency, non-compositionality, and/or fixedness.\n\n\n\n\n\nWe will follow frequency-based approach in this course."
  },
  {
    "objectID": "2025/slides/session-7.html#preview-discussion",
    "href": "2025/slides/session-7.html#preview-discussion",
    "title": "Session 7: Multiword Units",
    "section": "Preview discussion",
    "text": "Preview discussion\n\nWhich of the following type of MWUs are more challenging to identify from the frequency-based approach?\n\nCollocation: play + role, meet + expectation\nBinomials: black and white\nIdiom: kick the bucket, over the moon\nLexical bundles: in terms of the, the extent to which"
  },
  {
    "objectID": "2025/slides/session-7.html#questions",
    "href": "2025/slides/session-7.html#questions",
    "title": "Session 7: Multiword Units",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "2025/slides/session-7.html#extracting-recurrent-units",
    "href": "2025/slides/session-7.html#extracting-recurrent-units",
    "title": "Session 7: Multiword Units",
    "section": "Extracting recurrent units",
    "text": "Extracting recurrent units\nWe can extract recurrent units by:\n\nCounting the number of contigous sequences over a reference corpus\nManually filter them OR measuring Strengths Of Association"
  },
  {
    "objectID": "2025/slides/session-7.html#n-grams",
    "href": "2025/slides/session-7.html#n-grams",
    "title": "Session 7: Multiword Units",
    "section": "N-grams",
    "text": "N-grams\n\nN-grams: contiguous sequences of n words\n\nTake N-words using a sliding window approach.\n\n\nExample: I have not had Gyutan yet this time.\nBigram: [I have] [have not] [not had] ... [this time].\nTrigram: [I have not] [have not had] [not had Gyutan] ..."
  },
  {
    "objectID": "2025/slides/session-7.html#top-12-n-grams-in-brown-corpus",
    "href": "2025/slides/session-7.html#top-12-n-grams-in-brown-corpus",
    "title": "Session 7: Multiword Units",
    "section": "Top 12 N-grams in BROWN corpus",
    "text": "Top 12 N-grams in BROWN corpus\n\n4-grams corpus"
  },
  {
    "objectID": "2025/slides/session-7.html#n-grams-in-brown-corpus-111th---122nd",
    "href": "2025/slides/session-7.html#n-grams-in-brown-corpus-111th---122nd",
    "title": "Session 7: Multiword Units",
    "section": "N-grams in BROWN corpus (111th - 122nd)",
    "text": "N-grams in BROWN corpus (111th - 122nd)\n\n4-grams from 111-122"
  },
  {
    "objectID": "2025/slides/session-7.html#lexical-bundles",
    "href": "2025/slides/session-7.html#lexical-bundles",
    "title": "Session 7: Multiword Units",
    "section": "Lexical bundles",
    "text": "Lexical bundles\n\n\n‚ÄúFixed sequence of words that recur frequently‚Äù\n\nEssentially, N-grams plus manual filtering and categorization\n\nBiber (2004) identified lexical bundles from Classroom teaching, Textbooks, Conversation and Academic prose\n\n\n\n\nT2K-SWAL\n\n\n\n\n\n\n\nBiber, D. (2004). If you look at ‚Ä¶: Lexical Bundles in University Teaching and Textbooks. Applied Linguistics, 25(3), 371‚Äì405. https://doi.org/10.1093/applin/25.3.371"
  },
  {
    "objectID": "2025/slides/session-7.html#t2k-swal-corpus",
    "href": "2025/slides/session-7.html#t2k-swal-corpus",
    "title": "Session 7: Multiword Units",
    "section": "T2K-SWAL corpus",
    "text": "T2K-SWAL corpus\n\n\nAttempts to represent university language\n\n\n\n\n\n\n\n\nPerspective\nExample\n\n\n\n\nGenre representation\n‚Äúclassroom teaching, office hours, study groups, on-campus service encounters, textbooks, course packs, and other written materials e.g.¬†university catalogues, brochures.‚Äù (Biber et al., 2004)\n\n\nDisciplines\nBusiness, Education, Engineering, Humanities, Natural Science, and Social Science\n\n\nEducational level\nLower devision undergrad, Upper devision undergrad, and Graduate.\n\n\nRegional variation\nNorthern Arizona University, Iowa State University, California State University at Sacramento, Georgia State University"
  },
  {
    "objectID": "2025/slides/session-7.html#methods-to-identify-lexical-bundles",
    "href": "2025/slides/session-7.html#methods-to-identify-lexical-bundles",
    "title": "Session 7: Multiword Units",
    "section": "Methods to identify lexical bundles",
    "text": "Methods to identify lexical bundles\n\nUsed n-grams to compile a first list.\nFiltered the list with 40 times per million threshold.\nKept four-word sequences\nRange requirements was set to 5 texts."
  },
  {
    "objectID": "2025/slides/session-7.html#communicative-functions",
    "href": "2025/slides/session-7.html#communicative-functions",
    "title": "Session 7: Multiword Units",
    "section": "Communicative Functions",
    "text": "Communicative Functions\n\n\nBiber et al.¬†(2004) presented three important ‚Äúfunctions‚Äù of bundles\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nReferential\nSequences that ‚Äúmakes direct referents to physical or abstract entities‚Äù.\nthere's a lot of, a little bit of, in terms of the, as a result of\n\n\nStance\n‚Äúexpresses attitudes or assessments of certainty that frame some other proposition‚Äù\nI don't know if, are more likely, you might want to, it is important to\n\n\nDiscourse organizer\nsignals textual relations between current and previous/upcoming discourse\nif you look at, let's have a look, I mean you know"
  },
  {
    "objectID": "2025/slides/session-7.html#classify-the-following-bundles-into-three-categories",
    "href": "2025/slides/session-7.html#classify-the-following-bundles-into-three-categories",
    "title": "Session 7: Multiword Units",
    "section": "Classify the following bundles into three categories",
    "text": "Classify the following bundles into three categories\n\nChoice: Referential, Stance, Discourse\n\n\n\n\nBundle\nCategory\n\n\n\n\non the other hand\n\n\n\nI don‚Äôt know how\n\n\n\ngreater than or equal\n\n\n\nyou have to be"
  },
  {
    "objectID": "2025/slides/session-7.html#practical-considerations",
    "href": "2025/slides/session-7.html#practical-considerations",
    "title": "Session 7: Multiword Units",
    "section": "Practical considerations",
    "text": "Practical considerations\nTo be able to identify interesting recurrent patterns\n\nThe size of the corpus;\nlengths of each text in your corpus\nRepresentation of genre, topic, register\nReliability issue of manual classification of functions"
  },
  {
    "objectID": "2025/slides/session-7.html#summary-of-recurrent-units",
    "href": "2025/slides/session-7.html#summary-of-recurrent-units",
    "title": "Session 7: Multiword Units",
    "section": "Summary of recurrent units",
    "text": "Summary of recurrent units\n\nN-gram search (counting contiguous sequences of n-words) allows recurrent linguistic patterns to emerge.\n\nN-grams serve as first list for lexical bundle.\n\nLexical bundles are automatically identified, then filtered with frequency.\n\nThey are manually classified into functional categories."
  },
  {
    "objectID": "2025/slides/session-7.html#extracting-co-occurring-units-or-collocations",
    "href": "2025/slides/session-7.html#extracting-co-occurring-units-or-collocations",
    "title": "Session 7: Multiword Units",
    "section": "Extracting co-occurring units OR Collocations",
    "text": "Extracting co-occurring units OR Collocations\nWhen we talk about co-occurrence, things are a bit more complex.\n\nLet‚Äôs consider meet + expectation\nCan you make 3 sentences using this collocation?"
  },
  {
    "objectID": "2025/slides/session-7.html#examples-from-you",
    "href": "2025/slides/session-7.html#examples-from-you",
    "title": "Session 7: Multiword Units",
    "section": "Examples from you",
    "text": "Examples from you\n\nThe results of the experiment didn‚Äôt meet my expectation.\nThe results met the expectation that I have made before the experiment.\nWe hope the new policy will meet the public expectation.\nThe expectation of the univerity was met."
  },
  {
    "objectID": "2025/slides/session-7.html#the-nature-of-collocation",
    "href": "2025/slides/session-7.html#the-nature-of-collocation",
    "title": "Session 7: Multiword Units",
    "section": "The nature of collocation",
    "text": "The nature of collocation\n\nCollocation is about ‚Äúpreferences based on conventional uses‚Äù\n\n‚ÄúSemantic priming‚Äù (Hoey, 2005)"
  },
  {
    "objectID": "2025/slides/session-7.html#window-based-approach-basic",
    "href": "2025/slides/session-7.html#window-based-approach-basic",
    "title": "Session 7: Multiword Units",
    "section": "Window-based approach (Basic)",
    "text": "Window-based approach (Basic)\n\nWindow-based approach is most widely used (including AntConc).\n\nIt counts the number of words within a specified window\ne.g., +/- 4 words window = counts items 4 words before and after the node word.\nI    strongly  think  the  expectation  was  met   for  the  ...\n[L4]   [L3]    [L2]  [L1]   [ node ]   [R1]  [R2] [R3]  [R4]\n\nThis is called surface-level co-occurrences (Evert, 2008)"
  },
  {
    "objectID": "2025/slides/session-7.html#window-based-approach",
    "href": "2025/slides/session-7.html#window-based-approach",
    "title": "Session 7: Multiword Units",
    "section": "Window-based approach",
    "text": "Window-based approach\n\nSometimes, the collocates can be more distant.\nI    strongly  think  the  expectation  for  the  final exam  was  met ....\n[L4]   [L3]    [L2]  [L1]   [ node ]   [R1]  [R2] [R3]  [R4]      [???]\nCannot capture this unless we expand the window‚Ä¶.\nBut this raises trade-off between noises and more accurate retrieval."
  },
  {
    "objectID": "2025/slides/session-7.html#dependency-parsing-advanced",
    "href": "2025/slides/session-7.html#dependency-parsing-advanced",
    "title": "Session 7: Multiword Units",
    "section": "Dependency Parsing (Advanced)",
    "text": "Dependency Parsing (Advanced)\n\nMore recent research uses Syntactic co-occurrences\nWe can parse grammatical relations using dependency parsing\nDependency parsing (‰øÇÂèó„ÅëËß£Êûê) (more details tomorrow) can indicate grammatical relations between words."
  },
  {
    "objectID": "2025/slides/session-7.html#dependency-parsing-advanced-1",
    "href": "2025/slides/session-7.html#dependency-parsing-advanced-1",
    "title": "Session 7: Multiword Units",
    "section": "Dependency Parsing (Advanced)",
    "text": "Dependency Parsing (Advanced)\n\nWith dependency, we can capture meet + expectation accurately (even if they are far away).\n\n\nDependency Parsing example"
  },
  {
    "objectID": "2025/slides/session-7.html#typical-dependency-relations-for-lexical-collocations",
    "href": "2025/slides/session-7.html#typical-dependency-relations-for-lexical-collocations",
    "title": "Session 7: Multiword Units",
    "section": "Typical dependency relations for lexical collocations",
    "text": "Typical dependency relations for lexical collocations\n\nA few depedency labels are useful to identify collocations\n\n\n\n\n\n\n\n\nDependency label\nName\nExample\n\n\n\n\namod\nAdjectival Modifier\nsignificant + change\n\n\ndobj\nDirect Object\nplay + role in active voice (play a role)\n\n\nnsubjpass\nNominal subject of a passive construction\nplay + role in passive voice (role is played)\n\n\nadvmod\nAdverbial modifier\nchange + incrementally"
  },
  {
    "objectID": "2025/slides/session-7.html#summary-for-co-occurrence",
    "href": "2025/slides/session-7.html#summary-for-co-occurrence",
    "title": "Session 7: Multiword Units",
    "section": "Summary for co-occurrence",
    "text": "Summary for co-occurrence\nThere are two main ways to capture co-occurrence\n\nWindow-based approach (AntConc)\nDependency-based approach (needs advanced NLP tools; but we will cover)."
  },
  {
    "objectID": "2025/slides/session-7.html#why-is-frequency-not-sufficient",
    "href": "2025/slides/session-7.html#why-is-frequency-not-sufficient",
    "title": "Session 7: Multiword Units",
    "section": "Why is frequency not sufficient?",
    "text": "Why is frequency not sufficient?\nDiscuss:\n\nResearchers say that frequency alone is not sufficient to identify useful multiword sequences. Why?\n\n\ncollocation-freq"
  },
  {
    "objectID": "2025/slides/session-7.html#strengths-of-association-measures",
    "href": "2025/slides/session-7.html#strengths-of-association-measures",
    "title": "Session 7: Multiword Units",
    "section": "Strengths Of Association Measures",
    "text": "Strengths Of Association Measures\n\nThe issue: A word can co-occur just because they are frequent by default.\nStrengths Of Association (SOA) provides ways to take ‚Äúfrequency‚Äù of individual words account.\nSeveral SOA measures are commonly used (Gablasova et al., 2017).\n\nT-score\nMutual Information\nLogDice\n\nNow let‚Äôs first look at expected cooccurrences."
  },
  {
    "objectID": "2025/slides/session-7.html#mathematical-look-into-collocational-association",
    "href": "2025/slides/session-7.html#mathematical-look-into-collocational-association",
    "title": "Session 7: Multiword Units",
    "section": "Mathematical look into collocational association",
    "text": "Mathematical look into collocational association\nWhen we think about ‚Äúassociation‚Äù, we can think about it from a probability perspective.\n\n\n\n\n\n\n\n\n\n\n# with word 1\n# without word 1\n\n\n\n\n# with word 2\nFrequency of collocation\nRemaining frequency of word 2\n\n\n# without word 2\nRemaining frequency of word 1\nRest of corpus size"
  },
  {
    "objectID": "2025/slides/session-7.html#suppose-we-have-following",
    "href": "2025/slides/session-7.html#suppose-we-have-following",
    "title": "Session 7: Multiword Units",
    "section": "Suppose we have following",
    "text": "Suppose we have following\n\n\nCollocation freq: 50\nWord 1: 500\nWord 2: 500\nEntire corpus: 1,000,000\n\n\n\n\n\n\n\n# with word 1\n# without word 1\nTotal\n\n\n\n\n# with word 2\n50\n450\n500\n\n\n# without word 2\n450\n999,050\n999,500\n\n\nTotal\n500\n999,500\n1,000,000"
  },
  {
    "objectID": "2025/slides/session-7.html#expected-ocurrences-corrected",
    "href": "2025/slides/session-7.html#expected-ocurrences-corrected",
    "title": "Session 7: Multiword Units",
    "section": "Expected Ocurrences (Corrected)",
    "text": "Expected Ocurrences (Corrected)\n\nExpected co-occurrences are usually calculated as follows: \\[E_{11} = {(\\text{freq of node word} * \\text{freq of collocate } ) \\over Corpus size}\\]\n\nLet‚Äôs calculate the expected frequency.\n\nIf word1 and word 2 occur 500 times each in a million word corpus‚Ä¶\n\n\n(500 * 500) / 1000000\n\n[1] 0.25"
  },
  {
    "objectID": "2025/slides/session-7.html#mutual-information",
    "href": "2025/slides/session-7.html#mutual-information",
    "title": "Session 7: Multiword Units",
    "section": "Mutual Information",
    "text": "Mutual Information\nNow we are ready to calculate MI.\n\nThe expected frequency = .25\nThe observed frequency = 50\n\n\\[MI = {log_2{ \\text{Observed freq} \\over \\text{Expected frequency} }}\\]\nLet‚Äôs do this:\n\nMI = log(50/0.25, base = 2)\nMI\n\n[1] 7.643856"
  },
  {
    "objectID": "2025/slides/session-7.html#mutual-information-1",
    "href": "2025/slides/session-7.html#mutual-information-1",
    "title": "Session 7: Multiword Units",
    "section": "Mutual Information",
    "text": "Mutual Information\nMI is the ratio between observed and expected frequency in logarithmic scale.\n\\[MI = {log_2{ \\text{Observed freq} \\over \\text{Expected frequency} }}\\]"
  },
  {
    "objectID": "2025/slides/session-7.html#t-score",
    "href": "2025/slides/session-7.html#t-score",
    "title": "Session 7: Multiword Units",
    "section": "T-score",
    "text": "T-score\nT-score is calculated as follows:\n\n\\(\\text{T-score} = {\\text{Observed}  - \\text{Expected}  \\over \\sqrt{Observed}}\\)\n\nUsing the same numbers:\n\ntscore = (50 - 0.25) / sqrt(50)\ntscore\n\n[1] 7.035712"
  },
  {
    "objectID": "2025/slides/session-7.html#logdice",
    "href": "2025/slides/session-7.html#logdice",
    "title": "Session 7: Multiword Units",
    "section": "LogDice",
    "text": "LogDice\n\nLogDice is somewhat different in calculation.\nIt does not use Expected frequency\nThat is, it does not rely on ‚Äúcorpus size‚Äù normalization.\n\n\\(\\text{log Dice} = 14 + \\log_2( {{2 \\times Observed} \\over {R_1 + C_1}})\\)\n\nlogdice = 14 + log(2 * 50 / (500 + 500), base = 2)\nlogdice\n\n[1] 10.67807"
  },
  {
    "objectID": "2025/slides/session-7.html#summary",
    "href": "2025/slides/session-7.html#summary",
    "title": "Session 7: Multiword Units",
    "section": "Summary",
    "text": "Summary\n\nMajor association measures are MI, T-score, and LogDice.\nWe covered how to calculate each.\n\n\nMI; tscore; logdice\n\n[1] 7.643856\n\n\n[1] 7.035712\n\n\n[1] 10.67807"
  },
  {
    "objectID": "2025/slides/session-7.html#phraseological-complexity-1",
    "href": "2025/slides/session-7.html#phraseological-complexity-1",
    "title": "Session 7: Multiword Units",
    "section": "Phraseological complexity",
    "text": "Phraseological complexity\n\nPaquot (2019): ‚Äúthe range of phraseological units that surface in language production and the degree of sophistication of such phraseological units‚Äù (p.¬†124)\n\n\n\n\n\n\n\nConstructs\nDescription\n\n\n\n\nPhraseological diversity\n‚Äúa (derived) type‚Äìtoken ratio representing the number of unique phraseological units to the total number of phraseological units, by analogy with the measurement of lexical diversity.‚Äù (p.¬†125)\n\n\nPhraseological sophistication\n‚Äúthe selection of word combinations that are ‚Äòappropriate to the topic and style of writing, rather than just general, everyday vocabulary‚Äô‚Äù (p.¬†125)\n\n\n\n\n\n\nPaquot, M. (2018). Phraseological Competence: A Missing Component in University Entrance Language Tests? Insights From a Study of EFL Learners‚Äô Use of Statistical Collocations. Language Assessment Quarterly, 15(1), 29‚Äì43. https://doi.org/10.1080/15434303.2017.1405421\nPaquot, M. (2019). The phraseological dimension in interlanguage complexity research. Second Language Research, 35(1), 121‚Äì145. https://doi.org/10.1177/0267658317694221"
  },
  {
    "objectID": "2025/slides/session-7.html#operationalizing-phraseological-diversity",
    "href": "2025/slides/session-7.html#operationalizing-phraseological-diversity",
    "title": "Session 7: Multiword Units",
    "section": "Operationalizing Phraseological Diversity",
    "text": "Operationalizing Phraseological Diversity\n\n\nNot much progress compared to lexical diversity.\nPaquot (2019) uses RootTTR for operationalization\n\nPU = Phraseological unit\n\n\n\\(\\text{Diversity of PU} = {\\text{Type of PU} \\over \\sqrt{\\text{Token of PU}}}\\)\n\nGiven the findings on lexical sophistication, an application of Moving-Average could be interesting."
  },
  {
    "objectID": "2025/slides/session-7.html#operationalizing-phraseological-sophistication",
    "href": "2025/slides/session-7.html#operationalizing-phraseological-sophistication",
    "title": "Session 7: Multiword Units",
    "section": "Operationalizing Phraseological Sophistication",
    "text": "Operationalizing Phraseological Sophistication\n\nAs with lexical sophistication measures, typically Mean is typically used.\n\n\\(\\text{Sophistication of PU} = {\\text{Total score for SOAs} \\over \\text{# of Token with SOA values}}\\)\n\ne.g.,\\(\\text{Mean MI for amod} = {\\text{Total MI score for amod} \\over \\text{# of amod pairs with MI}}\\)"
  },
  {
    "objectID": "2025/slides/session-7.html#basic-findings",
    "href": "2025/slides/session-7.html#basic-findings",
    "title": "Session 7: Multiword Units",
    "section": "Basic findings",
    "text": "Basic findings\n\n\nRecent studies demonstrate benefits of adding phraseological complexity measures to explain second language proficiency:\n\nEguchi & Kyle (2020) in Oral Proficieincy Interview\nPaquot (2019): Dependency collocation explained more variance than syntactic complexity\nKyle & Eguchi (2021): Dependency-based collocation explained TOEFL speaking scores more than n-gram measures.\n\nMore research should integrate Phraseological Complexity more in explaining developmental patterns\n\ne.g., Siyanova & Spina (2020)\n\n\n\n\n\nPaquot, M. (2019). The phraseological dimension in interlanguage complexity research. Second Language Research, 35(1), 121‚Äì145. https://doi.org/10.1177/0267658317694221\nEguchi, M., & Kyle, K. (2020). Continuing to Explore the Multidimensional Nature of Lexical Sophistication: The Case of Oral Proficiency Interviews. The Modern Language Journal, 104(2), 381‚Äì400. https://doi.org/10.1111/modl.12637\nKyle, K., & Eguchi, M. (2021). Automatically assessing lexical sophistication using word, bigram, and dependency indices. In Perspectives on the L2 phrasicon: The view from learner corpora (pp.¬†126‚Äì151).\nSiyanova‚ÄêChanturia, A., & Spina, S. (2020). Multi‚ÄêWord Expressions in Second Language Writing: A Large‚ÄêScale Longitudinal Learner Corpus Study. Language Learning, 70(2), 420‚Äì463. https://doi.org/10.1111/lang.12383"
  },
  {
    "objectID": "2025/slides/session-7.html#summary-1",
    "href": "2025/slides/session-7.html#summary-1",
    "title": "Session 7: Multiword Units",
    "section": "Summary",
    "text": "Summary\n\nCorpus-based approach can look at recurrence and co-occurrence\nA central methods to the recurrence is n-gram search\nFor co-occurence, we have window-based (surface) and dependency-based (syntactic) collocations.\nSOA allows you to quantify how strongly associated two words are"
  },
  {
    "objectID": "2025/slides/session-7.html#what-questions-do-you-have",
    "href": "2025/slides/session-7.html#what-questions-do-you-have",
    "title": "Session 7: Multiword Units",
    "section": "What questions do you have?",
    "text": "What questions do you have?"
  },
  {
    "objectID": "2025/syllabus/schedule.html",
    "href": "2025/syllabus/schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "This course covers foundational concepts in corpus linguistics, corpus analysis methods, and their research applications across the following four areas: vocabulary, multiword units, and grammar.\n\n\n\nDay\nSession No.\nSession title\n\n\n\n\nDay 1 (Aug.¬†2nd, Sat)\n\nIntroduction to Linguistic Data Analysis\n\n\n\nSession 1\nGetting Started with Corpus Linguistics\n\n\n\nSession 2\nFoundations of Corpus Linguistics\n\n\n\nSession 3\nBasic Corpus Search\n\n\nDay 2 (Aug.¬†4th, Mon)\n\nAnalyzing Vocabulary\n\n\n\nSession 4\nConceptual overview\n\n\n\nSession 5\nFrequency lists & Lexical profiling\n\n\n\nSession 6\nLexical diversity & Sophistication\n\n\nDay 3 (Aug.¬†5th, Tue)\n\nAnalyzing Multiword Units\n\n\n\nSession 7\nConceptual overview\n\n\n\nSession 8\nCollocations & N-grams\n\n\n\nSession 9\nMini-research & Final project overview\n\n\nDay 4 (Aug.¬†6th, Wed)\n\nAnalyzing Grammar\n\n\n\nSession 10\nConceptual overview\n\n\n\nSession 11\nPOS-tagging and Dependency Parsing\n\n\n\nSession 12\nSyntactic Complexity\n\n\nDay 5 (Aug.¬†7th, Thu)\n\nAdvanced Topics & Wrap-up\n\n\n\nSession 13\nUsing large language models for language annotation\n\n\n\nSession 14\nFinal project preparation time\n\n\n\nSession 15\nFinal project presentation",
    "crumbs": [
      "Syllabus",
      "Course Schedule"
    ]
  },
  {
    "objectID": "2025/syllabus/schedule.html#overview",
    "href": "2025/syllabus/schedule.html#overview",
    "title": "Course Schedule",
    "section": "",
    "text": "This course covers foundational concepts in corpus linguistics, corpus analysis methods, and their research applications across the following four areas: vocabulary, multiword units, and grammar.\n\n\n\nDay\nSession No.\nSession title\n\n\n\n\nDay 1 (Aug.¬†2nd, Sat)\n\nIntroduction to Linguistic Data Analysis\n\n\n\nSession 1\nGetting Started with Corpus Linguistics\n\n\n\nSession 2\nFoundations of Corpus Linguistics\n\n\n\nSession 3\nBasic Corpus Search\n\n\nDay 2 (Aug.¬†4th, Mon)\n\nAnalyzing Vocabulary\n\n\n\nSession 4\nConceptual overview\n\n\n\nSession 5\nFrequency lists & Lexical profiling\n\n\n\nSession 6\nLexical diversity & Sophistication\n\n\nDay 3 (Aug.¬†5th, Tue)\n\nAnalyzing Multiword Units\n\n\n\nSession 7\nConceptual overview\n\n\n\nSession 8\nCollocations & N-grams\n\n\n\nSession 9\nMini-research & Final project overview\n\n\nDay 4 (Aug.¬†6th, Wed)\n\nAnalyzing Grammar\n\n\n\nSession 10\nConceptual overview\n\n\n\nSession 11\nPOS-tagging and Dependency Parsing\n\n\n\nSession 12\nSyntactic Complexity\n\n\nDay 5 (Aug.¬†7th, Thu)\n\nAdvanced Topics & Wrap-up\n\n\n\nSession 13\nUsing large language models for language annotation\n\n\n\nSession 14\nFinal project preparation time\n\n\n\nSession 15\nFinal project presentation",
    "crumbs": [
      "Syllabus",
      "Course Schedule"
    ]
  },
  {
    "objectID": "2025/syllabus/schedule.html#important-notes",
    "href": "2025/syllabus/schedule.html#important-notes",
    "title": "Course Schedule",
    "section": "Important Notes",
    "text": "Important Notes\n\nAll times are Japan Standard Time (JST)\nBring your laptop to all sessions\nComplete readings before each day",
    "crumbs": [
      "Syllabus",
      "Course Schedule"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html",
    "href": "2025/sessions/day4/session10.html",
    "title": "Session 10",
    "section": "",
    "text": "We will go over how people have measured grammar and what are the current states in grammatical complexity research.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#one-liner",
    "href": "2025/sessions/day4/session10.html#one-liner",
    "title": "Session 10",
    "section": "",
    "text": "We will go over how people have measured grammar and what are the current states in grammatical complexity research.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#learning-objectives",
    "href": "2025/sessions/day4/session10.html#learning-objectives",
    "title": "Session 10",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nProvide historical overview of the syntactic complexity research\nDescribe different approaches to grammatical features:\n\nGrammatical complexity strand\nFine-grained grammatical complexity strand\nDescriptive (register-based analysis) strand\nVerb Argument Construction (VAC) strand\n\nUnderstand current trends of syntactic complexity research",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#key-concepts",
    "href": "2025/sessions/day4/session10.html#key-concepts",
    "title": "Session 10",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nGrammatical complexity\nPredictive measures versus Descriptive measures",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#required-readings",
    "href": "2025/sessions/day4/session10.html#required-readings",
    "title": "Session 10",
    "section": "üìö Required Readings",
    "text": "üìö Required Readings\n\nDurrant Ch. 5.\nKyle, K., & Crossley, S. A. (2018). Measuring Syntactic Complexity in L2 Writing Using Fine‚ÄêGrained Clausal and Phrasal Indices. The Modern Language Journal, 102(2), 333‚Äì349. https://doi.org/10.1111/modl.12468",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day4/session10.html#dive-deeper---recommended-readings",
    "title": "Session 10",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings\n\nBiber, D., Gray, B., Staples, S., & Egbert, J. (2020). Investigating grammatical complexity in L2 English writing research: Linguistic description versus predictive measurement. Journal of English for Academic Purposes, 46, 100869. https://doi.org/10.1016/j.jeap.2020.100869\nBiber, D., Gray, B., & Poonpon, K. (2011). Should We Use Characteristics of Conversation to Measure Grammatical Complexity in L2 Writing Development? TESOL Quarterly, 45(1), 5‚Äì35. https://doi.org/10.5054/tq.2011.244483\nNorris, J. M., & Ortega, L. (2009). Towards an Organic Approach to Investigating CAF in Instructed SLA: The Case of Complexity. Applied Linguistics, 30(4), 555‚Äì578. https://doi.org/10.1093/applin/amp044\nLu, X. (2011). A Corpus‚ÄêBased Evaluation of Syntactic Complexity Measures as Indices of College‚ÄêLevel ESL Writers‚Äô Language Development. TESOL Quarterly, 45(1), 36‚Äì62. https://doi.org/10.5054/tq.2011.240859",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#materials",
    "href": "2025/sessions/day4/session10.html#materials",
    "title": "Session 10",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session10.html#reflection",
    "href": "2025/sessions/day4/session10.html#reflection",
    "title": "Session 10",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now:\n\nDescribe classic linguistic complexity measures\nDescribe how",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 10"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html",
    "href": "2025/sessions/day4/session12.html",
    "title": "Session 12",
    "section": "",
    "text": "You will identify fine-grained grammatical features using dependency parser in a small-scale corpus.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#one-liner",
    "href": "2025/sessions/day4/session12.html#one-liner",
    "title": "Session 12",
    "section": "",
    "text": "You will identify fine-grained grammatical features using dependency parser in a small-scale corpus.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#learning-objectives",
    "href": "2025/sessions/day4/session12.html#learning-objectives",
    "title": "Session 12",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nConduct linguistic complexity analysis using a template Python code provided by the instructor.\n(Optional) Apply the concept of linguistic complexity to the Japanese language.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#required-readings",
    "href": "2025/sessions/day4/session12.html#required-readings",
    "title": "Session 12",
    "section": "üìö Required Readings",
    "text": "üìö Required Readings\n\nReread Kyle & Crossley (2018) again.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day4/session12.html#dive-deeper---recommended-readings",
    "title": "Session 12",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings\n\nKyle, K., & Crossley, S. (2017). Assessing syntactic sophistication in L2 writing: A usage-based approach. Language Testing, 34(4), 513‚Äì535. https://doi.org/10.1177/0265532217712554\nKyle, K., Choe, A. T., Eguchi, M., LaFlair, G., & Ziegler, N. (2021). A Comparison of Spoken and Written Language Use in Traditional and Technology‚ÄêMediated Learning Environments. ETS Research Report Series, 2021(1), 1‚Äì29. https://doi.org/10.1002/ets2.12329\nKim, S., Williams, P., & McCallum, L. (2024). Modelling the use of the tool for the automatic analysis of syntactic sophistication and complexity (TAASSC). Research Methods in Applied Linguistics, 3(1), 100087. https://doi.org/10.1016/j.rmal.2023.100087",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#tools-used",
    "href": "2025/sessions/day4/session12.html#tools-used",
    "title": "Session 12",
    "section": "üõ†Ô∏è Tools Used",
    "text": "üõ†Ô∏è Tools Used\n\nTagAnt\nSimple Text Analyzer: A web app created for you.",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#materials",
    "href": "2025/sessions/day4/session12.html#materials",
    "title": "Session 12",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day4/session12.html#reflection",
    "href": "2025/sessions/day4/session12.html#reflection",
    "title": "Session 12",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 4",
      "Session 12"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html",
    "href": "2025/sessions/day3/index.html",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "",
    "text": "Day 3 explores multiword units, collocations, and statistical measures for analyzing word combinations in corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#overview",
    "href": "2025/sessions/day3/index.html#overview",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "",
    "text": "Day 3 explores multiword units, collocations, and statistical measures for analyzing word combinations in corpus linguistics.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#key-concepts",
    "href": "2025/sessions/day3/index.html#key-concepts",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nTypes of multiword units (collocation, n-grams, lexical bundles)\nAssociation strengths (t-score, Mutual Information, LogDice)\nContext window vs dependency bigram approaches\nn-gram search and window-based collocation search\nLinear regression analysis for corpus data",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#preparation",
    "href": "2025/sessions/day3/index.html#preparation",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 3:\n\nRead:\n\nDurrant (2023) Ch. 7\nGablasova, D., Brezina, V., & McEnery, T. (2017). Collocations in Corpus‚ÄêBased Language Learning Research. Language Learning, 67(S1), 155‚Äì179.\n\nSkim:\n\nDurrant (2023) Ch. 8 (Ignore R codes if you are not familiar)\nEguchi & Kyle (2020) - review if needed",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#schedule",
    "href": "2025/sessions/day3/index.html#schedule",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 7: Multiword Units ‚Äî Conceptual Overview\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 8: Hands-on Collocation Analysis\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 9: Learner Corpus Mini-Research\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#assignments",
    "href": "2025/sessions/day3/index.html#assignments",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Assignments",
    "text": "Assignments\n\nDue 8/7 (Thu): Corpus Lab Assignment 3\nPrepare mini-project research topic and questions for presentation",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/index.html#reflection",
    "href": "2025/sessions/day3/index.html#reflection",
    "title": "Day 3: Multiword Units and Collocations",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Day 3: Multiword Units and Collocations"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html",
    "href": "2025/sessions/day3/session8.html",
    "title": "Session 8",
    "section": "",
    "text": "You will learn how to calculate commonly used Strengths Of Association (SOA) measures.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#one-liner",
    "href": "2025/sessions/day3/session8.html#one-liner",
    "title": "Session 8",
    "section": "",
    "text": "You will learn how to calculate commonly used Strengths Of Association (SOA) measures.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#learning-objectives",
    "href": "2025/sessions/day3/session8.html#learning-objectives",
    "title": "Session 8",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nSearch for window-based collocations and n-grams in AntConc\nCalculate commonly used strengths of association measures by hand using spreadsheet software\nDiscuss benefits and drawbacks of different strength of association measures",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#key-concepts",
    "href": "2025/sessions/day3/session8.html#key-concepts",
    "title": "Session 8",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nn-gram search\nWindow-based collocation search\nStrengths of Association measures ‚Äî T-score, Mutual Information, LogDice",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#required-readings",
    "href": "2025/sessions/day3/session8.html#required-readings",
    "title": "Session 8",
    "section": "üìö Required Readings",
    "text": "üìö Required Readings\n\n(Skim) Durrant (2023) Ch. 8 (Ignore R codes if you are not familiar)\nStephanie Evert‚Äôs website on collocation measures\n\nThis webpage provides formulas to calculate various Strengths Of Association measures.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day3/session8.html#dive-deeper---recommended-readings",
    "title": "Session 8",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#materials",
    "href": "2025/sessions/day3/session8.html#materials",
    "title": "Session 8",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\nüìä View Interactive Slides",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day3/session8.html#reflection",
    "href": "2025/sessions/day3/session8.html#reflection",
    "title": "Session 8",
    "section": "Reflection",
    "text": "Reflection\n\nYou can now do the followings:\n\nGenerate lists of n-grams using AntConc.\nSearch for collocates with AntConc.\nCalculate major Strengths of Association (SOA) measures by hand.",
    "crumbs": [
      "Sessions",
      "Day 3",
      "Session 8"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html",
    "href": "2025/sessions/day2/session5.html",
    "title": "Session 5",
    "section": "",
    "text": "You will learn how to create frequency-lists for English and Japanese.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#one-liner",
    "href": "2025/sessions/day2/session5.html#one-liner",
    "title": "Session 5",
    "section": "",
    "text": "You will learn how to create frequency-lists for English and Japanese.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#learning-objectives",
    "href": "2025/sessions/day2/session5.html#learning-objectives",
    "title": "Session 5",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\nBy the end of this session, you will be able to:\n\n\nCompute frequency of a single-word lexical item in reference corpora\nDerive vocabulary frequency list using concordancing software (e.g., AntConc)\nApply tokenization on the Japanese language corpus for frequency analysis\nConduct Lexical Profiling using a web-application or desktop application (e.g., AntWordProfiler)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#key-concepts",
    "href": "2025/sessions/day2/session5.html#key-concepts",
    "title": "Session 5",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nLexical profiling\nFrequency Lists\nZipf‚Äôs law\nLexical coverage",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day2/session5.html#dive-deeper---recommended-readings",
    "title": "Session 5",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#tools-used",
    "href": "2025/sessions/day2/session5.html#tools-used",
    "title": "Session 5",
    "section": "üõ†Ô∏è Tools Used",
    "text": "üõ†Ô∏è Tools Used\n\nAntConc\nAntWordProfiler\nNew Word Levels Checker\nLexTutor",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#materials",
    "href": "2025/sessions/day2/session5.html#materials",
    "title": "Session 5",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\nüìä View Interactive Slides",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#reflection",
    "href": "2025/sessions/day2/session5.html#reflection",
    "title": "Session 5",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#frequency-list",
    "href": "2025/sessions/day2/session5.html#frequency-list",
    "title": "Session 5",
    "section": "Frequency list",
    "text": "Frequency list",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#lexical-profiling",
    "href": "2025/sessions/day2/session5.html#lexical-profiling",
    "title": "Session 5",
    "section": "Lexical Profiling",
    "text": "Lexical Profiling",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/session5.html#keyness-analysis",
    "href": "2025/sessions/day2/session5.html#keyness-analysis",
    "title": "Session 5",
    "section": "Keyness Analysis",
    "text": "Keyness Analysis",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Session 5"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html",
    "href": "2025/sessions/day2/index.html",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "",
    "text": "Day 2 focuses on analyzing vocabulary in corpus linguistics, introducing concepts of lexical richness, particularly diversity and sophistication.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#overview",
    "href": "2025/sessions/day2/index.html#overview",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "",
    "text": "Day 2 focuses on analyzing vocabulary in corpus linguistics, introducing concepts of lexical richness, particularly diversity and sophistication.",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#key-concepts",
    "href": "2025/sessions/day2/index.html#key-concepts",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nLexical Richness (text internal vs external measures)\nLexical Diversity (Type-Token Ratio, MTLD)\nLexical Sophistication (frequency, concreteness, phonological neighbors)\nLexical profiling\nFrequency Lists and Zipf law",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#preparation",
    "href": "2025/sessions/day2/index.html#preparation",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 2:\n\nRead:\n\nDurrant Ch. 3\n\nSkim:\n\nDurrant Ch. 4 (Ignore R codes if you are not familiar)\nEguchi, M., & Kyle, K. (2020). Continuing to Explore the Multidimensional Nature of Lexical Sophistication. The Modern Language Journal, 104(2), 381‚Äì400.\n\nWatch:\n\nLaurence Anthony‚Äôs intro to AntConc\n\nGetting started (10 mins)\nCorpus manager Basics (18 mins) \nWord list tool basics (7 mins)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#schedule",
    "href": "2025/sessions/day2/index.html#schedule",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 4: Analyzing vocabulary (1) ‚Äî Conceptual overview\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 5: Frequency Analysis and Lexical Profiling\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 6: Computing Lexical Measures\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#assignments",
    "href": "2025/sessions/day2/index.html#assignments",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Assignments",
    "text": "Assignments\n\nDue 8/6 (Wed): Corpus Lab Assignment 2\nComplete lexical analysis exercises using AntConc and web applications",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day2/index.html#reflection",
    "href": "2025/sessions/day2/index.html#reflection",
    "title": "Day 2: Analyzing Vocabulary",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 2",
      "Day 2: Analyzing Vocabulary"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session14.html",
    "href": "2025/sessions/day5/session14.html",
    "title": "Session 14",
    "section": "",
    "text": "Group project time. Please use the time wisely.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 14"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session14.html#one-liner",
    "href": "2025/sessions/day5/session14.html#one-liner",
    "title": "Session 14",
    "section": "",
    "text": "Group project time. Please use the time wisely.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 14"
    ]
  },
  {
    "objectID": "2025/sessions/day5/session14.html#reflection",
    "href": "2025/sessions/day5/session14.html#reflection",
    "title": "Session 14",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Session 14"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html",
    "href": "2025/sessions/day5/index.html",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "",
    "text": "Day 5 explores cutting-edge applications of Large Language Models in corpus linguistics and provides dedicated time for final project development.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#overview",
    "href": "2025/sessions/day5/index.html#overview",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "",
    "text": "Day 5 explores cutting-edge applications of Large Language Models in corpus linguistics and provides dedicated time for final project development.",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#key-concepts",
    "href": "2025/sessions/day5/index.html#key-concepts",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nLarge Language Models (LLMs) and Language Generation\nPrompt engineering\nFine-tuning\nLLM-assisted linguistic annotation\nResearch design and methodology\nProject presentation skills",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#preparation",
    "href": "2025/sessions/day5/index.html#preparation",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Preparation",
    "text": "Preparation\nBefore Day 5:\n\nRead:\n\nMizumoto, A., Shintani, N., Sasaki, M., & Teng, M. F. (2024). Testing the viability of ChatGPT as a companion in L2 writing accuracy assessment. Research Methods in Applied Linguistics, 3(2), 100116.\n\nSkim:\n\nKim, M., & Lu, X. (2024). Exploring the potential of using ChatGPT for rhetorical move-step analysis. Journal of English for Academic Purposes, 71, 101422.\n\nDownload\n\nFCE dataset\n\n\nWe will also use source codes from Mizumoto et al.¬†(2024)",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#schedule",
    "href": "2025/sessions/day5/index.html#schedule",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\n10:30-12:00\nSession 13: LLMs in Linguistic Analysis\n\n\n12:00-13:00\nLunch\n\n\n13:00-14:30\nSession 14: Group Project Time\n\n\n14:30-14:40\nBreak\n\n\n14:40-16:10\nSession 15: Project Presentations and Wrap-up\n\n\n16:10-17:00\nOffice Hour (You can ask questions.)",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#assignments",
    "href": "2025/sessions/day5/index.html#assignments",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Assignments",
    "text": "Assignments\n\nFinal Project: Final Project Guidelines\nGroup presentations today\nFinal submission deadline: [Check syllabus]",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day5/index.html#reflection",
    "href": "2025/sessions/day5/index.html#reflection",
    "title": "Day 5: Advanced Topics and Final Project",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 5",
      "Day 5: Advanced Topics and Final Project"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#learning-objectives",
    "href": "2025/sessions/day1/session1.html#learning-objectives",
    "title": "Session 1",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nOverview the content of the current course\nExplain key success criteria in this course\nConduct the very first corpus search\nExplain different types of corpus linguistic analysis for different focus:\n\nfrequency analysis,\nconcordance analysis,\ncollocation analysis,\nPart-Of-Speech Tagging, etc.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#key-concepts",
    "href": "2025/sessions/day1/session1.html#key-concepts",
    "title": "Session 1",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nConceptual Overview of the corpus linguistic methods\n\nFrequency\nConcordance\nCollocation analysis\nPart-Of-Speech Tagging\nDependency Parsing",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#required-readings",
    "href": "2025/sessions/day1/session1.html#required-readings",
    "title": "Session 1",
    "section": "üìö Required Readings",
    "text": "üìö Required Readings\n\n(Skim) Davies (2015) Available through the shared drive.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#notes",
    "href": "2025/sessions/day1/session1.html#notes",
    "title": "Session 1",
    "section": "üìù Notes",
    "text": "üìù Notes\nNeeds analysis (After giving overview) is conducted at the end of this session.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#materials",
    "href": "2025/sessions/day1/session1.html#materials",
    "title": "Session 1",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\nüìä View Interactive Slides",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session1.html#reflection",
    "href": "2025/sessions/day1/session1.html#reflection",
    "title": "Session 1",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 1"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html",
    "href": "2025/sessions/day1/session3.html",
    "title": "Session 3",
    "section": "",
    "text": "You will learn how to conduct basic corpus searches.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#one-liner",
    "href": "2025/sessions/day1/session3.html#one-liner",
    "title": "Session 3",
    "section": "",
    "text": "You will learn how to conduct basic corpus searches.",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#learning-objectives",
    "href": "2025/sessions/day1/session3.html#learning-objectives",
    "title": "Session 3",
    "section": "üéØ Learning Objectives",
    "text": "üéØ Learning Objectives\n\nBy the end of this session, students will be able to:\n\nConduct KWIC searches on English-Corpora.org\nSort KWIC search results to obtain qualitative observation about language use\nUse advanced search strings such as regular expression to fine-tune the search results",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#key-concepts",
    "href": "2025/sessions/day1/session3.html#key-concepts",
    "title": "Session 3",
    "section": "üîë Key Concepts",
    "text": "üîë Key Concepts\n\nKey Words In Context (KWIC)\nLexical Counting unit:\n\nToken\nLemma\nType\n\nRegular Expressions",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#tools-used",
    "href": "2025/sessions/day1/session3.html#tools-used",
    "title": "Session 3",
    "section": "üõ†Ô∏è Tools Used",
    "text": "üõ†Ô∏è Tools Used\n\nEnglish-Corpora.org\nAntConc",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#dive-deeper---recommended-readings",
    "href": "2025/sessions/day1/session3.html#dive-deeper---recommended-readings",
    "title": "Session 3",
    "section": "üåä Dive Deeper - Recommended Readings",
    "text": "üåä Dive Deeper - Recommended Readings",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#materials",
    "href": "2025/sessions/day1/session3.html#materials",
    "title": "Session 3",
    "section": "Materials",
    "text": "Materials\n\nSlides for the session\n\nView Interactive Slides",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/sessions/day1/session3.html#reflection",
    "href": "2025/sessions/day1/session3.html#reflection",
    "title": "Session 3",
    "section": "Reflection",
    "text": "Reflection",
    "crumbs": [
      "Sessions",
      "Day 1",
      "Session 3"
    ]
  },
  {
    "objectID": "2025/notebooks/session-11.html",
    "href": "2025/notebooks/session-11.html",
    "title": "First dependency analysis with Python",
    "section": "",
    "text": "In this notebook, I will show you how to run a simple dependency analysis with spaCy package in Python.\nFirst in Python, you will import necessary package.\nIn the following, I import a package called spacy.\nAfter importing it, I will then load English analysis model.\nShow code\n#importing spacy package\nimport spacy\n\n#loading en_core_web_sm model for English analysis\nnlp = spacy.load(\"en_core_web_sm\")\nGreat! You have now English analysis model loaded on the system runtime.\nNext, let‚Äôs conduct first parsing.\nWe will first define example sentence.\nShow code\n# Setting the example sentence to `example_text`.\nexample_text = \"Hi. This is my first awesome sentence to analyze.\"\nOkay. Now you have the example sentence. You can verify this by running the following print(example_text).\nShow code\nprint(example_text)\nAwesome! we have confirmed the example_text.\nNow, we will pass this example sentence to the spacy model and parse the sentence.\nThe result is then stored in an object doc.\nShow code\n# this line parses the example sentence.\ndoc = nlp(example_text)\nAlright then. We have parsed the first example sentence! Let us verify the parse result next."
  },
  {
    "objectID": "2025/notebooks/session-11.html#printing-the-result-of-analysis.",
    "href": "2025/notebooks/session-11.html#printing-the-result-of-analysis.",
    "title": "First dependency analysis with Python",
    "section": "Printing the result of analysis.",
    "text": "Printing the result of analysis.\nNow that we have parsed sentence called doc let‚Äôs verify the content.\ndoc contains multiple tokens. this token object has parsed information.\nWe can iterate through the doc to return token.\n\n\nShow code\nfor token in doc: #iterate through doc\n    print(token) # print token\n\n\nOkay‚Ä¶? we just print it vertical?"
  },
  {
    "objectID": "2025/notebooks/session-11.html#print-lemmatized-form",
    "href": "2025/notebooks/session-11.html#print-lemmatized-form",
    "title": "First dependency analysis with Python",
    "section": "Print lemmatized form",
    "text": "Print lemmatized form\nLet‚Äôs do more.\nYou can print lemmatized form by token.lemma_ (do not forget _ at the end.)\n\n\nShow code\n# Print lemmatized form next to text.\nfor token in doc:\n    print(token.text, token.lemma_, sep=\"\\t\")"
  },
  {
    "objectID": "2025/notebooks/session-11.html#print-part-of-speech-information",
    "href": "2025/notebooks/session-11.html#print-part-of-speech-information",
    "title": "First dependency analysis with Python",
    "section": "Print Part of Speech information",
    "text": "Print Part of Speech information\nYou can add more information, such as pos_\n\n\nShow code\nfor token in doc:\n    print(token.text, token.pos_, sep=\"\\t\")"
  },
  {
    "objectID": "2025/notebooks/session-11.html#your-turn-print-lemma-pos-and-tag-next-to-token.",
    "href": "2025/notebooks/session-11.html#your-turn-print-lemma-pos-and-tag-next-to-token.",
    "title": "First dependency analysis with Python",
    "section": "Your turn: Print lemma, pos and tag next to token.",
    "text": "Your turn: Print lemma, pos and tag next to token."
  },
  {
    "objectID": "2025/notebooks/session-11.html#spacy-token-information",
    "href": "2025/notebooks/session-11.html#spacy-token-information",
    "title": "First dependency analysis with Python",
    "section": "spaCy token information",
    "text": "spaCy token information\nSome useful token information are following:\n\n\n\ncode\nwhat it does\nexample\n\n\n\n\ntoken.lemma_\nlemmatized form\nbe, child\n\n\ntoken.pos_\nsimple POS (Universal Dependency)\nNOUN, VERB\n\n\ntoken.tag_\nfine-grained POS (PennTag set)\nNN, JJ, VB, BBZ\n\n\ntoken.dep_\ndependency type\namod, advmd\n\n\ntoken.head\ntoken information of the head of the dependency\n\n\n\n\n\n\nShow code\nfor token in doc:\n    pass # replace this with correct print statement."
  },
  {
    "objectID": "2025/notebooks/session-12.html",
    "href": "2025/notebooks/session-12.html",
    "title": "Extracting fine-grained grammatical feature with POS and dependency information",
    "section": "",
    "text": "This notebook walk you through parsing and identifying grammatical constructions using POS and dependency information. Most of the functionality (except the actual extraction rules) is already coded for you. This notebook does not assume prior knowledge in Python, but requires your understanding of the course materials (particularly POS and dependency labels and how grammatical structure is operationalized.)\nNow let me explain the approach we will take in this demo."
  },
  {
    "objectID": "2025/notebooks/session-12.html#algorithm-used-in-this-notebook.",
    "href": "2025/notebooks/session-12.html#algorithm-used-in-this-notebook.",
    "title": "Extracting fine-grained grammatical feature with POS and dependency information",
    "section": "Algorithm used in this notebook.",
    "text": "Algorithm used in this notebook.\nIn this notebook, the following analysis pipeline is implemented for you.\n\nYour input is file path to yout corpus files.\nThe current code loads the corpus files onto colab.\nIt then iterate through the corpus files one by one to do the following:\n\nParse the sentence using spacy\nConduct basic analysis (such as calculating the number of tokens, sentences, etc.)\nCount the number of specific grammatical structures (MAIN FEATURE)\nStore the results into a Python dictionary\n\nAfter every corpus file is processed, it can create a dataset to export.\nYou can export the results for further analysis"
  },
  {
    "objectID": "2025/notebooks/session-12.html#what-you-are-expected-to-do",
    "href": "2025/notebooks/session-12.html#what-you-are-expected-to-do",
    "title": "Extracting fine-grained grammatical feature with POS and dependency information",
    "section": "What you are expected to do",
    "text": "What you are expected to do\nYou are expected to do the following:\n\nArticulate extraction rules that can be used to identify desired grammatical structure\nIdentify which extraction template functions to use in order to extract the grammatical structure\nFollow the instruction of the current notebook to run the analysis."
  },
  {
    "objectID": "2025/notebooks/session-12.html#define-functions",
    "href": "2025/notebooks/session-12.html#define-functions",
    "title": "Extracting fine-grained grammatical feature with POS and dependency information",
    "section": "Define functions",
    "text": "Define functions\nIn the following I will define necessary functions for this pipeline to work. Let me know if you are curious to know what each does\n\n\nShow code\ndef load_file(filepath: str):\n    \"\"\"Load text from file\"\"\"\n    with open(filepath, 'r', encoding='utf-8') as f:\n        return f.read()\n\n\ndef find_filename(filename: str):\n    new_name = os.path.split(filename)[-1]\n    return new_name\n\n\ndef update_results(index_name: str, result_dictionary):\n    if index_name in result_dictionary:\n        result_dictionary[index_name] += 1\n    else:\n        result_dictionary[index_name] = 1\n\n\n\n\nShow code\ndef run_basic_stats(doc):\n    basic_stats = {}\n    \n    basic_stats[\"nToken\"] = len(doc)\n    basic_stats[\"nSentence\"] = len(list(doc.sents))\n    return basic_stats"
  },
  {
    "objectID": "2025/notebooks/session-12.html#change-the-rule-in-the-run_extraction-below",
    "href": "2025/notebooks/session-12.html#change-the-rule-in-the-run_extraction-below",
    "title": "Extracting fine-grained grammatical feature with POS and dependency information",
    "section": "Change the rule in the run_extraction below!",
    "text": "Change the rule in the run_extraction below!\n\n\nShow code\ndef run_grammar_extraction(doc, filepath: str):\n    global extraction_results\n    extraction_results = {}\n\n    for token in doc:\n        # Extract simple dependency-based features\n        extract_by_simple_dependency(extraction_results, token, dep_rel=\"amod\", index_name=\"adjectival_modifier\")\n        extract_by_simple_dependency(extraction_results, token, dep_rel=\"dobj\", index_name=\"direct_object\")\n        ## Add more rule here\n\n        # Extract POS-based features\n        extract_by_pos(extraction_results, token, pos= \"NOUN\", index_name= \"Nouns\")\n        ## Add more rule here\n\n        # Extract tag-based features\n        extract_by_tag(extraction_results, token, tag=\"VBZ\", index_name=\"Third-Person Singular Verbs\")\n        ## Add more rule here\n\n\n    return extraction_results"
  },
  {
    "objectID": "2025/notebooks/session-12.html#wanna-test",
    "href": "2025/notebooks/session-12.html#wanna-test",
    "title": "Extracting fine-grained grammatical feature with POS and dependency information",
    "section": "Wanna test?",
    "text": "Wanna test?\nYou can test if you will get the desired result by passing example sentence and your rules.\n\nRun the main analysis loop\nThis where the processing happens.\n\n\nShow code\n# Main processing loop\ndef main(CORPUS_FILES):\n    results = {}\n\n    for file in CORPUS_FILES:  # Process first 5 files for testing\n        global extraction_results\n        # 1. Load the corpus file\n        text = load_file(file)\n        filename = find_filename(file)\n        \n        # 2. Parse the sentences\n        doc = nlp(text)\n        \n        # 3. run extraction pipeline\n        basic_stats = run_basic_stats(doc)\n        result = run_grammar_extraction(doc, filename)\n        \n        # 4. Append results\n        results[filename] = basic_stats\n        results[filename].update(result)\n        \n        print(f\"Processed: {file}\")\n    return results\n\n\n\n\nShow code\nCORPUS_FILES = [\"../../corpus_data/brown_single/ca_ca01.txt\"]\nresults = main(CORPUS_FILES)\n\n\nProcessed: ../../corpus_data/brown_single/ca_ca01.txt\n\n\nCheck the results\n\n\nShow code\nresults\n\n\n{'ca_ca01.txt': {'nToken': 2376,\n  'nSentence': 88,\n  'Nouns': 458,\n  'adjectival_modifier': 112,\n  'direct_object': 95,\n  'Third-Person Singular Verbs': 34}}"
  },
  {
    "objectID": "2025/notebooks/session-12.html#write-the-results-into-dataset",
    "href": "2025/notebooks/session-12.html#write-the-results-into-dataset",
    "title": "Extracting fine-grained grammatical feature with POS and dependency information",
    "section": "Write the results into dataset",
    "text": "Write the results into dataset\n\n\nShow code\ndef results_to_dataframe(results):\n    \"\"\"\n    Convert results dictionary to pandas DataFrame with additional options\n    \n    Parameters:\n    - results: dictionary with corpus_size, unigram, and bigram data\n    - min_freq: minimum collocation frequency to include (default: 1)\n    - include_dep_rel: whether to include dependency relation in output (default: True)\n    \"\"\"\n    rows = []\n    \n    for filename, grammar_info in results.items():\n            \n        row = {\n            \"filename\": filename,\n              }\n        \n        for key, value in grammar_info.items():\n            print(key, value)\n            row.update({key:value})\n        \n        rows.append(row)\n    \n    # Create DataFrame and sort by collocation frequency\n    df = pd.DataFrame(rows)\n    \n    return df\n\n\n\n\nShow code\ndf = results_to_dataframe(results)\ndf\n\n\nnToken 2376\nnSentence 88\nNouns 458\nadjectival_modifier 112\ndirect_object 95\nThird-Person Singular Verbs 34\n\n\n\n\n\n\n\n\n\nfilename\nnToken\nnSentence\nNouns\nadjectival_modifier\ndirect_object\nThird-Person Singular Verbs\n\n\n\n\n0\nca_ca01.txt\n2376\n88\n458\n112\n95\n34"
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html",
    "href": "2025/notebooks/japanese-nlp-test.html",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "",
    "text": "This notebook implements and compares two approaches for Japanese morphological analysis with BCCWJ frequency matching:\nEach approach is designed for reproducible setup, implementation, validation, and operational use."
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html#environment-setup-verification",
    "href": "2025/notebooks/japanese-nlp-test.html#environment-setup-verification",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "1. Environment Setup & Verification",
    "text": "1. Environment Setup & Verification\nFirst, let‚Äôs verify and set up our environment with all required packages.\n\n\nShow code\n# Environment verification and setup\nimport sys\nimport subprocess\nfrom pathlib import Path\n\nprint(f\"Python version: {sys.version}\")\nprint(f\"Working directory: {Path.cwd()}\")\n\n# Required packages\nrequired_packages = [\n    'fugashi', 'unidic', 'unidic-lite', 'spacy', 'ginza', \n    'ja-ginza', 'sudachipy', 'pandas', 'numpy', 'matplotlib', 'collections'\n]\n\nprint(\"\\nChecking package availability:\")\nfor package in required_packages:\n    try:\n        if package == 'collections':\n            import collections\n            print(f\"‚úì {package} (built-in)\")\n        else:\n            __import__(package)\n            print(f\"‚úì {package}\")\n    except ImportError:\n        print(f\"‚úó {package} - NOT FOUND\")\n\n\nPython version: 3.12.2 (main, Feb 25 2024, 03:55:42) [Clang 17.0.6 ]\nWorking directory: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/2025/notebooks\n\nChecking package availability:\n‚úì fugashi\n‚úì unidic\n‚úó unidic-lite - NOT FOUND\n‚úì spacy\n‚úì ginza\n‚úó ja-ginza - NOT FOUND\n‚úì sudachipy\n‚úì pandas\n‚úì numpy\n‚úì matplotlib\n‚úì collections (built-in)\n\n\n\n\nShow code\n# Import all necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter, defaultdict\nimport time\nimport warnings\nfrom typing import List, Tuple, Dict, Optional\n\n# Japanese NLP libraries\nimport fugashi\nimport unidic\nimport spacy\nfrom spacy.tokens import Token, Doc\n\n# Statistical analysis\ntry:\n    from scipy.stats import spearmanr\n    scipy_available = True\nexcept ImportError:\n    print(\"scipy not available - will use numpy for correlation\")\n    scipy_available = False\n\nprint(\"All imports successful!\")\nwarnings.filterwarnings('ignore')\n\n\nscipy not available - will use numpy for correlation\nAll imports successful!\n\n\n\n\nShow code\n# Check UniDic installation and download if needed\ntry:\n    print(f\"UniDic directory: {unidic.DICDIR}\")\n    print(\"UniDic is properly installed\")\nexcept Exception as e:\n    print(f\"UniDic issue: {e}\")\n    print(\"You may need to run: python -m unidic download\")\n\n# Test basic fugashi functionality\ntry:\n    tagger = fugashi.Tagger(f'-d \"{unidic.DICDIR}\"')\n    test_result = list(tagger(\"„ÉÜ„Çπ„Éà\"))\n    print(f\"Fugashi + UniDic test successful: {test_result[0].surface}\")\nexcept Exception as e:\n    print(f\"Fugashi test failed: {e}\")\n\n\nUniDic directory: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/unidic/dicdir\nUniDic is properly installed\nFugashi + UniDic test successful: „ÉÜ„Çπ„Éà"
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html#sample-data-preparation",
    "href": "2025/notebooks/japanese-nlp-test.html#sample-data-preparation",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "2. Sample Data Preparation",
    "text": "2. Sample Data Preparation\nLet‚Äôs create realistic Japanese text samples for testing our pipelines.\n\n\nShow code\n# Sample Japanese texts for testing\nsample_texts = [\n    \"ÂΩº„ÅØÊó•„Åî„Çç„Åã„ÇâÊú¨„ÇíË™≠„ÇÄ„ÅÆ„ÅåÂ•Ω„Åç„Åß„Åô„ÄÇ\",\n    \"„Å≤„Åî„Çç„ÅÆÂãâÂº∑„ÅåÂ§ßÂàá„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ\",\n    \"Êó•È†É„ÅÆÂä™Âäõ„ÅåÂÆü„ÇíÁµê„Å∂„Åß„Åó„Çá„ÅÜ„ÄÇ\",\n    \"ÂΩºÂ•≥„ÅØÊõ∏„Åç„ÅÇ„Çâ„Çè„Åô„Åì„Å®„ÅåÂæóÊÑè„Åß„Åô„ÄÇ\",\n    \"„Åù„ÅÆÂïèÈ°å„ÇíÊõ∏„ÅçË°®„Åô„ÅÆ„ÅØÈõ£„Åó„ÅÑ„ÄÇ\",\n    \"‰ªäÊó•„ÅØÊù±‰∫¨„Ç™„É™„É≥„Éî„ÉÉ„ÇØ„Å´„Å§„ÅÑ„Å¶Ë©±„Åó„Åæ„Åó„Çá„ÅÜ„ÄÇ\",\n    \"„Ç≥„Éº„Éí„Éº„ÇíÈ£≤„Çì„Åß„ÄÅÂëë„ÅøËæº„Çì„Åß„ÄÅ„Åæ„ÅüÈ£≤„Çì„Åß„Åó„Åæ„Å£„Åü„ÄÇ\",\n    \"ÂõΩÈöõÁöÑ„Å™ÂçîÂäõ„ÅåÂøÖË¶Å‰∏çÂèØÊ¨†„Åß„Åô„ÄÇ\",\n    \"Ê©üÊ¢∞Â≠¶Áøí„ÅÆÊäÄË°ì„ÅåÈÄ≤Ê≠©„Åó„Å¶„ÅÑ„Çã„ÄÇ\",\n    \"Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅØËààÂë≥Ê∑±„ÅÑÂàÜÈáé„Å†„ÄÇ\"\n]\n\nprint(\"Sample texts prepared:\")\nfor i, text in enumerate(sample_texts, 1):\n    print(f\"{i:2d}. {text}\")\n\n# Create a larger corpus by repeating and slightly modifying texts\nextended_corpus = sample_texts * 3  # Simulate frequency variations\nprint(f\"\\nExtended corpus: {len(extended_corpus)} texts\")\n\n\nSample texts prepared:\n 1. ÂΩº„ÅØÊó•„Åî„Çç„Åã„ÇâÊú¨„ÇíË™≠„ÇÄ„ÅÆ„ÅåÂ•Ω„Åç„Åß„Åô„ÄÇ\n 2. „Å≤„Åî„Çç„ÅÆÂãâÂº∑„ÅåÂ§ßÂàá„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ\n 3. Êó•È†É„ÅÆÂä™Âäõ„ÅåÂÆü„ÇíÁµê„Å∂„Åß„Åó„Çá„ÅÜ„ÄÇ\n 4. ÂΩºÂ•≥„ÅØÊõ∏„Åç„ÅÇ„Çâ„Çè„Åô„Åì„Å®„ÅåÂæóÊÑè„Åß„Åô„ÄÇ\n 5. „Åù„ÅÆÂïèÈ°å„ÇíÊõ∏„ÅçË°®„Åô„ÅÆ„ÅØÈõ£„Åó„ÅÑ„ÄÇ\n 6. ‰ªäÊó•„ÅØÊù±‰∫¨„Ç™„É™„É≥„Éî„ÉÉ„ÇØ„Å´„Å§„ÅÑ„Å¶Ë©±„Åó„Åæ„Åó„Çá„ÅÜ„ÄÇ\n 7. „Ç≥„Éº„Éí„Éº„ÇíÈ£≤„Çì„Åß„ÄÅÂëë„ÅøËæº„Çì„Åß„ÄÅ„Åæ„ÅüÈ£≤„Çì„Åß„Åó„Åæ„Å£„Åü„ÄÇ\n 8. ÂõΩÈöõÁöÑ„Å™ÂçîÂäõ„ÅåÂøÖË¶Å‰∏çÂèØÊ¨†„Åß„Åô„ÄÇ\n 9. Ê©üÊ¢∞Â≠¶Áøí„ÅÆÊäÄË°ì„ÅåÈÄ≤Ê≠©„Åó„Å¶„ÅÑ„Çã„ÄÇ\n10. Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅØËààÂë≥Ê∑±„ÅÑÂàÜÈáé„Å†„ÄÇ\n\nExtended corpus: 30 texts\n\n\n\n\nShow code\n# Create mock BCCWJ frequency data for testing\n# In real usage, this would be loaded from an actual BCCWJ frequency file\n\nmock_bccwj_data = [\n    ('Êó•È†É', '„Éí„Ç¥„É≠', 'ÂêçË©û', 1250),\n    ('Êú¨', '„Éõ„É≥', 'ÂêçË©û', 8500),\n    ('Ë™≠„ÇÄ', '„É®„É†', 'ÂãïË©û', 3200),\n    ('Â•Ω„Åç', '„Çπ„Ç≠', 'ÂΩ¢ÂÆπÂãïË©û', 2100),\n    ('ÂãâÂº∑', '„Éô„É≥„Ç≠„Éß„Ç¶', 'ÂêçË©û', 4200),\n    ('Â§ßÂàá', '„Çø„Ç§„Çª„ÉÑ', 'ÂΩ¢ÂÆπÂãïË©û', 1800),\n    ('ÊÄù„ÅÜ', '„Ç™„É¢„Ç¶', 'ÂãïË©û', 9500),\n    ('Âä™Âäõ', '„Éâ„É™„Éß„ÇØ', 'ÂêçË©û', 2200),\n    ('ÂÆü', '„Éü', 'ÂêçË©û', 1100),\n    ('Áµê„Å∂', '„É†„Çπ„Éñ', 'ÂãïË©û', 800),\n    ('Êõ∏„Åè', '„Ç´„ÇØ', 'ÂãïË©û', 4100),\n    ('Ë°®„Åô', '„Ç¢„É©„ÉØ„Çπ', 'ÂãïË©û', 1500),\n    ('ÂæóÊÑè', '„Éà„ÇØ„Ç§', 'ÂΩ¢ÂÆπÂãïË©û', 1300),\n    ('ÂïèÈ°å', '„É¢„É≥„ÉÄ„Ç§', 'ÂêçË©û', 6200),\n    ('Èõ£„Åó„ÅÑ', '„É†„Ç∫„Ç´„Ç∑„Ç§', 'ÂΩ¢ÂÆπË©û', 3800),\n    ('‰ªäÊó•', '„Ç≠„Éß„Ç¶', 'ÂêçË©û', 5500),\n    ('Êù±‰∫¨', '„Éà„Ç¶„Ç≠„Éß„Ç¶', 'ÂêçË©û', 4800),\n    ('Ë©±„Åô', '„Éè„Éä„Çπ', 'ÂãïË©û', 3600),\n    ('È£≤„ÇÄ', '„Éé„É†', 'ÂãïË©û', 2400),\n    ('Âëë„ÇÄ', '„Éé„É†', 'ÂãïË©û', 150),\n    ('ÂõΩÈöõ', '„Ç≥„ÇØ„Çµ„Ç§', 'ÂêçË©û', 2800),\n    ('ÂçîÂäõ', '„Ç≠„Éß„Ç¶„É™„Éß„ÇØ', 'ÂêçË©û', 1900),\n    ('ÂøÖË¶Å', '„Éí„ÉÑ„É®„Ç¶', 'ÂΩ¢ÂÆπÂãïË©û', 4500),\n    ('ÊäÄË°ì', '„ÇÆ„Ç∏„É•„ÉÑ', 'ÂêçË©û', 3900),\n    ('ÈÄ≤Ê≠©', '„Ç∑„É≥„Éù', 'ÂêçË©û', 1100)\n]\n\n# Create DataFrame\ndf_bccwj = pd.DataFrame(mock_bccwj_data, columns=['lemma', 'reading', 'pos', 'freq_bccwj'])\ndf_bccwj['key'] = list(zip(df_bccwj.lemma, df_bccwj.reading, df_bccwj.pos))\n\nprint(\"Mock BCCWJ frequency data:\")\nprint(df_bccwj.head(10))\nprint(f\"\\nTotal entries: {len(df_bccwj)}\")\n\n\nMock BCCWJ frequency data:\n  lemma reading   pos  freq_bccwj               key\n0    Êó•È†É     „Éí„Ç¥„É≠    ÂêçË©û        1250     (Êó•È†É, „Éí„Ç¥„É≠, ÂêçË©û)\n1     Êú¨      „Éõ„É≥    ÂêçË©û        8500       (Êú¨, „Éõ„É≥, ÂêçË©û)\n2    Ë™≠„ÇÄ      „É®„É†    ÂãïË©û        3200      (Ë™≠„ÇÄ, „É®„É†, ÂãïË©û)\n3    Â•Ω„Åç      „Çπ„Ç≠  ÂΩ¢ÂÆπÂãïË©û        2100    (Â•Ω„Åç, „Çπ„Ç≠, ÂΩ¢ÂÆπÂãïË©û)\n4    ÂãâÂº∑   „Éô„É≥„Ç≠„Éß„Ç¶    ÂêçË©û        4200   (ÂãâÂº∑, „Éô„É≥„Ç≠„Éß„Ç¶, ÂêçË©û)\n5    Â§ßÂàá    „Çø„Ç§„Çª„ÉÑ  ÂΩ¢ÂÆπÂãïË©û        1800  (Â§ßÂàá, „Çø„Ç§„Çª„ÉÑ, ÂΩ¢ÂÆπÂãïË©û)\n6    ÊÄù„ÅÜ     „Ç™„É¢„Ç¶    ÂãïË©û        9500     (ÊÄù„ÅÜ, „Ç™„É¢„Ç¶, ÂãïË©û)\n7    Âä™Âäõ    „Éâ„É™„Éß„ÇØ    ÂêçË©û        2200    (Âä™Âäõ, „Éâ„É™„Éß„ÇØ, ÂêçË©û)\n8     ÂÆü       „Éü    ÂêçË©û        1100        (ÂÆü, „Éü, ÂêçË©û)\n9    Áµê„Å∂     „É†„Çπ„Éñ    ÂãïË©û         800     (Áµê„Å∂, „É†„Çπ„Éñ, ÂãïË©û)\n\nTotal entries: 25"
  },
  {
    "objectID": "2025/notebooks/japanese-nlp-test.html#plan-a-mecab-fugashi-unidic-direct-pipeline",
    "href": "2025/notebooks/japanese-nlp-test.html#plan-a-mecab-fugashi-unidic-direct-pipeline",
    "title": "Japanese NLP Analysis: Comparative Study of UniDic-based Approaches",
    "section": "3. Plan A: MeCab (fugashi) + UniDic Direct Pipeline",
    "text": "3. Plan A: MeCab (fugashi) + UniDic Direct Pipeline\n\nA-1 to A-3: Setup and Configuration\nUniDic provides the morphological analysis system used in BCCWJ, making it ideal for frequency matching.\n\n\nShow code\n# A-3: Initialize fugashi with UniDic\nprint(\"Initializing Plan A: fugashi + UniDic pipeline\")\n\n# Initialize tagger with explicit UniDic path\ntagger_a = fugashi.Tagger(f'-d \"{unidic.DICDIR}\"')\nprint(f\"Tagger initialized with UniDic dictionary: {unidic.DICDIR}\")\n\n# Test the tagger\ntest_text = \"Êó•„Åî„Çç„Åã„ÇâÂãâÂº∑„Åó„Å¶„ÅÑ„Çã„ÄÇ\"\ntokens = list(tagger_a(test_text))\nprint(f\"\\nTest analysis of '{test_text}':\")\nfor token in tokens:\n    print(f\"  {token.surface} -&gt; {token.feature.lemma} [{','.join(token.pos)}]\")\n\n\nInitializing Plan A: fugashi + UniDic pipeline\nTagger initialized with UniDic dictionary: /Users/eguchi/Dropbox/teaching/Tohoku-2025/linguistic-data-analysis-I/.venv/lib/python3.12/site-packages/unidic/dicdir\n\nTest analysis of 'Êó•„Åî„Çç„Åã„ÇâÂãâÂº∑„Åó„Å¶„ÅÑ„Çã„ÄÇ':\n  Êó•„Åî„Çç -&gt; Êó•È†É [Âêç,Ë©û,,,ÊôÆ,ÈÄö,Âêç,Ë©û,,,ÂâØ,Ë©û,ÂèØ,ËÉΩ,,,*]\n  „Åã„Çâ -&gt; „Åã„Çâ [Âä©,Ë©û,,,Ê†º,Âä©,Ë©û,,,*,,,*]\n  ÂãâÂº∑ -&gt; ÂãâÂº∑ [Âêç,Ë©û,,,ÊôÆ,ÈÄö,Âêç,Ë©û,,,„Çµ,Â§â,ÂèØ,ËÉΩ,,,*]\n  „Åó -&gt; ÁÇ∫„Çã [Âãï,Ë©û,,,Èùû,Ëá™,Á´ã,ÂèØ,ËÉΩ,,,*,,,*]\n  „Å¶ -&gt; „Å¶ [Âä©,Ë©û,,,Êé•,Á∂ö,Âä©,Ë©û,,,*,,,*]\n  „ÅÑ„Çã -&gt; Â±Ö„Çã [Âãï,Ë©û,,,Èùû,Ëá™,Á´ã,ÂèØ,ËÉΩ,,,*,,,*]\n  „ÄÇ -&gt; „ÄÇ [Ë£ú,Âä©,Ë®ò,Âè∑,,,Âè•,ÁÇπ,,,*,,,*]\n\n\n\n\nShow code\n# A-4: Morphological field extraction function\ndef iter_lemma_keys_plan_a(text: str, tagger) -&gt; List[Tuple[str, str, str]]:\n    \"\"\"\n    Extract (lemma, reading, pos_major) tuples from text using UniDic.\n    \n    Args:\n        text: Input Japanese text\n        tagger: fugashi Tagger instance\n    \n    Returns:\n        List of (dictionary_form, reading, pos_major) tuples\n    \"\"\"\n    keys = []\n    for m in tagger(text):\n        if m.surface.strip():  # Skip empty tokens\n            # UniDic POS is hierarchical; use major category (pos[0])\n            pos_major = m.pos[0] if m.pos else 'UNKNOWN'\n            lemma = m.feature[10] if m.feature[10] else m.surface\n            reading = m.feature[11] if m.feature[11] else ''\n            keys.append((lemma, reading, pos_major))\n    return keys\n\n# Test the extraction function\ntest_keys = iter_lemma_keys_plan_a(test_text, tagger_a)\nprint(f\"Extracted keys from '{test_text}':\")\nfor lemma, reading, pos in test_keys:\n    print(f\"  ({lemma}, {reading}, {pos})\")\n\n\nExtracted keys from 'Êó•„Åî„Çç„Åã„ÇâÂãâÂº∑„Åó„Å¶„ÅÑ„Çã„ÄÇ':\n  (Êó•„Åî„Çç, „Éí„Ç¥„É≠, Âêç)\n  („Åã„Çâ, „Ç´„É©, Âä©)\n  (ÂãâÂº∑, „Éô„É≥„Ç≠„Éß„Éº, Âêç)\n  („Åô„Çã, „Çπ„É´, Âãï)\n  („Å¶, „ÉÜ, Âä©)\n  („ÅÑ„Çã, „Ç§„É´, Âãï)\n  („ÄÇ, *, Ë£ú)\n\n\n\n\nShow code\n# Fixed version with proper fugashi/UniDic attribute handling\ndef iter_lemma_keys_fixed(text: str, tagger) -&gt; List[Tuple[str, str, str]]:\n    \"\"\"\n    Extract (lemma, reading, pos_major) tuples from text using UniDic.\n    Fixed version that handles fugashi attribute variations.\n    \"\"\"\n    keys = []\n    for m in tagger(text):\n        if m.surface.strip():  # Skip empty tokens\n            # UniDic POS is hierarchical; use major category (pos[0])\n            pos_major = m.pos[0] if m.pos else 'UNKNOWN'\n            \n            # Handle different attribute names for lemma\n            try:\n                lemma = m.lemma if hasattr(m, 'lemma') else m.feature[10]\n            except:\n                lemma = m.surface  # fallback\n            \n            # Handle different attribute names for reading\n            try:\n                reading = m.feature[9] if len(m.feature) &gt; 9 else ''\n            except:\n                reading = ''  # fallback\n            \n            keys.append((lemma, reading, pos_major))\n    return keys\n\n# Use the fixed function\niter_lemma_keys_plan_a = iter_lemma_keys_fixed\n\n# Test the fixed function\ntest_keys = iter_lemma_keys_plan_a(test_text, tagger_a)\nprint(f\"Extracted keys from '{test_text}' (fixed version):\")\nfor lemma, reading, pos in test_keys:\n    print(f\"  ({lemma}, {reading}, {pos})\")\n\n\nExtracted keys from 'Êó•„Åî„Çç„Åã„ÇâÂãâÂº∑„Åó„Å¶„ÅÑ„Çã„ÄÇ' (fixed version):\n  (Êó•„Åî„Çç, „Éí„Ç¥„É≠, Âêç)\n  („Åã„Çâ, „Ç´„É©, Âä©)\n  (ÂãâÂº∑, „Éô„É≥„Ç≠„Éß„Éº, Âêç)\n  („Åô„Çã, „Ç∑, Âãï)\n  („Å¶, „ÉÜ, Âä©)\n  („ÅÑ„Çã, „Ç§„É´, Âãï)\n  („ÄÇ, *, Ë£ú)\n\n\n\n\nShow code\n# A-5: Frequency analysis with BCCWJ matching\ndef analyze_corpus_plan_a(corpus: List[str], tagger, bccwj_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Analyze corpus using Plan A and match with BCCWJ frequencies.\"\"\"\n    freq = Counter()\n    \n    print(f\"Analyzing {len(corpus)} texts with Plan A...\")\n    for text in corpus:\n        for key in iter_lemma_keys_plan_a(text, tagger):\n            freq[key] += 1\n    \n    # Convert to DataFrame\n    rows = []\n    for (lemma, reading, pos), count in freq.items():\n        rows.append((lemma, reading, pos, count))\n    \n    df_local = pd.DataFrame(rows, columns=['lemma', 'reading', 'pos', 'freq_local'])\n    df_local['key'] = list(zip(df_local.lemma, df_local.reading, df_local.pos))\n    \n    # Merge with BCCWJ data\n    merged = df_local.merge(bccwj_df[['key', 'freq_bccwj']], on='key', how='left')\n    \n    return merged.sort_values('freq_local', ascending=False)\n\n# Run Plan A analysis\nresults_a = analyze_corpus_plan_a(extended_corpus, tagger_a, df_bccwj)\nprint(f\"\\nPlan A Results (top 15):\")\nprint(results_a.head(15)[['lemma', 'reading', 'pos', 'freq_local', 'freq_bccwj']])\n\n\nAnalyzing 30 texts with Plan A...\n\nPlan A Results (top 15):\n   lemma reading pos  freq_local  freq_bccwj\n11     „ÄÇ       *   Ë£ú          30         NaN\n8      „Åå       „Ç¨   Âä©          18         NaN\n1      „ÅØ       „ÉØ   Âä©          15         NaN\n7      „ÅÆ       „Éé   Âä©          15         NaN\n5      „Çí       „Ç™   Âä©          12         NaN\n10    „Åß„Åô      „Éá„Çπ   Âä©           9         NaN\n42     „Åß       „Éá   Âä©           9         NaN\n15     „Å†       „ÉÄ   Âä©           6         NaN\n37     „Å¶       „ÉÜ   Âä©           6         NaN\n41    È£≤„ÇÄ      „Éé„É≥   Âãï           6         NaN\n43     „ÄÅ       *   Ë£ú           6         NaN\n48    ÂõΩÈöõ    „Ç≥„ÇØ„Çµ„Ç§   Âêç           3         NaN\n47     „Åü       „Çø   Âä©           3         NaN\n46   „Åó„Åæ„ÅÜ     „Ç∑„Éû„ÉÉ   Âãï           3         NaN\n0      ÂΩº      „Ç´„É¨   ‰ª£           3         NaN\n\n\n\n\nShow code\n# A-6: Evaluation metrics for Plan A\ndef calculate_metrics(df: pd.DataFrame) -&gt; Dict[str, float]:\n    \"\"\"Calculate coverage and correlation metrics.\"\"\"\n    # Coverage: percentage of local tokens found in BCCWJ\n    matched = df.dropna(subset=['freq_bccwj'])\n    coverage = len(matched) / len(df) * 100\n    \n    # Token coverage (by frequency)\n    total_tokens = df['freq_local'].sum()\n    matched_tokens = matched['freq_local'].sum()\n    token_coverage = matched_tokens / total_tokens * 100\n    \n    # Spearman correlation for matched items\n    if len(matched) &gt; 1:\n        if scipy_available:\n            correlation, p_value = spearmanr(matched['freq_local'], matched['freq_bccwj'])\n        else:\n            correlation = np.corrcoef(matched['freq_local'].rank(), matched['freq_bccwj'].rank())[0,1]\n            p_value = None\n    else:\n        correlation, p_value = None, None\n    \n    return {\n        'type_coverage': coverage,\n        'token_coverage': token_coverage,\n        'correlation': correlation,\n        'p_value': p_value,\n        'total_types': len(df),\n        'matched_types': len(matched),\n        'total_tokens': total_tokens,\n        'matched_tokens': matched_tokens\n    }\n\nmetrics_a = calculate_metrics(results_a)\nprint(\"Plan A Evaluation Metrics:\")\nfor key, value in metrics_a.items():\n    if isinstance(value, float) and value is not None:\n        print(f\"  {key}: {value:.3f}\")\n    else:\n        print(f\"  {key}: {value}\")\n\n\nPlan A Evaluation Metrics:\n  type_coverage: 0.000\n  token_coverage: 0.000\n  correlation: None\n  p_value: None\n  total_types: 66\n  matched_types: 0\n  total_tokens: 297\n  matched_tokens: 0"
  },
  {
    "objectID": "resources/code-examples/python/dl_aozorabunko.html#lets-download-the-data-from-huggingface",
    "href": "resources/code-examples/python/dl_aozorabunko.html#lets-download-the-data-from-huggingface",
    "title": "Downloading Aozorabunko data and make it a usable corpus",
    "section": "Let‚Äôs download the data from Huggingface",
    "text": "Let‚Äôs download the data from Huggingface\nWe will use the data from the following repository\n\n\nShow code\nimport pandas as pd\n\ndf = pd.read_json(\"hf://datasets/globis-university/aozorabunko-clean/aozorabunko-dedupe-clean.jsonl.gz\", lines=True)\n\n\n\n\nShow code\ndf\n\n\n\n\n\n\n\n\n\ntext\nfootnote\nmeta\n\n\n\n\n0\nÊ∑±„ÅÑ„Åä„Å©„Çç„Åç„Å´„ÅÜ„Åü„Çå„Å¶„ÄÅ\\nÂêçÈ´ò„ÅÑ„Ç¶„Çß„Çπ„Éà„Éü„É≥„Çπ„Çø„Éº„Å´\\nÁúüÈçÆ„ÇÑÁü≥„ÅÆË®òÂøµÁ¢ë„Å®„Å™„Å£„Å¶\\n„Åô„Åπ„Å¶...\nÂ∫ïÊú¨Ôºö„Äå„Çπ„Ç±„ÉÉ„ÉÅ„Éª„Éñ„ÉÉ„ÇØ„ÄçÊñ∞ÊΩÆÊñáÂ∫´„ÄÅÊñ∞ÊΩÆÁ§æ\\n„ÄÄ„ÄÄ„ÄÄ1957ÔºàÊò≠Âíå32ÔºâÂπ¥5Êúà20Êó•Áô∫Ë°å\\n...\n{'‰ΩúÂìÅID': '059898', '‰ΩúÂìÅÂêç': '„Ç¶„Çß„Çπ„Éà„Éü„É≥„Çπ„Çø„ÉºÂØ∫Èô¢', '‰ΩúÂìÅÂêçË™≠...\n\n\n1\n„ÅÑ„Åñ„ÄÅ„Åì„Çå„Çà„ÇäÊ®Ç„Åó„Åæ„ÇÄ„ÄÅ\\n‰ªïÁΩÆ„ÇíÂèó„Åè„ÇãÊÜÇ„Å™„Åè„ÄÅ\\nÈÅä„Å≥„Åü„ÅÆ„Åó„ÇÄÊôÇ„Åû‰æÜ„Å¨„ÄÅ\\nÊôÇ„Åû‰æÜ„Å¨„Çå„Å∞„ÄÅ...\nÂ∫ïÊú¨Ôºö„Äå„Çπ„Ç±„ÉÉ„ÉÅ„Éª„Éñ„ÉÉ„ÇØ„ÄçÂ≤©Ê≥¢ÊñáÂ∫´„ÄÅÂ≤©Ê≥¢Êõ∏Â∫ó\\n„ÄÄ„ÄÄ„ÄÄ1935ÔºàÊò≠Âíå10ÔºâÂπ¥9Êúà15Êó•Á¨¨1Âà∑...\n{'‰ΩúÂìÅID': '056078', '‰ΩúÂìÅÂêç': 'ÈßÖ‰ºùÈ¶¨Ëªä', '‰ΩúÂìÅÂêçË™≠„Åø': '„Åà„Åç...\n\n\n2\n„Åô„Åπ„Å¶„Çà„Åó„ÄÇ\\n‰Ωï„Åó„Å¶ÈÅä„Åº„Å®\\nÂè±„Çâ„Çå„Å™„ÅÑ„ÄÇ\\nÊôÇ„ÅØ„Åç„Åü„ÄÇ\\n„Åï„Å£„Åï„Å®\\nÊú¨„Å™„Å©Êäï„Åí„Å†„Åù„ÅÜ„ÄÇ...\nÂ∫ïÊú¨Ôºö„Äå„Çπ„Ç±„ÉÉ„ÉÅ„Éª„Éñ„ÉÉ„ÇØ„ÄçÊñ∞ÊΩÆÊñáÂ∫´„ÄÅÊñ∞ÊΩÆÁ§æ\\n„ÄÄ„ÄÄ„ÄÄ1957ÔºàÊò≠Âíå32ÔºâÂπ¥5Êúà20Êó•Áô∫Ë°å\\n...\n{'‰ΩúÂìÅID': '060224', '‰ΩúÂìÅÂêç': 'ÈßÖÈ¶¨Ëªä', '‰ΩúÂìÅÂêçË™≠„Åø': '„Åà„Åç„Å∞...\n\n\n3\nÂπ¥ËÄÅ„ÅÑ„Åü‰∫∫„Çí„ÅÑ„Åü„Çè„Çä„Å™„Åï„ÅÑ„ÄÇ„Åù„ÅÆÈäÄÈ´™„ÅØ„ÄÅ\\nÂêçË™â„Å®Â∞äÊï¨„Çí„Å§„Å≠„Å´ÈõÜ„ÇÅ„Å¶„Åç„Åü„ÅÆ„Åß„Åô„ÄÇ\\n‚Äï‚Äï„Éû„Éº...\nÂ∫ïÊú¨Ôºö„Äå„Çπ„Ç±„ÉÉ„ÉÅ„Éª„Éñ„ÉÉ„ÇØ„ÄçÊñ∞ÊΩÆÊñáÂ∫´„ÄÅÊñ∞ÊΩÆÁ§æ\\n„ÄÄ„ÄÄ„ÄÄ1957ÔºàÊò≠Âíå32ÔºâÂπ¥5Êúà20Êó•Áô∫Ë°å\\n...\n{'‰ΩúÂìÅID': '060225', '‰ΩúÂìÅÂêç': 'ÂØ°Â©¶„Å®„Åù„ÅÆÂ≠ê', '‰ΩúÂìÅÂêçË™≠„Åø': '...\n\n\n4\n„Å†„Åå„ÄÅ„ÅÇ„ÅÆ„Å™„Å§„Åã„Åó„ÅÑ„ÄÅÊÄù„ÅÑÂá∫„Åµ„Åã„ÅÑ„ÇØ„É™„Çπ„Éû„Çπ„ÅÆ„ÅäÁà∫„Åï„Çì„ÅØ„ÇÇ„ÅÜÈÄù„Å£„Å¶„Åó„Åæ„Å£„Åü„ÅÆ„Å†„Çç„ÅÜ„Åã„ÄÇ„ÅÇ„Å®...\nÂ∫ïÊú¨Ôºö„Äå„Çπ„Ç±„ÉÉ„ÉÅ„Éª„Éñ„ÉÉ„ÇØ„ÄçÊñ∞ÊΩÆÊñáÂ∫´„ÄÅÊñ∞ÊΩÆÁ§æ\\n„ÄÄ„ÄÄ„ÄÄ1957ÔºàÊò≠Âíå32ÔºâÂπ¥5Êúà20Êó•Áô∫Ë°å\\n...\n{'‰ΩúÂìÅID': '060231', '‰ΩúÂìÅÂêç': '„ÇØ„É™„Çπ„Éû„Çπ', '‰ΩúÂìÅÂêçË™≠„Åø': '„ÇØ...\n\n\n...\n...\n...\n...\n\n\n16946\nÂ§¢„ÅÆË©±„Çí„Åô„Çã„ÅÆ„ÅØ„ÅÇ„Åæ„ÇäÊ∞ó„ÅÆ„Åç„ÅÑ„Åü„Åì„Å®„Åß„ÅØ„Å™„ÅÑ„ÄÇÁ¢∫„ÅãÁó¥‰∫∫Â§¢„ÇíË™¨„Åè„Å®„ÅÑ„ÅÜË®ÄËëâ„Åå„ÅÇ„Å£„Åü„ÅØ„Åö„Å†„ÄÇ„Åù...\nÂ∫ïÊú¨Ôºö„ÄåÂíåËæªÂì≤ÈÉéÂÖ®ÈõÜ„ÄÄË£úÈÅ∫„ÄçÂ≤©Ê≥¢Êõ∏Â∫ó\\n„ÄÄ„ÄÄ„ÄÄ1978ÔºàÊò≠Âíå53ÔºâÂπ¥6Êúà16Êó•Á¨¨1Âà∑Áô∫Ë°å\\n...\n{'‰ΩúÂìÅID': '055622', '‰ΩúÂìÅÂêç': 'Â§¢', '‰ΩúÂìÅÂêçË™≠„Åø': '„ÇÜ„ÇÅ', ...\n\n\n16947\nËá™ÂàÜ„ÅØÁèæ‰ª£„ÅÆÁîªÂÆ∂‰∏≠„Å´Â≤∏Áî∞Âêõ„Åª„Å©Êòé„Çâ„Åã„Å™„ÄåÊàêÈï∑„Äç„ÇíÁ§∫„Åó„Å¶„ÅÑ„Çã‰∫∫„ÇíÁü•„Çâ„Å™„ÅÑ„ÄÇË™áÂºµ„Åß„Å™„ÅèÂ≤∏Áî∞Âêõ„ÅØ...\nÂ∫ïÊú¨Ôºö„ÄåÂíåËæªÂì≤ÈÉéÈöèÁ≠ÜÈõÜ„ÄçÂ≤©Ê≥¢ÊñáÂ∫´„ÄÅÂ≤©Ê≥¢Êõ∏Â∫ó\\n„ÄÄ„ÄÄ„ÄÄ1995ÔºàÂπ≥Êàê7ÔºâÂπ¥9Êúà18Êó•Á¨¨1Âà∑Áô∫Ë°å...\n{'‰ΩúÂìÅID': '049876', '‰ΩúÂìÅÂêç': '„ÄéÂäâÁîüÁîªÈõÜÂèäËä∏Ë°ìË¶≥„Äè„Å´„Å§„ÅÑ„Å¶', '‰Ωú...\n\n\n16948\n‰∏Ä\\n\\n„ÄÄËçíÊº†„Åü„ÇãÁßã„ÅÆÈáé„Å´Á´ã„Å§„ÄÇÊòü„ÅØÊúà„ÅÆÂæ°Â∫ß„ÇíÂõ≤„ÅøÊúà„ÅØÊ∏Ö„Çâ„Åã„Å´Âú∞„ÅÆËä±„ÇíËºù„Çâ„Åô„ÄÇËä±„ÅØÁ¥Ö„Å®Âí≤„Åç...\nÂ∫ïÊú¨Ôºö„ÄåÂÅ∂ÂÉèÂÜçËàà„ÉªÈù¢„Å®„Éö„É´„ÇΩ„Éä„ÄÄÂíåËæªÂì≤ÈÉéÊÑüÊÉ≥ÈõÜ„ÄçË¨õË´áÁ§æÊñáËä∏ÊñáÂ∫´„ÄÅË¨õË´áÁ§æ\\n„ÄÄ„ÄÄ„ÄÄ2007ÔºàÂπ≥...\n{'‰ΩúÂìÅID': '049913', '‰ΩúÂìÅÂêç': 'ÈúäÁöÑÊú¨ËÉΩ‰∏ªÁæ©', '‰ΩúÂìÅÂêçË™≠„Åø': '...\n\n\n16949\nÈñ¢Êù±Â§ßÈúáÁÅΩ„ÅÆÂâçÊï∞Âπ¥„ÅÆÈñì„ÄÅÂÖàËº©„Åü„Å°„Å´„Åæ„Åò„Å£„Å¶Èú≤‰º¥ÂÖàÁîü„Åã„Çâ‰ø≥Ë´ß„ÅÆÊåáÂ∞é„Çí„ÅÜ„Åë„Åü„Åì„Å®„Åå„ÅÇ„Çã„ÄÇ„Åù„ÅÆÊôÇ...\nÂ∫ïÊú¨Ôºö„ÄåÂíåËæªÂì≤ÈÉéÈöèÁ≠ÜÈõÜ„ÄçÂ≤©Ê≥¢ÊñáÂ∫´„ÄÅÂ≤©Ê≥¢Êõ∏Â∫ó\\n„ÄÄ„ÄÄ„ÄÄ1995ÔºàÂπ≥Êàê7ÔºâÂπ¥9Êúà18Êó•Á¨¨1Âà∑Áô∫Ë°å...\n{'‰ΩúÂìÅID': '049914', '‰ΩúÂìÅÂêç': 'Èú≤‰º¥ÂÖàÁîü„ÅÆÊÄù„ÅÑÂá∫', '‰ΩúÂìÅÂêçË™≠„Åø':...\n\n\n16950\nË®≥ËÄÖÂ∫è\\n\\n„ÄÄ‰∏Ä‰πù„Äá‰πùÂπ¥„ÄÅ„É¨„Ç™„É≥„Éª„ÉØ„É´„É©„Çπ„ÅÆ‰∏ÉÂçÅ‰∫îÊ≠≥„ÅÆÈΩ¢„ÇíË®òÂøµ„Åó„Å¶„ÄÅ„É≠„Éº„Ç∂„É≥„ÉåÂ§ß...\nÂ∫ïÊú¨Ôºö„ÄåÁ¥îÁ≤πÁ∂ìÊøüÂ≠∏Ë¶ÅË´ñ„ÄÄ‰∏äÂç∑„ÄçÂ≤©Ê≥¢ÊñáÂ∫´„ÄÅÂ≤©Ê≥¢Êõ∏Â∫ó\\n„ÄÄ„ÄÄ„ÄÄ1953ÔºàÊò≠Âíå28ÔºâÂπ¥11Êúà25Êó•...\n{'‰ΩúÂìÅID': '045210', '‰ΩúÂìÅÂêç': 'Á¥îÁ≤ãÁµåÊ∏àÂ≠¶Ë¶ÅË´ñ', '‰ΩúÂìÅÂêçË™≠„Åø': ...\n\n\n\n\n16951 rows √ó 3 columns\n\n\n\n\n\nShow code\ndf['meta'][1]\n\n\n{'‰ΩúÂìÅID': '056078',\n '‰ΩúÂìÅÂêç': 'ÈßÖ‰ºùÈ¶¨Ëªä',\n '‰ΩúÂìÅÂêçË™≠„Åø': '„Åà„Åç„Åß„Çì„Å∞„Åó„ÇÉ',\n '„ÇΩ„Éº„ÉàÁî®Ë™≠„Åø': '„Åà„Åç„Å¶„Çì„ÅØ„Åó„ÇÑ',\n 'ÂâØÈ°å': '',\n 'ÂâØÈ°åË™≠„Åø': '',\n 'ÂéüÈ°å': '',\n 'ÂàùÂá∫': '',\n 'ÂàÜÈ°ûÁï™Âè∑': 'NDC 933',\n 'ÊñáÂ≠óÈÅ£„ÅÑÁ®ÆÂà•': 'ÊóßÂ≠óÊóß‰ªÆÂêç',\n '‰ΩúÂìÅËëó‰ΩúÊ®©„Éï„É©„Ç∞': '„Å™„Åó',\n 'ÂÖ¨ÈñãÊó•': '2013-09-20',\n 'ÊúÄÁµÇÊõ¥Êñ∞Êó•': '2014-09-16',\n 'Âõ≥Êõ∏„Ç´„Éº„ÉâURL': 'https://www.aozora.gr.jp/cards/001257/card56078.html',\n '‰∫∫Áâ©ID': '001257',\n 'Âßì': '„Ç¢„Éº„É¥„Ç£„É≥„Ç∞',\n 'Âêç': '„ÉØ„Ç∑„É≥„Éà„É≥',\n 'ÂßìË™≠„Åø': '„Ç¢„Éº„É¥„Ç£„É≥„Ç∞',\n 'ÂêçË™≠„Åø': '„ÉØ„Ç∑„É≥„Éà„É≥',\n 'ÂßìË™≠„Åø„ÇΩ„Éº„ÉàÁî®': '„ÅÇ„ÅÇ„ÅÜ„ÅÑ„Çì„Åè',\n 'ÂêçË™≠„Åø„ÇΩ„Éº„ÉàÁî®': '„Çè„Åó„Çì„Å®„Çì',\n 'Âßì„É≠„Éº„ÉûÂ≠ó': 'Irving',\n 'Âêç„É≠„Éº„ÉûÂ≠ó': 'Washington',\n 'ÂΩπÂâ≤„Éï„É©„Ç∞': 'ËëóËÄÖ',\n 'ÁîüÂπ¥ÊúàÊó•': '1783-04-03',\n 'Ê≤°Âπ¥ÊúàÊó•': '1859-11-28',\n '‰∫∫Áâ©Ëëó‰ΩúÊ®©„Éï„É©„Ç∞': '„Å™„Åó',\n 'Â∫ïÊú¨Âêç1': '„Çπ„Ç±„ÉÉ„ÉÅ„Éª„Éñ„ÉÉ„ÇØ',\n 'Â∫ïÊú¨Âá∫ÁâàÁ§æÂêç1': 'Â≤©Ê≥¢ÊñáÂ∫´„ÄÅÂ≤©Ê≥¢Êõ∏Â∫ó',\n 'Â∫ïÊú¨ÂàùÁâàÁô∫Ë°åÂπ¥1': '1935ÔºàÊò≠Âíå10ÔºâÂπ¥9Êúà15Êó•',\n 'ÂÖ•Âäõ„Å´‰ΩøÁî®„Åó„ÅüÁâà1': '2010ÔºàÂπ≥Êàê22ÔºâÂπ¥2Êúà23Êó•Á¨¨31Âà∑',\n 'Ê†°Ê≠£„Å´‰ΩøÁî®„Åó„ÅüÁâà1': '1992ÔºàÂπ≥Êàê4ÔºâÂπ¥2Êúà26Êó•Á¨¨30Âà∑',\n 'Â∫ïÊú¨„ÅÆË¶™Êú¨Âêç1': '',\n 'Â∫ïÊú¨„ÅÆË¶™Êú¨Âá∫ÁâàÁ§æÂêç1': '',\n 'Â∫ïÊú¨„ÅÆË¶™Êú¨ÂàùÁâàÁô∫Ë°åÂπ¥1': '',\n 'Â∫ïÊú¨Âêç2': '',\n 'Â∫ïÊú¨Âá∫ÁâàÁ§æÂêç2': '',\n 'Â∫ïÊú¨ÂàùÁâàÁô∫Ë°åÂπ¥2': '',\n 'ÂÖ•Âäõ„Å´‰ΩøÁî®„Åó„ÅüÁâà2': '',\n 'Ê†°Ê≠£„Å´‰ΩøÁî®„Åó„ÅüÁâà2': '',\n 'Â∫ïÊú¨„ÅÆË¶™Êú¨Âêç2': '',\n 'Â∫ïÊú¨„ÅÆË¶™Êú¨Âá∫ÁâàÁ§æÂêç2': '',\n 'Â∫ïÊú¨„ÅÆË¶™Êú¨ÂàùÁâàÁô∫Ë°åÂπ¥2': '',\n 'ÂÖ•ÂäõËÄÖ': 'ÈõÄ',\n 'Ê†°Ê≠£ËÄÖ': 'Â∞èÊûóÁπÅÈõÑ',\n '„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´URL': 'https://www.aozora.gr.jp/cards/001257/files/56078_ruby_51155.zip',\n '„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´ÊúÄÁµÇÊõ¥Êñ∞Êó•': '2013-09-03',\n '„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´Á¨¶Âè∑ÂåñÊñπÂºè': 'ShiftJIS',\n '„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´ÊñáÂ≠óÈõÜÂêà': 'JIS X 0208',\n '„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´‰øÆÊ≠£ÂõûÊï∞': '0',\n 'XHTML/HTML„Éï„Ç°„Ç§„É´URL': 'https://www.aozora.gr.jp/cards/001257/files/56078_51422.html',\n 'XHTML/HTML„Éï„Ç°„Ç§„É´ÊúÄÁµÇÊõ¥Êñ∞Êó•': '2013-09-03',\n 'XHTML/HTML„Éï„Ç°„Ç§„É´Á¨¶Âè∑ÂåñÊñπÂºè': 'ShiftJIS',\n 'XHTML/HTML„Éï„Ç°„Ç§„É´ÊñáÂ≠óÈõÜÂêà': 'JIS X 0208',\n 'XHTML/HTML„Éï„Ç°„Ç§„É´‰øÆÊ≠£ÂõûÊï∞': '0'}\n\n\n\n\nShow code\nimport os\n\n# Set random seed for reproducibility\nrandom_seed = 42\n\n# Sample 1000 files from the dataframe\ndf_sample = df.sample(n=500, random_state=random_seed)\n\n# Create directory if it doesn't exist\noutput_dir = \"../../../corpus_data/aozora_500\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save each text to a file named by ‰ΩúÂìÅID\nfor idx, (_, row) in enumerate(df_sample.iterrows()):\n    # Get the ‰ΩúÂìÅID from meta field\n    sakuhin_id = row['meta']['‰ΩúÂìÅID']\n    \n    # Get the text content\n    text_content = row['text']\n    \n    # Create filename based on ‰ΩúÂìÅID\n    filename = f\"{sakuhin_id}.txt\"\n    filepath = os.path.join(output_dir, filename)\n    \n    # Write text to file\n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write(text_content)\n    \n    # Print progress every 100 files\n    if (idx + 1) % 100 == 0:\n        print(f\"Processed {idx + 1} files...\")\n\nprint(f\"Completed! Saved {len(df_sample)} text files to {output_dir}\")\n\n\nProcessed 100 files...\nProcessed 200 files...\nProcessed 300 files...\nProcessed 400 files...\nProcessed 500 files...\nCompleted! Saved 500 text files to ../../../corpus_data/aozora_500\n\n\n\n\nShow code\n# Sample 1000 files from the dataframe\ndf_sample = df.sample(n=50, random_state=random_seed)\n\n# Create directory if it doesn't exist\noutput_dir = \"../../../corpus_data/aozora_50\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save each text to a file named by ‰ΩúÂìÅID\nfor idx, (_, row) in enumerate(df_sample.iterrows()):\n    # Get the ‰ΩúÂìÅID from meta field\n    sakuhin_id = row['meta']['‰ΩúÂìÅID']\n    \n    # Get the text content\n    text_content = row['text']\n    \n    # Create filename based on ‰ΩúÂìÅID\n    filename = f\"{sakuhin_id}.txt\"\n    filepath = os.path.join(output_dir, filename)\n    \n    # Write text to file\n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write(text_content)\n    \n    # Print progress every 100 files\n    if (idx + 1) % 100 == 0:\n        print(f\"Processed {idx + 1} files...\")\n\nprint(f\"Completed! Saved {len(df_sample)} text files to {output_dir}\")\n\n\nCompleted! Saved 50 text files to ../../../corpus_data/aozora_50\n\n\n\n\nShow code\n\ndf_sample = df\n\n# Create directory if it doesn't exist\noutput_dir = \"../../../corpus_data/aozorabunko\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save each text to a file named by ‰ΩúÂìÅID\nfor idx, (_, row) in enumerate(df_sample.iterrows()):\n    # Get the ‰ΩúÂìÅID from meta field\n    sakuhin_id = row['meta']['‰ΩúÂìÅID']\n    \n    # Get the text content\n    text_content = row['text']\n    \n    # Create filename based on ‰ΩúÂìÅID\n    filename = f\"{sakuhin_id}.txt\"\n    filepath = os.path.join(output_dir, filename)\n    \n    # Write text to file\n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write(text_content)\n    \n    # Print progress every 100 files\n    if (idx + 1) % 100 == 0:\n        print(f\"Processed {idx + 1} files...\")\n\nprint(f\"Completed! Saved {len(df_sample)} text files to {output_dir}\")\n\n\nProcessed 100 files...\nProcessed 200 files...\nProcessed 300 files...\nProcessed 400 files...\nProcessed 500 files...\nProcessed 600 files...\nProcessed 700 files...\nProcessed 800 files...\nProcessed 900 files...\nProcessed 1000 files...\nProcessed 1100 files...\nProcessed 1200 files...\nProcessed 1300 files...\nProcessed 1400 files...\nProcessed 1500 files...\nProcessed 1600 files...\nProcessed 1700 files...\nProcessed 1800 files...\nProcessed 1900 files...\nProcessed 2000 files...\nProcessed 2100 files...\nProcessed 2200 files...\nProcessed 2300 files...\nProcessed 2400 files...\nProcessed 2500 files...\nProcessed 2600 files...\nProcessed 2700 files...\nProcessed 2800 files...\nProcessed 2900 files...\nProcessed 3000 files...\nProcessed 3100 files...\nProcessed 3200 files...\nProcessed 3300 files...\nProcessed 3400 files...\nProcessed 3500 files...\nProcessed 3600 files...\nProcessed 3700 files...\nProcessed 3800 files...\nProcessed 3900 files...\nProcessed 4000 files...\nProcessed 4100 files...\nProcessed 4200 files...\nProcessed 4300 files...\nProcessed 4400 files...\nProcessed 4500 files...\nProcessed 4600 files...\nProcessed 4700 files...\nProcessed 4800 files...\nProcessed 4900 files...\nProcessed 5000 files...\nProcessed 5100 files...\nProcessed 5200 files...\nProcessed 5300 files...\nProcessed 5400 files...\nProcessed 5500 files...\nProcessed 5600 files...\nProcessed 5700 files...\nProcessed 5800 files...\nProcessed 5900 files...\nProcessed 6000 files...\nProcessed 6100 files...\nProcessed 6200 files...\nProcessed 6300 files...\nProcessed 6400 files...\nProcessed 6500 files...\nProcessed 6600 files...\nProcessed 6700 files...\nProcessed 6800 files...\nProcessed 6900 files...\nProcessed 7000 files...\nProcessed 7100 files...\nProcessed 7200 files...\nProcessed 7300 files...\nProcessed 7400 files...\nProcessed 7500 files...\nProcessed 7600 files...\nProcessed 7700 files...\nProcessed 7800 files...\nProcessed 7900 files...\nProcessed 8000 files...\nProcessed 8100 files...\nProcessed 8200 files...\nProcessed 8300 files...\nProcessed 8400 files...\nProcessed 8500 files...\nProcessed 8600 files...\nProcessed 8700 files...\nProcessed 8800 files...\nProcessed 8900 files...\nProcessed 9000 files...\nProcessed 9100 files...\nProcessed 9200 files...\nProcessed 9300 files...\nProcessed 9400 files...\nProcessed 9500 files...\nProcessed 9600 files...\nProcessed 9700 files...\nProcessed 9800 files...\nProcessed 9900 files...\nProcessed 10000 files...\nProcessed 10100 files...\nProcessed 10200 files...\nProcessed 10300 files...\nProcessed 10400 files...\nProcessed 10500 files...\nProcessed 10600 files...\nProcessed 10700 files...\nProcessed 10800 files...\nProcessed 10900 files...\nProcessed 11000 files...\nProcessed 11100 files...\nProcessed 11200 files...\nProcessed 11300 files...\nProcessed 11400 files...\nProcessed 11500 files...\nProcessed 11600 files...\nProcessed 11700 files...\nProcessed 11800 files...\nProcessed 11900 files...\nProcessed 12000 files...\nProcessed 12100 files...\nProcessed 12200 files...\nProcessed 12300 files...\nProcessed 12400 files...\nProcessed 12500 files...\nProcessed 12600 files...\nProcessed 12700 files...\nProcessed 12800 files...\nProcessed 12900 files...\nProcessed 13000 files...\nProcessed 13100 files...\nProcessed 13200 files...\nProcessed 13300 files...\nProcessed 13400 files...\nProcessed 13500 files...\nProcessed 13600 files...\nProcessed 13700 files...\nProcessed 13800 files...\nProcessed 13900 files...\nProcessed 14000 files...\nProcessed 14100 files...\nProcessed 14200 files...\nProcessed 14300 files...\nProcessed 14400 files...\nProcessed 14500 files...\nProcessed 14600 files...\nProcessed 14700 files...\nProcessed 14800 files...\nProcessed 14900 files...\nProcessed 15000 files...\nProcessed 15100 files...\nProcessed 15200 files...\nProcessed 15300 files...\nProcessed 15400 files...\nProcessed 15500 files...\nProcessed 15600 files...\nProcessed 15700 files...\nProcessed 15800 files...\nProcessed 15900 files...\nProcessed 16000 files...\nProcessed 16100 files...\nProcessed 16200 files...\nProcessed 16300 files...\nProcessed 16400 files...\nProcessed 16500 files...\nProcessed 16600 files...\nProcessed 16700 files...\nProcessed 16800 files...\nProcessed 16900 files...\nCompleted! Saved 16951 text files to ../../../corpus_data/aozorabunko"
  },
  {
    "objectID": "resources/code-examples/python/extracting-collocations.html",
    "href": "resources/code-examples/python/extracting-collocations.html",
    "title": "Extracting collocations in Python",
    "section": "",
    "text": "In this notebook, I will demonstrate how to extract collocations from a corpus."
  },
  {
    "objectID": "resources/code-examples/python/extracting-collocations.html#preparation",
    "href": "resources/code-examples/python/extracting-collocations.html#preparation",
    "title": "Extracting collocations in Python",
    "section": "Preparation",
    "text": "Preparation\n\nLet‚Äôs load necessary package.\n\n\nShow code\n# Load packages\n\nimport spacy\nimport glob\nimport pandas as pd\n\n# Initialize spaCy model\nnlp = spacy.load(\"en_core_web_sm\")\n\n\n\n\nLet‚Äôs specify the corpus path\n\n\nShow code\nCORPUS_FILES = glob.glob(\"../../../corpus_data/brown_single/*.txt\")"
  },
  {
    "objectID": "resources/code-examples/index.html",
    "href": "resources/code-examples/index.html",
    "title": "Code Examples",
    "section": "",
    "text": "Downloading Aozora Bunko\nExtracting Collocations\nConducting Parallel Analysis for lexical diversity measurement",
    "crumbs": [
      "Resources",
      "Code Examples"
    ]
  },
  {
    "objectID": "resources/code-examples/index.html#available-resources",
    "href": "resources/code-examples/index.html#available-resources",
    "title": "Code Examples",
    "section": "",
    "text": "Downloading Aozora Bunko\nExtracting Collocations\nConducting Parallel Analysis for lexical diversity measurement",
    "crumbs": [
      "Resources",
      "Code Examples"
    ]
  },
  {
    "objectID": "resources/tools/japanese-nlp.html",
    "href": "resources/tools/japanese-nlp.html",
    "title": "Japanese NLP",
    "section": "",
    "text": "This page provides some advanced resources on Japanese NLP."
  },
  {
    "objectID": "resources/tools/japanese-nlp.html#demo-pages",
    "href": "resources/tools/japanese-nlp.html#demo-pages",
    "title": "Japanese NLP",
    "section": "Demo pages",
    "text": "Demo pages\n\nSpacy GiNZA morphological analysis\nWeb app for UniDic morphological analysis"
  },
  {
    "objectID": "resources/tools/japanese-nlp.html#using-unidic-with-fugashi",
    "href": "resources/tools/japanese-nlp.html#using-unidic-with-fugashi",
    "title": "Japanese NLP",
    "section": "Using Unidic with Fugashi",
    "text": "Using Unidic with Fugashi\n\n\nFugashi\n\n# Core tools\npip install fugashi unidic-lite  # quick start (smaller)\n# OR for full UniDic (larger, closer to BCCWJ)\npip install fugashi unidic\npython -m unidic download  # downloads the full UniDic dictionary"
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html",
    "href": "resources/tools/byu-corpora-guide.html",
    "title": "English Corpora Guide",
    "section": "",
    "text": "The EnglishCorpora.org, formally BYU (Brigham Young University) corpora, provide web-based interfaces to some of the largest and most widely-used corpora in the world. These include COCA (Corpus of Contemporary American English), BNC (British National Corpus), and many others.\nYou will need a usable account for english-corpora.org on on Day 1.",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html#overview",
    "href": "resources/tools/byu-corpora-guide.html#overview",
    "title": "English Corpora Guide",
    "section": "",
    "text": "The EnglishCorpora.org, formally BYU (Brigham Young University) corpora, provide web-based interfaces to some of the largest and most widely-used corpora in the world. These include COCA (Corpus of Contemporary American English), BNC (British National Corpus), and many others.\nYou will need a usable account for english-corpora.org on on Day 1.",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html#available-corpora",
    "href": "resources/tools/byu-corpora-guide.html#available-corpora",
    "title": "English Corpora Guide",
    "section": "Available Corpora",
    "text": "Available Corpora\n\nMajor English Corpora\n\nCOCA (Corpus of Contemporary American English): 1 billion words, 1990-2019\nBNC (British National Corpus): 100 million words\nGloWbE (Global Web-Based English): 1.9 billion words\nNOW (News on the Web): 14+ billion words, updated daily\nCOHA (Corpus of Historical American English): 400 million words, 1810-2009\n\n\n\nSpecialized Corpora\n\nSOAP (Corpus of American Soap Operas): 100 million words\nTIME (TIME Magazine Corpus): 100 million words, 1920s-2000s\nWikipedia Corpus: 1.9 billion words",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html#registration-and-access",
    "href": "resources/tools/byu-corpora-guide.html#registration-and-access",
    "title": "English Corpora Guide",
    "section": "Registration and Access",
    "text": "Registration and Access\n\nFree Access\n\nVisit english-corpora.org\nClick on desired corpus\nRegister for free account\nLimited to 20 queries per day\n\n\n\nAcademic License (Do not purchase for this class)\n\nExtended query limits\nDownload capabilities\nAvailable through institution\n\n\n\nSearch hints\n\nOverall hint",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/byu-corpora-guide.html#a-list-of-corpora",
    "href": "resources/tools/byu-corpora-guide.html#a-list-of-corpora",
    "title": "English Corpora Guide",
    "section": "A list of corpora",
    "text": "A list of corpora\n\n\n\nCorpus\nSize\nRegions\nTime\nGenre\n\n\n\n\nIWEB\n13.9b\n6\n2017\nWeb\n\n\nNOW\n16.2b\n20\n2010-now\nWeb: News\n\n\nCORONA\n1.58b\n20\n2020-now\nWeb: News\n\n\nGLOWBE\n1.9b\n20\n2012-13\nWeb/blogs\n\n\nWIKI\n1.9b\n(+)\n2014\nWikipedia\n\n\nCOCA\n1.0b\nAm\n1990-2019\nBalanced\n\n\nCOHA\n400m\nAm\n1810-2009\nBalanced\n\n\nTV\n325m\n6\n1950-2018\nTV shows\n\n\nMOVIES\n200m\n6\n1930-2018\nMovies\n\n\nSOAP\n100m\nAm\n2001-2012\nTV shows\n\n\nHANSARD\n1.6b\nBr\n1803-2005\nParliament\n\n\nEEBO\n755m\nBr\n1470s-1690s\nVarious\n\n\nSUP CRT\n130m\nAm\n1790s-2010s\nLegal\n\n\nTIME\n100m\nAm\n1923-2006\nMagazine\n\n\nBNC\n100m\nBr\n1980s-1993\nBalanced\n\n\nCAN\n50m\nCan\n1970s-2000s\nBalanced\n\n\nCORE\n50m\n6\n2014\nWeb",
    "crumbs": [
      "Resources",
      "Tools",
      "English Corpora Guide"
    ]
  },
  {
    "objectID": "resources/tools/taales-taassc.html",
    "href": "resources/tools/taales-taassc.html",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "",
    "text": "This guide will help you install and set up three powerful linguistic analysis tools:\n\nTAALED (Tool for the Automatic analysis of LExical Diversity) - Version 1.4.1\nTAALES (Tool for the Automatic analysis of Lexical Sophistication) - Version 2.2\n\nTAASSC (Tool for the Automatic analysis of Syntactic Sophistication and Complexity) - Version 1.3.8\n\nAll three tools are Java applications that require proper Java installation and may need security settings adjustments to run properly on your system."
  },
  {
    "objectID": "resources/tools/taales-taassc.html#what-is-java-and-why-do-you-need-it",
    "href": "resources/tools/taales-taassc.html#what-is-java-and-why-do-you-need-it",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "What is Java and Why Do You Need It?",
    "text": "What is Java and Why Do You Need It?\nJava is a programming platform that these linguistic analysis tools are built on. You need either the Java Development Kit (JDK) or Java Runtime Environment (JRE) installed on your computer to run these applications."
  },
  {
    "objectID": "resources/tools/taales-taassc.html#recommended-java-versions",
    "href": "resources/tools/taales-taassc.html#recommended-java-versions",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Recommended Java Versions",
    "text": "Recommended Java Versions\nFor optimal compatibility with TAALED, TAALES, and TAASSC, we recommend: - Java 8 (JDK 1.8) or Java 11 (LTS) for maximum compatibility - Java 17 (LTS) or Java 21 (LTS) for newer systems"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#java-installation-instructions",
    "href": "resources/tools/taales-taassc.html#java-installation-instructions",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Java Installation Instructions",
    "text": "Java Installation Instructions\n\nOption 1: Oracle JDK (Recommended for beginners)\nFor Windows: 1. Visit the Oracle Java Downloads page 2. Select your operating system (Windows) 3. Download the installer (.exe file) for Java 11 or Java 17 4. Run the installer and follow the installation wizard 5. Accept the license agreement and complete installation\nFor macOS: 1. Visit the Oracle Java Downloads page 2. Select macOS and download the .dmg file 3. Double-click the downloaded file and follow installation instructions 4. You may need to allow the installation in System Preferences &gt; Security & Privacy\nFor Linux (Ubuntu/Debian): 1. Download the .deb package from Oracle‚Äôs website, or 2. Use the terminal commands:\nsudo apt update\nsudo apt install default-jdk\n\n\nOption 2: OpenJDK (Free and Open Source)\nFor Windows: 1. Visit Microsoft Build of OpenJDK 2. Download the Windows installer 3. Run the installer and follow the setup wizard\nFor macOS: 1. Install using Homebrew (recommended):\nbrew install openjdk@11\n\nOr download from OpenJDK website\n\nFor Linux:\n# Ubuntu/Debian\nsudo apt update\nsudo apt install openjdk-11-jdk\n\n# Fedora/CentOS/RHEL\nsudo yum install java-11-openjdk-devel"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#verifying-java-installation",
    "href": "resources/tools/taales-taassc.html#verifying-java-installation",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Verifying Java Installation",
    "text": "Verifying Java Installation\nAfter installation, verify Java is properly installed:\nWindows: 1. Open Command Prompt (Press Win + R, type cmd, press Enter) 2. Type: java -version 3. You should see Java version information\nmacOS/Linux: 1. Open Terminal 2. Type: java -version 3. You should see Java version information\nIf you see version information, Java is successfully installed!"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#setting-java_home-windows-only",
    "href": "resources/tools/taales-taassc.html#setting-java_home-windows-only",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Setting JAVA_HOME (Windows only)",
    "text": "Setting JAVA_HOME (Windows only)\nIf you encounter issues, you may need to set the JAVA_HOME environment variable:\n\nFind your Java installation directory (usually C:\\Program Files\\Java\\jdk-[version])\nRight-click ‚ÄúThis PC‚Äù ‚Üí Properties ‚Üí Advanced System Settings\nClick ‚ÄúEnvironment Variables‚Äù\nUnder ‚ÄúSystem Variables,‚Äù click ‚ÄúNew‚Äù\nVariable name: JAVA_HOME\nVariable value: Your Java installation path\nClick ‚ÄúOK‚Äù to save\nFind the ‚ÄúPath‚Äù variable, click ‚ÄúEdit‚Äù\nAdd a new entry: %JAVA_HOME%\\bin\nClick ‚ÄúOK‚Äù to save all changes"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#about-taaled",
    "href": "resources/tools/taales-taassc.html#about-taaled",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "About TAALED",
    "text": "About TAALED\nTAALED is an analysis tool designed to calculate a wide variety of lexical diversity indices. Homographs are disambiguated using part of speech tags, and indices are calculated using lemma forms. It processes plain text files and produces CSV output for analysis."
  },
  {
    "objectID": "resources/tools/taales-taassc.html#download-and-installation",
    "href": "resources/tools/taales-taassc.html#download-and-installation",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Download and Installation",
    "text": "Download and Installation\n\nDownload TAALED 1.4.1:\n\nVisit: https://www.linguisticanalysistools.org/taaled.html\nClick on ‚ÄúTAALED 1.4.1‚Äù download link\nThe file will be downloaded as a .jar or .zip file\n\nExtract the Files:\n\nIf downloaded as .zip, extract all files to a folder (e.g., TAALED_1.4.1)\nIf downloaded as .jar, place it in a dedicated folder\n\nRunning TAALED:\n\nDouble-click the .jar file, or\nOpen Terminal/Command Prompt, navigate to the TAALED folder, and run:\n\njava -jar TAALED_1.4.1.jar"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#security-settings",
    "href": "resources/tools/taales-taassc.html#security-settings",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Security Settings",
    "text": "Security Settings\n\nFor macOS Users:\nGatekeeper Security Warning: If you see ‚ÄúTAALED cannot be opened because it is from an unidentified developer‚Äù:\n\nMethod 1 - One-time Override:\n\nRight-click (Control+click) on the TAALED .jar file\nSelect ‚ÄúOpen‚Äù from the context menu\nClick ‚ÄúOpen‚Äù in the security dialog\n\nMethod 2 - System Preferences:\n\nGo to System Preferences &gt; Security & Privacy &gt; General\nIf you see ‚ÄúTAALED was blocked from use because it is not from an identified developer‚Äù\nClick ‚ÄúOpen Anyway‚Äù\n\nMethod 3 - Terminal Command (Advanced):\nsudo spctl --add /path/to/TAALED_1.4.1.jar\n\n\n\nFor Windows Users:\nWindows Defender SmartScreen: If Windows blocks the application:\n\nWhen you see ‚ÄúWindows protected your PC,‚Äù click ‚ÄúMore info‚Äù\nClick ‚ÄúRun anyway‚Äù\nOr temporarily disable SmartScreen:\n\nSettings &gt; Update & Security &gt; Windows Security &gt; App & browser control\nSet ‚ÄúCheck apps and files‚Äù to ‚ÄúWarn‚Äù or ‚ÄúOff‚Äù\n\n\nJava Security Settings: 1. Open Java Control Panel: - Windows: Control Panel &gt; Java - Or search ‚ÄúConfigure Java‚Äù in Start menu 2. Go to Security tab 3. Set Security Level to ‚ÄúMedium‚Äù (if available) or ‚ÄúHigh‚Äù 4. Click ‚ÄúEdit Site List‚Äù and add local file paths if needed"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#about-taales",
    "href": "resources/tools/taales-taassc.html#about-taales",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "About TAALES",
    "text": "About TAALES\nTAALES is a tool that measures over 400 classic and new indices of lexical sophistication, and includes indices related to a wide range of sub-constructs. It provides comprehensive index diagnostics and coverage output."
  },
  {
    "objectID": "resources/tools/taales-taassc.html#download-and-installation-1",
    "href": "resources/tools/taales-taassc.html#download-and-installation-1",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Download and Installation",
    "text": "Download and Installation\n\nDownload TAALES 2.2:\n\nVisit: https://www.linguisticanalysistools.org/taales.html\nDownload the version appropriate for your system:\n\nWindows: TAALES 2.2 for Windows (64-bit)\nmacOS: TAALES 2.2 for Mac (Note: Not compatible with Big Sur or later)\nLinux: TAALES 2.2 for Linux\n\n\nInstallation:\n\nWindows: Extract the downloaded .zip file to a folder\nmacOS: Mount the .dmg file and copy the application\nLinux: Extract the .tar.gz file to a directory\n\nRunning TAALES:\n\nWindows/Linux: Double-click the .jar file or run from command line\nmacOS: Double-click the application bundle"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#security-settings-1",
    "href": "resources/tools/taales-taassc.html#security-settings-1",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Security Settings",
    "text": "Security Settings\n\nFor macOS Users:\nImportant Note: TAALES 2.2 is not compatible with macOS Big Sur (11.0) or later versions. Use an older macOS version or consider alternatives.\nFollow the same Gatekeeper bypass procedures as described in the TAALED section above.\n\n\nFor Windows Users:\nFollow the same security procedures as described in the TAALED section above.\nAdditional Java Security Settings: 1. After running TAALES for the first time, if you encounter security warnings: 2. Open Java Control Panel &gt; Security 3. Add the TAALES installation directory to the Exception Site List: - Click ‚ÄúEdit Site List‚Äù - Click ‚ÄúAdd‚Äù - Enter: file:///path/to/taales/directory - Click ‚ÄúOK‚Äù"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#about-taassc",
    "href": "resources/tools/taales-taassc.html#about-taassc",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "About TAASSC",
    "text": "About TAASSC\nTAASSC is an advanced syntactic analysis tool. It measures a number of indices related to syntactic development. Included are classic indices of syntactic complexity (e.g., mean length of T-unit) and fine-grained indices of phrasal (e.g., number of adjectives per noun phrase) and clausal (e.g., number of adverbials per clause) complexity."
  },
  {
    "objectID": "resources/tools/taales-taassc.html#download-and-installation-2",
    "href": "resources/tools/taales-taassc.html#download-and-installation-2",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Download and Installation",
    "text": "Download and Installation\n\nDownload TAASSC 1.3.8:\n\nVisit: https://www.linguisticanalysistools.org/taassc.html\nDownload TAASSC version 1.3.8 for your operating system\n\nInstallation:\n\nExtract the downloaded file to a dedicated folder\nEnsure all files are in the same directory\n\nRunning TAASSC:\n\nDouble-click the main .jar file, or\nRun from command line:\n\njava -jar TAASSC_1.3.8.jar"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#security-settings-2",
    "href": "resources/tools/taales-taassc.html#security-settings-2",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Security Settings",
    "text": "Security Settings\nFollow the same security setting procedures described for TAALED and TAALES above, adapting the file names and paths as needed."
  },
  {
    "objectID": "resources/tools/taales-taassc.html#java-issues",
    "href": "resources/tools/taales-taassc.html#java-issues",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Java Issues",
    "text": "Java Issues\n‚ÄúJava not found‚Äù or ‚ÄúJava is not recognized‚Äù: - Ensure Java is properly installed - Check that JAVA_HOME is set correctly (Windows) - Restart your computer after Java installation\n‚ÄúUnsupported major.minor version‚Äù error: - This means the application requires a newer version of Java - Install Java 8 or higher"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#security-issues",
    "href": "resources/tools/taales-taassc.html#security-issues",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Security Issues",
    "text": "Security Issues\nApplications won‚Äôt start due to security restrictions: - Follow the security bypass procedures for your operating system - Consider temporarily lowering security settings during installation - Add the application directories to your Java security exceptions\nmacOS Gatekeeper repeatedly blocks applications: - Try the sudo spctl --add command for persistent permissions - Consider signing the applications yourself (advanced users)"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#performance-issues",
    "href": "resources/tools/taales-taassc.html#performance-issues",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Performance Issues",
    "text": "Performance Issues\nApplications run slowly: - Increase Java memory allocation:\njava -Xmx2g -jar application.jar\n\nClose other applications to free up system resources\n\nLarge file processing issues: - For large text corpora, increase memory further:\njava -Xmx4g -jar application.jar"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#file-organization",
    "href": "resources/tools/taales-taassc.html#file-organization",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "File Organization",
    "text": "File Organization\n\nCreate dedicated folders for each tool to avoid conflicts\nKeep input files organized in clearly labeled directories\nSave output files with descriptive names including date and tool used"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#security-management",
    "href": "resources/tools/taales-taassc.html#security-management",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Security Management",
    "text": "Security Management\n\nOnly bypass security for trusted applications from official sources\nReturn security settings to default after installation\nKeep Java updated for security patches"
  },
  {
    "objectID": "resources/tools/taales-taassc.html#citations",
    "href": "resources/tools/taales-taassc.html#citations",
    "title": "TAALED, TAALES, TAASSC set-up guide",
    "section": "Citations",
    "text": "Citations\nWhen using these tools in your research, please cite:\nFor TAALED: Kyle, K., Crossley, S. A., & Jarvis, S. (2021). Assessing the validity of lexical diversity using direct judgements. Language Assessment Quarterly 18(2), pp.¬†154-170.\nFor TAALES: Kyle, K. & Crossley, S. A. (2015). Automatically assessing lexical sophistication: Indices, tools, findings, and application. TESOL Quarterly 49(4), pp.¬†757-786.\nKyle, K., Crossley, S. A., & Berger, C. (2018). The tool for the analysis of lexical sophistication (TAALES): Version 2.0. Behavior Research Methods 50(3), pp.¬†1030-1046.\nFor TAASSC: Kyle, K. (2016). Measuring syntactic development in L2 writing: Fine grained indices of syntactic complexity and usage-based indices of syntactic sophistication (Doctoral Dissertation)."
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Course resources organized by category.",
    "crumbs": [
      "Resources",
      "Resources"
    ]
  },
  {
    "objectID": "resources/index.html#overview",
    "href": "resources/index.html#overview",
    "title": "Resources",
    "section": "",
    "text": "Course resources organized by category.",
    "crumbs": [
      "Resources",
      "Resources"
    ]
  },
  {
    "objectID": "resources/index.html#categories",
    "href": "resources/index.html#categories",
    "title": "Resources",
    "section": "Categories",
    "text": "Categories\n\nTools and Software\nCorpora\nCode Examples",
    "crumbs": [
      "Resources",
      "Resources"
    ]
  },
  {
    "objectID": "resources/corpora/learner-corpora.html",
    "href": "resources/corpora/learner-corpora.html",
    "title": "Learner Corpora",
    "section": "",
    "text": "Content to be added.\n\nJapanese Learner corpus Natane\nInternational Center for Japanese Studies\nÊó•Êú¨Ë™ûÂ≠¶ÁøíËÄÖ‰ºöË©±„Éá„Éº„Çø„Éô„Éº„Çπ",
    "crumbs": [
      "Resources",
      "Corpora",
      "Learner Corpora"
    ]
  },
  {
    "objectID": "resources/corpora/learner-corpora.html#placeholder",
    "href": "resources/corpora/learner-corpora.html#placeholder",
    "title": "Learner Corpora",
    "section": "",
    "text": "Content to be added.\n\nJapanese Learner corpus Natane\nInternational Center for Japanese Studies\nÊó•Êú¨Ë™ûÂ≠¶ÁøíËÄÖ‰ºöË©±„Éá„Éº„Çø„Éô„Éº„Çπ",
    "crumbs": [
      "Resources",
      "Corpora",
      "Learner Corpora"
    ]
  },
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "todo.html#day-1",
    "href": "todo.html#day-1",
    "title": "",
    "section": "Day 1",
    "text": "Day 1\n[x] Session 1: ÂâçÊó•„Å´ÊúÄÁµÇË™øÊï¥ [x] Session 2: [x] Session 3:\n\nReflection\n\nSession 3 could have had a bit more guidance on\n\n\n\nhands-on 1\n\nCorpus search results"
  },
  {
    "objectID": "todo.html#day-2",
    "href": "todo.html#day-2",
    "title": "",
    "section": "Day 2",
    "text": "Day 2\n[ ] Session 4: - [x] Lexical Richness (conceptual introduction) - [x] lexical diversity - [x] lexical sophistication - [x] lexical frequency profile - [ ] Eguchi & Kyle (2020)\n\nIncomplete Content Several sections are marked but not filled:\n\n\nMTLD and MATTR explanations in Session 6\nThe Eguchi & Kyle (2020) study in Session 4\nThe vocabulary profiling section in Session 5\n\n[ ] (80% done) Session 5: - The most important thing is to tokenize the text and make frequency lists. - [x] Loading a corpus to AntConc - [x] Creating a frequency list - [x] Visualize frequency distributions - [x] Tokenize non-English language - [x] (If time left) Vocabulary profiling\nFor Hands on - Frequency list of Japanese - TagAnt„Åßtokenize - 21:19 - 21:24 (5 mins) - AntConc„Åßfrequency list„Çí‰Ωú„Çã„ÄÇ - Simple text analyzer„ÅßÁêÜËß£„Åó„Å¶Ë®òËø∞„Åô„Çã -&gt; ÊèêÂá∫\n[ ] (80% done) Session 6: - Let‚Äôs calculate lexical diversity and sophistication - Operationalization - Lexical diversity by hand - [x] create spreadsheet - Lexical diversity using TAALED - [x] students run TAALED analysis with gig corpus - [x] web app to accept TAALED results - Lexical sophistication - [x]\nFor hands on - lexical diversity - Google spreadsheet„ÅßIndex„ÅÆÈï∑ÊâÄ„Å®Áü≠ÊâÄ„ÇíÁêÜËß£„Åô„Çã„ÄÇ - Ê°à1: Parallel analysis „ÅÆÁµêÊûú„Çí„Åæ„Å®„ÇÅ„Å¶„ÇÇ„Çâ„ÅÜ - TAALED„Åßlexical diversity„ÇíÁÆóÂá∫ - Simple text analyzer„Åßplot„Åó„Å¶ÁêÜËß£„Åô„Çã -&gt; ÊèêÂá∫\n\nlexical sophistication\n\n‰Ωï„ÇíÁêÜËß£„Åó„Å¶„ÇÇ„Çâ„ÅÜ„Åã„ÄÇ -&gt; Depends on the web app -&gt; Choose three texts; run sophistication analysis; -&gt; Use ICNALE\n\n\n-&gt; „Åì„ÅÆÊó•„Å´‰Ωï„ÇíÊèêÂá∫„Åó„Å¶„ÇÇ„Çâ„ÅÜ„Åã„ÇÇ„ÅÜÂ∞ë„ÅóËÄÉ„Åà„ÇãÂøÖË¶Å„ÅÇ„Çä„ÄÇ"
  },
  {
    "objectID": "todo.html#day-3",
    "href": "todo.html#day-3",
    "title": "",
    "section": "Day 3",
    "text": "Day 3\n[ ] Session 7: - Multiword units - Strengths Of Association (SOA) - Research findings - Durrant & Schmitt - Paquot - Eguchi Kyle - Kyle Eguchi - Corpus approach - window based - depedency based\n[ ] Session 8: - [ ]\n- Let‚Äôs calculate - window based - depedency based - Calculating collocation on Spreadsheet - [x] collocation„ÇíÊäΩÂá∫ - [x] Spreadsheet„Çí‰ΩúÊàê - [ ] ÊÆã„Çä„ÅØÊ∏°„Åô„ÅãÔºü- Dependency „Å®„ÅãÔºü - [ ] Plot - Discussion of Collocational strengths\n\nCollocation spreadsheet\n\nlight-weight (can finish within class)\nwebappÂà©Áî®„Å™„Åó‰∫àÂÆö google spreadsheet„ÅßÂÆåÊàê\n\n\n[ ] (80% done) Session 9: (Think about the outline) - Choose corpus (Masaki Explains each) - ICNALE - GiG corpus - Japanese - Then they can choose a focus - Modeling proficiency score - Modeling longitudinal development - Depends on the webapp - [] Make sure ICNALE can be combined\n\nMini-research\n\nÊú¨ÂΩì„ÅØ‰Ωï„Åãvisualization„Åß„Åç„Çã„ÇÇ„Çì„Åå„ÅÑ„ÅÑ„ÅÆ„Åß„ÅØ"
  },
  {
    "objectID": "todo.html#day-4",
    "href": "todo.html#day-4",
    "title": "",
    "section": "Day 4",
    "text": "Day 4\n[ ] Session 10 - Syntactic complexity - T-unit - Complex nominal - fine-grained syntactic complexity - Dependency parsing\n[ ] Session 11 - Task 1: POS tagging with TagAnt - Task 2: POS-sensitive frequency list - Task 3: Understanding dependency grammar through visualization\n[ ] Session 12 - extracting constructions in the input corpus\n\nhands-on 4"
  },
  {
    "objectID": "todo.html#day-5",
    "href": "todo.html#day-5",
    "title": "",
    "section": "Day 5",
    "text": "Day 5\n[ ] Session 13 [ ] Session 14 [ ] Session 15"
  }
]